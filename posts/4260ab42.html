<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformer大语言模型架构原理学习笔记 | 迷路的小朋友</title><meta name="author" content="欣冻"><meta name="copyright" content="欣冻"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer大语言模型架构原理学习笔记 1. 模型架构与优化方法  在架构里面，我们做的就是把输入的x映射到输出的y，这个映射过程就是模型架构。但是如果模型里面的参数或者是这个模型是随机的，那么这个模型就没有意义，所以我们需要优化方法来优化模型。 就笔者的理解，模型内部实际上就是一堆矩阵（线性变换），与输入x进行矩阵乘法，得到输出y。通过训练，我们希望模型内部的矩阵能够尽量使得输入x映射">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer大语言模型架构原理学习笔记">
<meta property="og:url" content="https://sakjijdidji55.github.io/posts/4260ab42.html">
<meta property="og:site_name" content="迷路的小朋友">
<meta property="og:description" content="Transformer大语言模型架构原理学习笔记 1. 模型架构与优化方法  在架构里面，我们做的就是把输入的x映射到输出的y，这个映射过程就是模型架构。但是如果模型里面的参数或者是这个模型是随机的，那么这个模型就没有意义，所以我们需要优化方法来优化模型。 就笔者的理解，模型内部实际上就是一堆矩阵（线性变换），与输入x进行矩阵乘法，得到输出y。通过训练，我们希望模型内部的矩阵能够尽量使得输入x映射">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.imgs.ovh/2025/11/17/CfYA2b.md.png">
<meta property="article:published_time" content="2025-11-17T12:51:00.000Z">
<meta property="article:modified_time" content="2025-11-19T07:26:31.720Z">
<meta property="article:author" content="欣冻">
<meta property="article:tag" content="博客, 技术, 生活, tanxin, tanxin.me, 吃好喝好, 玩好, 睡好, 迷路的小朋友,tanxin55">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgs.ovh/2025/11/17/CfYA2b.md.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformer大语言模型架构原理学习笔记",
  "url": "https://sakjijdidji55.github.io/posts/4260ab42.html",
  "image": "https://i.imgs.ovh/2025/11/17/CfYA2b.md.png",
  "datePublished": "2025-11-17T12:51:00.000Z",
  "dateModified": "2025-11-19T07:26:31.720Z",
  "author": [
    {
      "@type": "Person",
      "name": "欣冻",
      "url": "https://sakjijdidji55.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/logo.ico"><link rel="canonical" href="https://sakjijdidji55.github.io/posts/4260ab42.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer大语言模型架构原理学习笔记',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="迷路的小朋友" type="application/atom+xml">
</head><body><div id="web_bg" style="background-image: url(https://i.imgs.ovh/2025/07/03/qLFy9.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/my-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-home"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://i.imgs.ovh/2025/11/17/CfYA2b.md.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.png" alt="Logo"><span class="site-name">迷路的小朋友</span></a><a class="nav-page-title" href="/"><span class="site-name">Transformer大语言模型架构原理学习笔记</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-home"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Transformer大语言模型架构原理学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-11-17T12:51:00.000Z" title="发表于 2025-11-17 20:51:00">2025-11-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-11-19T07:26:31.720Z" title="更新于 2025-11-19 15:26:31">2025-11-19</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1>Transformer大语言模型架构原理学习笔记</h1>
<h2 id="1-模型架构与优化方法">1. 模型架构与优化方法</h2>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image1.png" alt=""></p>
<p>在架构里面，我们做的就是把输入的x映射到输出的y，这个映射过程就是模型架构。但是如果模型里面的参数或者是这个模型是随机的，那么这个模型就没有意义，所以我们需要优化方法来优化模型。</p>
<p>就笔者的理解，模型内部实际上就是一堆矩阵（线性变换），与输入x进行矩阵乘法，得到输出y。通过训练，我们希望模型内部的矩阵能够尽量使得输入x映射到输出y。</p>
<p>但是完全的映射是不能的，因为输入和输出都是<strong>离散</strong>的，所以我们需要一个<strong>函数</strong>来衡量模型的预测结果与真实结果之间的差距，然后通过<strong>优化方法</strong>来最小化这个<strong>函数</strong>，从而优化模型内部的矩阵。</p>
<p>这个函数就是所谓的损失函数，常见的损失函数有<strong>均方误差、交叉熵</strong>等。（交叉熵是分类任务常用的损失函数，均方误差是回归任务常用的损失函数）</p>
<p>优化方法有梯度下降、牛顿法等。</p>
<p>这里Transformer模型架构的学习我采用了<strong>交叉熵</strong>（原因参见下文）作为损失函数，采用梯度下降作为优化方法（这个算法成本最低，其他的都还不懂）。</p>
<h2 id="2-大语言模型架构">2. 大语言模型架构</h2>
<p>Transformer大语言模型是一种基于自注意力机制的深度神经网络模型，用于处理自然语言处理任务。Transformer模型的主要架构包括编码器和解码器，以及它们之间的连接。</p>
<p>Transformer大语言模型的主要特点包括：</p>
<ul>
<li>
<p><strong>自注意力机制</strong>：Transformer模型使用自注意力机制来计算序列中每个元素与其他元素之间的关系，从而捕捉序列中的长距离依赖关系。自注意力机制通过计算查询、键和值向量之间的点积来计算注意力权重，然后使用这些权重对值向量进行加权求和，得到每个元素的上下文表示。</p>
</li>
<li>
<p><strong>位置编码</strong>：Transformer模型使用<strong>位置编码</strong>来为序列中的每个元素添加位置信息，以便模型能够区分序列中的不同位置。位置编码通常使用正弦和余弦函数来生成。</p>
</li>
<li>
<p><strong>前馈神经网络</strong>：Transformer模型中的每个编码器和解码器都包含一个前馈神经网络，用于对输入进行<strong>非线性变换</strong>。前馈神经网络由两个线性层和一个非线性激活函数组成。</p>
</li>
<li>
<p><strong>多头注意力</strong>：Transformer模型使用多头注意力机制来<strong>捕捉序列中每个元素与其他元素之间的关系</strong>。多头注意力机制通过将输入序列分成多个子序列，并使用不同的<strong>查询、键和值向量</strong>来计算注意力权重，从而捕捉序列中的不同特征。</p>
</li>
<li>
<p><strong>残差连接</strong>：Transformer模型使用残差连接来<strong>缓解深层神经网络中的梯度消失问题</strong>。残差连接通过将输入直接添加到前馈神经网络的输出中，使得梯度能够直接传播到更深的层。</p>
</li>
<li>
<p><strong>层归一化</strong>：Transformer模型使用层归一化来<strong>稳定深层神经网络的训练过程</strong>。层归一化通过对每个输入序列的每个元素进行归一化，使得输入的分布更加稳定。</p>
</li>
</ul>
<h2 id="3-模型架构原理">3. 模型架构原理</h2>
<p>模型架构可以先看下面的流程图</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image2.png" alt=""></p>
<p>在整体记录前，我们需要知道这个输入 <strong>x</strong> 是啥。</p>
<p>首先<strong>x</strong>不可能是用户输入的文本，因为文本无法映射到模型内部的矩阵，那么我们就需要分词，将一段连续的文本分割成一个个的词，然后通过词嵌入（<strong>Embedding</strong>）将词映射到模型内部的矩阵。</p>
<p>注意词很难独立映射到模型（<strong>Embedding</strong>）内部的矩阵，在分完词后，我们需要用一个 <strong>wordToId</strong> 和 <strong>IdToWord</strong> 来记录词和id之间的映射关系。这里的 <strong>id</strong> 是一个数字，所有的词会映射到独立的数字，我们用连续的数字映射不同的词，那样就可以用一个二维矩阵来表示所有的词（每一行向量都有自己的词相对应）。注意这里分完词后的词表大小，我们用 <strong>vocab_size</strong> 来表示，这个东西很重要。</p>
<p>然后我们就可以将词映射到模型内部的矩阵了，这个矩阵就是词嵌入矩阵，我们用 <strong>E</strong> 来表示。这个矩阵的维度是 <strong>[vocab_size, dim_model]</strong>，其中 <strong>dim_model</strong> 是嵌入向量的维度(就是每个词所对应的向量的维度)。</p>
<p>我们拿到用户输入的文本后，先通过分词将文本转换成一个词的列表，然后通过 <strong>wordToId</strong> 将词列表转换成 <strong>id</strong> 列表（注意在原本创建词列表时需要多四个符号，分别是 <strong>unk</strong> 表示不知道，<strong>pad</strong> 表示填充，<strong>bos</strong> 表示文本开始，<strong>eos</strong> 表示文本结束），在id列表的前后加上 <strong>bos</strong> 和 <strong>eos</strong> ，<strong>id</strong> 列表的长度也为 <strong>seq_len</strong>。</p>
<p>这里的<strong>id</strong>列表就是我们的输入<strong>x</strong>了。</p>
<p>到这里，你已经明白了怎么将输入的文本变成模型可以处理的输入了，那么，模型是如何预测的呢？</p>
<p>这里看到第一个架构</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image1.png" alt=""></p>
<p>通过中间的Model，将输入x映射到输出y，这里的Model就是我们要设计和训练的东西了。</p>
<p>而输出y是啥呢，我学习到，这里应该会是一个logit数组（即通过逻辑回归得到的数组），通过 Softmax 函数将logit数组映射到概率分布，后通过概率分布取最大概率的id，通过 IdToWord 将id映射到词，这样就可以得到预测的文本了。</p>
<p>所以我们训练时的损失函数就应该是 CrossEntropy Loss，即交叉熵损失函数。公式如下</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image3.png" alt=""></p>
<p>其中 N 是样本数量，M 是类别数量，y_{ij} 是样本 i 的第 j 类真实标签，y’_{ij} 是样本 i 的第 j 类预测概率。</p>
<p>我们训练数据实际上会是一大段一大段的文本，我们通过滑动窗口的方式，将文本分成很多个样本，每个样本的长度是 seq_len，然后通过上面的流程图，将样本映射到输出y，这里的y会是[window_size, vocab_size]形状的，然后通过交叉熵损失函数计算损失，然后通过梯度下降优化方法来优化模型内部的矩阵。</p>
<p>以上就是大语言模型的基础架构了。就可以拿回之前的流程图。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image4.png" alt=""></p>
<p>这样就清晰些了吧~</p>
<p>Transformer流程图</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image5.png" alt=""></p>
<p>接下来是训练流程和输出流程</p>
<h2 id="4-训练流程">4. 训练流程</h2>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image4.png" alt=""></p>
<h3 id="4-1-分词">4.1 分词</h3>
<p>当我们拿到一段文本时，我们需要将文本分词，将文本转换成词的列表。这里我们使用jieba分词。（jieba分词是Python中一个常用的中文分词库，它支持多种分词模式，包括精确模式、全模式和搜索引擎模式等。）</p>
<p>实际上，我们训练时会用一大段一大段的txt文件，我们要计划好在哪里分词，哪里插入对应的符号。</p>
<p>由于本人能力有限，只能在网上爬几十本小说来训练，我们就需要批量读取文本，这里先放代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Dict</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataProcessor</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;文本处理与词表构建模块&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_paths=<span class="literal">None</span>, max_vocab_size=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Initializing DataProcessor...&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 默认参数不使用可变列表</span></span><br><span class="line">        <span class="keyword">if</span> file_paths <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            file_paths = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 特殊 token</span></span><br><span class="line">        <span class="variable language_">self</span>.pad = <span class="string">&#x27;&lt;pad&gt;&#x27;</span></span><br><span class="line">        <span class="variable language_">self</span>.unk = <span class="string">&#x27;&lt;unk&gt;&#x27;</span></span><br><span class="line">        <span class="variable language_">self</span>.bos = <span class="string">&#x27;&lt;bos&gt;&#x27;</span></span><br><span class="line">        <span class="variable language_">self</span>.eos = <span class="string">&#x27;&lt;eos&gt;&#x27;</span></span><br><span class="line">        <span class="variable language_">self</span>.special_tokens = [<span class="variable language_">self</span>.pad, <span class="variable language_">self</span>.unk, <span class="variable language_">self</span>.bos, <span class="variable language_">self</span>.eos]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 数据存储</span></span><br><span class="line">        <span class="variable language_">self</span>.file_paths = file_paths</span><br><span class="line">        <span class="variable language_">self</span>.texts: <span class="type">List</span>[<span class="built_in">str</span>] = []</span><br><span class="line">        <span class="variable language_">self</span>.word_freq: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="built_in">int</span>] = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化特殊 token 词频</span></span><br><span class="line">        <span class="keyword">for</span> tok <span class="keyword">in</span> <span class="variable language_">self</span>.special_tokens:</span><br><span class="line">            <span class="variable language_">self</span>.word_freq[tok] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载文本并统计词频</span></span><br><span class="line">        <span class="variable language_">self</span>._load_and_process_data()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据词频自动构建 vocab</span></span><br><span class="line">        <span class="variable language_">self</span>.build_vocab(max_vocab_size)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Loaded <span class="subst">&#123;<span class="built_in">len</span>(self.texts)&#125;</span> paragraphs, vocab size=<span class="subst">&#123;<span class="built_in">len</span>(self.vocab)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># -----------------------------</span></span><br><span class="line">    <span class="comment"># 数据加载与预处理</span></span><br><span class="line">    <span class="comment"># -----------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_load_and_process_data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;加载文件、清洗文本、分词并统计词频&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> file_path <span class="keyword">in</span> <span class="variable language_">self</span>.file_paths:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Loading file: <span class="subst">&#123;file_path&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    content = f.read()</span><br><span class="line"></span><br><span class="line">                paragraphs = [p.strip() <span class="keyword">for</span> p <span class="keyword">in</span> content.split(<span class="string">&quot;\n\n&quot;</span>) <span class="keyword">if</span> p.strip()]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> idx, paragraph <span class="keyword">in</span> <span class="built_in">enumerate</span>(paragraphs):</span><br><span class="line"></span><br><span class="line">                    paragraph = <span class="variable language_">self</span>._clean_text(paragraph)</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> paragraph:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 保存段落文本</span></span><br><span class="line">                    <span class="variable language_">self</span>.texts.append(paragraph)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 分词</span></span><br><span class="line">                    words = jieba.lcut(paragraph)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 统计词频</span></span><br><span class="line">                    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">                        <span class="variable language_">self</span>.word_freq[w] = <span class="variable language_">self</span>.word_freq.get(w, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 每段增加一次 BOS/EOS</span></span><br><span class="line">                    <span class="variable language_">self</span>.word_freq[<span class="variable language_">self</span>.bos] += <span class="number">1</span></span><br><span class="line">                    <span class="variable language_">self</span>.word_freq[<span class="variable language_">self</span>.eos] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> idx % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">f&quot;Processed <span class="subst">&#123;idx&#125;</span> paragraphs&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Error processing file <span class="subst">&#123;file_path&#125;</span>: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># -----------------------------</span></span><br><span class="line">    <span class="comment"># 文本清洗</span></span><br><span class="line">    <span class="comment"># -----------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_clean_text</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;清理全角空格、零宽字符、多余空白等&quot;&quot;&quot;</span></span><br><span class="line">        text = text.replace(<span class="string">&#x27;\u3000&#x27;</span>, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        text = text.replace(<span class="string">&#x27;\u200b&#x27;</span>, <span class="string">&#x27;&#x27;</span>).replace(<span class="string">&#x27;\u200d&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        text = re.sub(<span class="string">r&#x27;\s+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, text)</span><br><span class="line">        <span class="keyword">return</span> text.strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># -----------------------------</span></span><br><span class="line">    <span class="comment"># 词表构建</span></span><br><span class="line">    <span class="comment"># -----------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_vocab</span>(<span class="params">self, max_vocab_size=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;按词频排序构建词表，可限制最大词表大小&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        sorted_words = <span class="built_in">sorted</span>(</span><br><span class="line">            <span class="variable language_">self</span>.word_freq.items(),</span><br><span class="line">            key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>],</span><br><span class="line">            reverse=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最终 vocab：特殊词 + 高频词</span></span><br><span class="line">        words = [w <span class="keyword">for</span> w, _ <span class="keyword">in</span> sorted_words <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.special_tokens]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> max_vocab_size:</span><br><span class="line">            words = words[:max_vocab_size - <span class="built_in">len</span>(<span class="variable language_">self</span>.special_tokens)]</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.vocab = <span class="variable language_">self</span>.special_tokens + words</span><br><span class="line"></span><br><span class="line">    <span class="comment"># -----------------------------</span></span><br><span class="line">    <span class="comment"># 工具函数</span></span><br><span class="line">    <span class="comment"># -----------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_vocab</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.vocab</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_texts</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.texts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_word_frequency</span>(<span class="params">self, word</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.word_freq.get(word, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_total_words</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="variable language_">self</span>.word_freq.values())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_sorted_vocab_by_freq</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sorted</span>(<span class="variable language_">self</span>.vocab, key=<span class="keyword">lambda</span> x: <span class="variable language_">self</span>.word_freq.get(x, <span class="number">0</span>), reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># -----------------------------</span></span><br><span class="line">    <span class="comment"># 保存函数</span></span><br><span class="line">    <span class="comment"># -----------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_vocab</span>(<span class="params">self, path=<span class="string">&quot;./vocab/vocab.txt&quot;</span></span>):</span><br><span class="line">        os.makedirs(os.path.dirname(path), exist_ok=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> <span class="variable language_">self</span>.vocab:</span><br><span class="line">                f.write(w + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Saved vocab to <span class="subst">&#123;path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_texts</span>(<span class="params">self, path=<span class="string">&quot;./vocab/processed_texts.txt&quot;</span></span>):</span><br><span class="line">        os.makedirs(os.path.dirname(path), exist_ok=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> <span class="variable language_">self</span>.texts:</span><br><span class="line">                f.write(t + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Saved processed texts to <span class="subst">&#123;path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_word_freq</span>(<span class="params">self, path=<span class="string">&quot;./vocab/vocab_freq.json&quot;</span></span>):</span><br><span class="line">        os.makedirs(os.path.dirname(path), exist_ok=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            json.dump(<span class="variable language_">self</span>.word_freq, f, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">4</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Saved word freq to <span class="subst">&#123;path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这个分词的工作流程</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image6.png" alt=""></p>
<p>这里的分词很好看懂，接下来就是用这个分词创建数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> bisect</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TextDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Transformer/GPT 语言模型数据集</span></span><br><span class="line"><span class="string">    支持：</span></span><br><span class="line"><span class="string">    - 分词 -&gt; 数字化</span></span><br><span class="line"><span class="string">    - 滑动窗口序列</span></span><br><span class="line"><span class="string">    - input 与 target 序列对</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_processor, seq_length=<span class="number">512</span>, path=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.data_processor = data_processor</span><br><span class="line">        <span class="variable language_">self</span>.seq_length = seq_length</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> path:</span><br><span class="line">            <span class="variable language_">self</span>.load(path)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># -----------------------</span></span><br><span class="line">        <span class="comment"># 构建词表映射</span></span><br><span class="line">        <span class="comment"># -----------------------</span></span><br><span class="line">        vocab = data_processor.get_vocab()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.id_to_vocab = &#123;i + <span class="number">1</span>: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">        <span class="variable language_">self</span>.vocab_to_id = &#123;w: i + <span class="number">1</span> <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pad → id = 0</span></span><br><span class="line">        <span class="variable language_">self</span>.id_to_vocab[<span class="number">0</span>] = data_processor.pad</span><br><span class="line">        <span class="variable language_">self</span>.vocab_to_id[data_processor.pad] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.vocab_size = <span class="built_in">len</span>(<span class="variable language_">self</span>.vocab_to_id)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -----------------------</span></span><br><span class="line">        <span class="comment"># 从文本构建连续 token 流</span></span><br><span class="line">        <span class="comment"># -----------------------</span></span><br><span class="line">        <span class="variable language_">self</span>.tokens = <span class="variable language_">self</span>.build_token_stream(data_processor.get_texts())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 数据集大小（滑动窗口）</span></span><br><span class="line">        <span class="variable language_">self</span>.size = <span class="built_in">len</span>(<span class="variable language_">self</span>.tokens) - seq_length - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------------------------------------------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_token_stream</span>(<span class="params">self, texts</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;将所有文本串联成一个长 token 序列&quot;&quot;&quot;</span></span><br><span class="line">        all_tokens = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> texts:</span><br><span class="line">            words = [<span class="variable language_">self</span>.data_processor.bos]</span><br><span class="line">            words.extend(jieba.lcut(text))</span><br><span class="line">            words.append(<span class="variable language_">self</span>.data_processor.eos)</span><br><span class="line"></span><br><span class="line">            ids = [<span class="variable language_">self</span>.vocab_to_id.get(w, <span class="variable language_">self</span>.vocab_to_id[<span class="variable language_">self</span>.data_processor.unk]) <span class="keyword">for</span> w <span class="keyword">in</span> words]</span><br><span class="line">            all_tokens.extend(ids)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> all_tokens</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------------------------------------------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------------------------------------------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># input: tokens[idx: idx+seq_length]</span></span><br><span class="line">        <span class="comment"># target: tokens[idx+1: idx+seq_length+1]</span></span><br><span class="line">        x = <span class="variable language_">self</span>.tokens[idx : idx + <span class="variable language_">self</span>.seq_length]</span><br><span class="line">        y = <span class="variable language_">self</span>.tokens[idx + <span class="number">1</span> : idx + <span class="variable language_">self</span>.seq_length + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------------------------------------------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, path</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Saving dataset...&quot;</span>)</span><br><span class="line">        torch.save(<span class="variable language_">self</span>, path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------------------------------------------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, path</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Loading dataset...&quot;</span>)</span><br><span class="line">        data = torch.load(path)</span><br><span class="line">        <span class="variable language_">self</span>.__dict__.update(data.__dict__)</span><br></pre></td></tr></table></figure>
<p>这个数据集的工作流程</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image7.png" alt=""></p>
<p>这样，就可以通过下面代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">file_paths = []</span><br><span class="line">dir_path = <span class="string">&quot;./data&quot;</span> <span class="comment"># 数据集文件夹路径</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> os.listdir(dir_path):</span><br><span class="line">    file_paths.append(os.path.join(dir_path, name))</span><br><span class="line"></span><br><span class="line">data_processor = DataProcessor(file_paths)</span><br><span class="line"></span><br><span class="line">text_dataset = TextDataset(</span><br><span class="line">   data_processor=data_processor,</span><br><span class="line">   seq_length=<span class="number">128</span> <span class="comment"># 滑动窗口长度，根据自己服务器内存大小调整</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>创建数据集结束，到这里我们获得 x 了</p>
<h3 id="4-2-EMbedding">4.2 EMbedding</h3>
<p>这个英语单词是嵌入的意思，就是将单词转换为向量，这里的向量实际上也算是模型的参数，是需要在训练中学习的，这个过程就是词向量的训练。</p>
<p>所以这个应该也需要设置一个模型，我们命名为 Embedding，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">512</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        位置编码类的初始化函数</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            d_model: 模型的维度</span></span><br><span class="line"><span class="string">            max_len: 序列的最大长度，默认为512</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建一个位置编码矩阵，初始值为0</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        <span class="comment"># 创建位置张量，并增加一个维度</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 计算位置编码的除数项，用于交替计算sin和cos</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() *</span><br><span class="line">                             (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用sin函数计算偶数位置</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        <span class="comment"># 使用cos函数计算奇数位置</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        <span class="comment"># 增加维度并转置位置编码矩阵</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将位置编码矩阵注册为buffer，这样它会被视为模型的一部分，但不会作为参数更新</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播函数</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            x: 输入张量，形状为(seq_len, batch_size, d_model)</span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            添加了位置编码的输入张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.pe[:x.size(<span class="number">1</span>), :].transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WordEmbeddingModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim, max_seq_length=<span class="number">512</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化词嵌入模型</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            vocab_size (int): 词汇表大小</span></span><br><span class="line"><span class="string">            embedding_dim (int): 词嵌入维度</span></span><br><span class="line"><span class="string">            max_seq_length (int): 最大序列长度，默认为512</span></span><br><span class="line"><span class="string">            dropout (float): dropout比率，默认为0.1</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(WordEmbeddingModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embedding_dim = embedding_dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化词嵌入层</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        <span class="comment"># 初始化位置编码层</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_encoding = PositionalEncoding(embedding_dim, max_seq_length)</span><br><span class="line">        <span class="comment"># 初始化dropout层</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 初始化层归一化层</span></span><br><span class="line">        <span class="variable language_">self</span>.layer_norm = nn.LayerNorm(embedding_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播过程</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            x (torch.Tensor): 输入张量，形状为(batch_size, sequence_length)</span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            torch.Tensor: 经过嵌入、位置编码和归一化后的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># x形状: (batch_size, sequence_length)</span></span><br><span class="line">        <span class="comment"># seq_length = x.size(1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 词嵌入 + 缩放（Transformer标准做法）</span></span><br><span class="line">        embedded = <span class="variable language_">self</span>.embedding(x) * math.sqrt(<span class="variable language_">self</span>.embedding_dim) <span class="comment"># (batch_size, sequence_length, embedding_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 位置编码</span></span><br><span class="line">        embedded = <span class="variable language_">self</span>.pos_encoding(embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 层归一化和dropout</span></span><br><span class="line">        embedded = <span class="variable language_">self</span>.layer_norm(embedded)</span><br><span class="line">        embedded = <span class="variable language_">self</span>.dropout(embedded)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> embedded  <span class="comment"># (batch_size, sequence_length, embedding_dim)</span></span><br></pre></td></tr></table></figure>
<p>这里需要加上位置编码，需要记录每个词的位置因为每个词在不同的语境意思不同，就需要加一个位置编码来区分不同位置。这个有公式如下</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image8.png" alt=""></p>
<p>最后出来的 <strong>X_</strong> 就是词嵌入向量。又或者说 embedded， 形状是 （batch_size, sequence_length, embedding_dim）。</p>
<h3 id="4-3-Attention">4.3 Attention</h3>
<p>注意力层用于计算输入序列中每个元素对输出序列中每个元素的影响程度，从而生成更准确的输出。注意力机制的核心思想是，对于输出序列中的每个元素，模型都会计算输入序列中每个元素对该元素的影响程度，并根据这些影响程度对输入序列进行加权求和，得到该元素的输出。</p>
<p>简而言之就是找到上下文之间的关系，那么ATTENTION就被提出，专门用来找这个关系</p>
<p>单头注意力机制流程图，我们可以先从单头注意力机制来实现，然后再扩展到多头注意力机制。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image9.png" alt=""></p>
<p>有单头注意力机制代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;单头因果自注意力层（GPT 风格）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, k_dim, v_dim, emb_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(AttentionLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.k_dim = k_dim</span><br><span class="line">        <span class="variable language_">self</span>.v_dim = v_dim</span><br><span class="line">        <span class="variable language_">self</span>.emb_dim = emb_dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q, K, V 的线性变换</span></span><br><span class="line">        <span class="variable language_">self</span>.query = nn.Linear(emb_dim, k_dim)</span><br><span class="line">        <span class="variable language_">self</span>.key = nn.Linear(emb_dim, k_dim)</span><br><span class="line">        <span class="variable language_">self</span>.value = nn.Linear(emb_dim, v_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x shape: (batch_size, seq_len, emb_dim)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到 Q, K, V</span></span><br><span class="line">        Q = <span class="variable language_">self</span>.query(x)   <span class="comment"># (batch, seq, k_dim)</span></span><br><span class="line">        K = <span class="variable language_">self</span>.key(x)     <span class="comment"># (batch, seq, k_dim)</span></span><br><span class="line">        V = <span class="variable language_">self</span>.value(x)   <span class="comment"># (batch, seq, v_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Scaled Dot-Product Attention</span></span><br><span class="line">        attention_scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>))  <span class="comment"># (batch, seq, seq)</span></span><br><span class="line">        attention_scores = attention_scores / math.sqrt(<span class="variable language_">self</span>.k_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构造因果 Mask（下三角矩阵）</span></span><br><span class="line">        <span class="comment"># 保证每个 token 只能看到自己和过去的 token</span></span><br><span class="line">        mask = torch.tril(torch.ones_like(attention_scores)).<span class="built_in">bool</span>()</span><br><span class="line">        attention_scores = attention_scores.masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># softmax 得到注意力权重</span></span><br><span class="line">        attention_weights = torch.softmax(attention_scores, dim=-<span class="number">1</span>)  <span class="comment"># (batch, seq, seq)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加权求和 V</span></span><br><span class="line">        attention_output = torch.matmul(attention_weights, V)  <span class="comment"># (batch, seq, v_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> attention_output</span><br></pre></td></tr></table></figure>
<p>由流程图看，先通过 K, Q, V 的线性变换得到 k, q, v，然后计算得分矩阵 S，然后应用因果 Mask，然后缩放，然后 softmax 得到注意力权重，最后加权求和得到输出。</p>
<p>但是单头注意力往往不够，一句话里面的意思具有多重性，事物也有不同的特征，故需要不同的注意力头来捕获不同的特征才能得到更好的效果，所以需要多头注意力机制。</p>
<p>多头注意力机制流程图</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image10.png" alt=""></p>
<p>多头注意力不必要把多个单独的 Attention 串联起来，而是可以并行计算，最后再拼接起来，所以速度会快很多。我们通过定义三个矩阵来表示总的 K Q V，将三个矩阵分割开就变成了多个 K Q V，然后分别计算，最后拼接起来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttentionLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头自注意力（Multi-Head Self-Attention）层&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_heads, emb_dim, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        多头注意力机制</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            num_heads: 注意力头数量 h</span></span><br><span class="line"><span class="string">            emb_dim: 输入/输出 embedding 维度 d_model</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttentionLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        <span class="variable language_">self</span>.emb_dim = emb_dim</span><br><span class="line">        <span class="variable language_">self</span>.head_dim = emb_dim // num_heads  <span class="comment"># 每个头的维度 d_k 或 d_v</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> emb_dim % num_heads == <span class="number">0</span>, <span class="string">&quot;emb_dim 必须能被 num_heads 整除&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q, K, V 的线性映射：从 d_model → d_model</span></span><br><span class="line">        <span class="comment"># 然后再 reshape 成多个注意力头</span></span><br><span class="line">        <span class="variable language_">self</span>.W_q = nn.Linear(emb_dim, emb_dim)</span><br><span class="line">        <span class="variable language_">self</span>.W_k = nn.Linear(emb_dim, emb_dim)</span><br><span class="line">        <span class="variable language_">self</span>.W_v = nn.Linear(emb_dim, emb_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 h 个 head 拼接后，再映射回 d_model</span></span><br><span class="line">        <span class="variable language_">self</span>.W_o = nn.Linear(emb_dim, emb_dim)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.layer_norm = nn.LayerNorm(emb_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: (batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">            mask: 可选的掩码 (seq_len, seq_len) 或 (batch, 1, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output: (batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, seq_len, _ = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 残差连接的输入</span></span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---------- Step 1: 线性投影并拆分成多个头 ----------</span></span><br><span class="line">        <span class="comment"># 得到形状 (batch, seq_len, h, head_dim) → 转置成 (batch, h, seq_len, head_dim)</span></span><br><span class="line">        Q = <span class="variable language_">self</span>.W_q(x).view(batch_size, seq_len, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        K = <span class="variable language_">self</span>.W_k(x).view(batch_size, seq_len, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        V = <span class="variable language_">self</span>.W_v(x).view(batch_size, seq_len, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---------- Step 2: QK^T 计算注意力分数 ----------</span></span><br><span class="line">        <span class="comment"># 形状变成 (batch, h, seq_len, seq_len)</span></span><br><span class="line">        attention_scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(<span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---------- Step 3: 应用掩码（masking） ----------</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># mask=0 的地方填入 -inf，使 softmax=0</span></span><br><span class="line">            attention_scores = attention_scores.masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 因果 mask (GPT 使用)：下三角为1，上三角为0</span></span><br><span class="line">            causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).<span class="built_in">bool</span>()</span><br><span class="line">            attention_scores = attention_scores.masked_fill(~causal_mask, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---------- Step 4: softmax 得注意力权重 ----------</span></span><br><span class="line">        attention_weights = torch.softmax(attention_scores, dim=-<span class="number">1</span>)</span><br><span class="line">        attention_weights = <span class="variable language_">self</span>.dropout(attention_weights)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---------- Step 5: 加权求和得到注意力输出 ----------</span></span><br><span class="line">        <span class="comment"># 每个头的输出: (batch, h, seq_len, head_dim)</span></span><br><span class="line">        attention_output = torch.matmul(attention_weights, V)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---------- Step 6: 合并多个头 ----------</span></span><br><span class="line">        <span class="comment"># 转回 (batch, seq_len, h * head_dim = d_model)</span></span><br><span class="line">        attention_output = attention_output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(</span><br><span class="line">            batch_size, seq_len, <span class="variable language_">self</span>.emb_dim</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---------- Step 7: 输出线性层 ----------</span></span><br><span class="line">        attention_output = <span class="variable language_">self</span>.W_o(attention_output)</span><br><span class="line">        attention_output = <span class="variable language_">self</span>.dropout(attention_output)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---------- Step 8: 残差连接 + LayerNorm ----------</span></span><br><span class="line">        output = <span class="variable language_">self</span>.layer_norm(attention_output + residual)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>前馈神经网络（Feed Forward Neural Network）由两个线性层和一个 ReLU 激活函数组成。第一个线性层将输入映射到更高维度，第二个线性层将映射后的结果映射回原始维度。中间的 ReLU 激活函数引入非线性，使模型能够学习更复杂的模式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForwardNetwork</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer 前馈全连接网络（Position-wise Feed Forward Network, FFN）&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, emb_dim, expansion_factor=<span class="number">4</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FeedForwardNetwork, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输入/输出维度都是 emb_dim，隐藏层维度扩大 expansion_factor 倍（通常是 4×）</span></span><br><span class="line">        <span class="variable language_">self</span>.emb_dim = emb_dim</span><br><span class="line">        <span class="variable language_">self</span>.hidden_dim = emb_dim * expansion_factor</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前馈网络 FFN = Linear → GELU(ReLU) → Dropout → Linear → Dropout</span></span><br><span class="line">        <span class="variable language_">self</span>.network = nn.Sequential(</span><br><span class="line">            nn.Linear(emb_dim, <span class="variable language_">self</span>.hidden_dim),   <span class="comment"># 第 1 个全连接层（升维）</span></span><br><span class="line">            nn.GELU(),                             <span class="comment"># 激活函数（可改为 ReLU）</span></span><br><span class="line">            nn.Dropout(dropout),                   <span class="comment"># Dropout 防止过拟合</span></span><br><span class="line">            nn.Linear(<span class="variable language_">self</span>.hidden_dim, emb_dim),   <span class="comment"># 第 2 个全连接层（降回 emb_dim）</span></span><br><span class="line">            nn.Dropout(dropout)                    <span class="comment"># 输出后再做一次 Dropout</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 残差连接后的 LayerNorm</span></span><br><span class="line">        <span class="variable language_">self</span>.layer_norm = nn.LayerNorm(emb_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: (batch_size, seq_len, emb_dim)</span></span><br><span class="line">        </span><br><span class="line">        residual = x                     <span class="comment"># 残差连接（将输入保留下来）</span></span><br><span class="line">        output = <span class="variable language_">self</span>.network(x)         <span class="comment"># 通过两层全连接+激活函数的 FFN</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加上残差再 LayerNorm（Transformer 标准结构）</span></span><br><span class="line">        output = <span class="variable language_">self</span>.layer_norm(output + residual)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output                    <span class="comment"># 输出形状与输入一致: (batch_size, seq_len, emb_dim)</span></span><br></pre></td></tr></table></figure>
<p>这个前馈神经网络（Feed Forward Neural Network）比较简单，主要是要看使用了 残差连接避免梯度消失</p>
<p>将前面组合成完整的 Transformer 层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;完整的Transformer块&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_heads, emb_dim, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerBlock, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.attention = MultiHeadAttentionLayer(num_heads, emb_dim, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = FeedForwardNetwork(emb_dim, dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 自注意力子层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.attention(x, mask)</span><br><span class="line">        <span class="comment"># 前馈网络子层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.feed_forward(x) <span class="comment"># (batch_size, seq_length, emb_dim)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="4-4-输出预测层">4.4 输出预测层</h3>
<p>通过前面的铺垫，我们获得了完整语义，完整语境，现在就要来说接下来是啥话了。我们需要通过一个全连接层来输出预测 logit，通过 SoftMax 就可以拿到预测概率然后进行选词。</p>
<p>不过前面的参数量较大，我们可以选择用一个新的 FFN 来表示，也可以共享第一层里面的输出层的参数。</p>
<p>这里我使用第二种，不过给出第一种的代码。</p>
<p>第一种</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">OutputLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;输出预测层&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, emb_dim, vocab_size, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(OutputLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出层：emb_dim -&gt; vocab_size</span></span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(emb_dim, vocab_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dropout</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: (batch_size, seq_len, emb_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dropout</span></span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出层    # (batch_size, seq_len, vocab_size)</span></span><br><span class="line">        output = <span class="variable language_">self</span>.linear(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>第二种</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LinearLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;权重共享输出层 - 大幅减少参数&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, emb_dim, vocab_size, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 注意：这个层需要与词嵌入层共享权重</span></span><br><span class="line">        <span class="variable language_">self</span>.output = nn.Linear(emb_dim, vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.output(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tie_weights</span>(<span class="params">self, embedding_layer</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;与词嵌入层共享权重&quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.output.weight = embedding_layer.weight</span><br></pre></td></tr></table></figure>
<p>这个是普通的线性层，没有那么多讲究，维度正确就行</p>
<h3 id="4-5-组合成完整的-Transformer-模型（GTP2架构）">4.5 组合成完整的 Transformer 模型（GTP2架构）</h3>
<p>跟着流程图搭建model</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/img/image13.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    完整的 Transformer 语言模型（GPT 类架构，Decoder-only）。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    主要组成：</span></span><br><span class="line"><span class="string">    - 词嵌入（含位置编码）</span></span><br><span class="line"><span class="string">    - 多个 Transformer Block（自注意力 + 前馈网络）</span></span><br><span class="line"><span class="string">    - 输出线性层 + 权重共享（tie weights）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">        x: (batch_size, seq_length)  —— 输入 token id 序列</span></span><br><span class="line"><span class="string">        mask: (batch_size, 1, 1, seq_length) 或 None —— 可选的注意力掩码</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    输出：</span></span><br><span class="line"><span class="string">        logits: (batch_size, seq_length, vocab_size)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_heads, num_transformer_blocks, emb_dim, seq_length, vocab_size, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.emb_dim = emb_dim</span><br><span class="line">        <span class="variable language_">self</span>.seq_length = seq_length</span><br><span class="line">        <span class="variable language_">self</span>.vocab_size = vocab_size</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        <span class="variable language_">self</span>.num_transformer_blocks = num_transformer_blocks</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ===============================</span></span><br><span class="line">        <span class="comment"># 1. 词 + 位置编码 Embedding 模块</span></span><br><span class="line">        <span class="comment"># ===============================</span></span><br><span class="line">        <span class="comment"># 输出形状：(batch_size, seq_length, emb_dim)</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = WordEmbeddingModel(</span><br><span class="line">            vocab_size,</span><br><span class="line">            emb_dim,</span><br><span class="line">            max_seq_length=seq_length,</span><br><span class="line">            dropout=dropout</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ===============================</span></span><br><span class="line">        <span class="comment"># 2. N 个 Transformer Block 堆叠</span></span><br><span class="line">        <span class="comment"># ===============================</span></span><br><span class="line">        <span class="comment"># 每个 Block 包含：</span></span><br><span class="line">        <span class="comment"># - 多头自注意力（含残差 + LayerNorm）</span></span><br><span class="line">        <span class="comment"># - 前馈网络 FFN（含残差 + LayerNorm）</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># 输入 / 输出形状均为 (batch_size, seq_length, emb_dim)</span></span><br><span class="line">        <span class="variable language_">self</span>.transformer_blocks = nn.ModuleList(</span><br><span class="line">            [TransformerBlock(num_heads, emb_dim, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_transformer_blocks)]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ===============================</span></span><br><span class="line">        <span class="comment"># 3. 输出层（预测词分布）</span></span><br><span class="line">        <span class="comment"># ===============================</span></span><br><span class="line">        <span class="comment"># 将最后的 embedding 映射到 vocab_size</span></span><br><span class="line">        <span class="comment"># 输出 logits: (batch_size, seq_length, vocab_size)</span></span><br><span class="line">        <span class="variable language_">self</span>.linear = LinearLayer(emb_dim, vocab_size, dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 权重共享（tie weights）</span></span><br><span class="line">        <span class="comment"># 输出层权重 = 输入词嵌入权重</span></span><br><span class="line">        <span class="variable language_">self</span>.linear.tie_weights(<span class="variable language_">self</span>.embedding.embedding)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ---------------------------------------------------</span></span><br><span class="line">    <span class="comment"># 权重初始化（未自动启用，需要调用 model.apply(model._init_weights)）</span></span><br><span class="line">    <span class="comment"># ---------------------------------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, module</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;遵循 GPT/Transformer 标准初始化方式&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, nn.Linear):</span><br><span class="line">            <span class="comment"># 正态初始化</span></span><br><span class="line">            torch.nn.init.normal_(module.weight, mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)</span><br><span class="line">            <span class="keyword">if</span> module.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                torch.nn.init.zeros_(module.bias)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(module, nn.Embedding):</span><br><span class="line">            torch.nn.init.normal_(module.weight, mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(module, nn.LayerNorm):</span><br><span class="line">            torch.nn.init.zeros_(module.bias)</span><br><span class="line">            torch.nn.init.ones_(module.weight)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ---------------------------------------------------</span></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="comment"># ---------------------------------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: 输入 token id 序列，形状 (batch_size, seq_length)</span></span><br><span class="line"><span class="string">        mask: 注意力掩码（可选），用于遮盖未来 token（因果掩码）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回：</span></span><br><span class="line"><span class="string">        logits: (batch_size, seq_length, vocab_size)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. 词嵌入 (embedding + position embedding)</span></span><br><span class="line">        <span class="comment"># 输出形状: (batch_size, seq_length, emb_dim)</span></span><br><span class="line">        x = <span class="variable language_">self</span>.embedding(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 依次通过 Transformer Blocks</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.transformer_blocks:</span><br><span class="line">            x = block(x, mask)</span><br><span class="line">            <span class="comment"># 每个 block 输出形状： (batch_size, seq_length, emb_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 输出层 — 映射到词表大小</span></span><br><span class="line">        logits = <span class="variable language_">self</span>.linear(x)  <span class="comment"># (batch_size, seq_length, vocab_size)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ---------------------------------------------------</span></span><br><span class="line">    <span class="comment"># 保存模型参数</span></span><br><span class="line">    <span class="comment"># ---------------------------------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, path</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;保存模型参数到文件&quot;&quot;&quot;</span></span><br><span class="line">        torch.save(<span class="variable language_">self</span>.state_dict(), path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ---------------------------------------------------</span></span><br><span class="line">    <span class="comment"># 加载模型参数</span></span><br><span class="line">    <span class="comment"># ---------------------------------------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, path</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;从文件中加载模型参数&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(path):</span><br><span class="line">            <span class="variable language_">self</span>.load_state_dict(torch.load(path))</span><br></pre></td></tr></table></figure>
<p>到这里，模型搭建完毕</p>
<h3 id="4-6-损失函数">4.6 损失函数</h3>
<p>由于是预测模型，我们用 <strong>交叉熵</strong> 损失函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">crossLoss = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Loss</span>(<span class="params">y_pred, y_true</span>):</span><br><span class="line">    y_pred = y_pred.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> crossLoss(y_pred, y_true)</span><br></pre></td></tr></table></figure>
<p>注意要调整维度，设置ignore_index（忽略掉pad）</p>
<h3 id="4-7-训练">4.7 训练</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ============================</span></span><br><span class="line"><span class="comment"># 选择设备（GPU 优先）</span></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line"><span class="comment"># 加载数据集文件路径</span></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line">file_paths = []</span><br><span class="line">dir_path = <span class="string">&quot;./data&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> os.listdir(dir_path):</span><br><span class="line">    file_paths.append(os.path.join(dir_path, name))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line"><span class="comment"># 预处理训练数据（构建词表 + 分词 + 转 ID）</span></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line">data_processor = DataProcessor(file_paths)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line"><span class="comment"># 构建训练集</span></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line">text_dataset = TextDataset(</span><br><span class="line">    data_processor=data_processor,</span><br><span class="line">    seq_length=<span class="number">128</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line"><span class="comment"># 创建测试数据处理器（复用词表）</span></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line">test_data_processor = DataProcessor()</span><br><span class="line">test_data_processor.loadbutnottextWithdataprocessor(data_processor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载测试文本，将其处理成 tokens</span></span><br><span class="line">test_data_processor.process_text(<span class="string">&quot;test_data\神明将世，看见血条的我杀疯了.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line"><span class="comment"># 构建测试集</span></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line">test_dataset = TextDataset(</span><br><span class="line">    data_processor=test_data_processor,</span><br><span class="line">    seq_length=<span class="number">128</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line">net = Model(</span><br><span class="line">    num_heads=<span class="number">8</span>,</span><br><span class="line">    num_transformer_blocks=<span class="number">6</span>,</span><br><span class="line">    emb_dim=<span class="number">256</span>,</span><br><span class="line">    seq_length=<span class="number">128</span>,</span><br><span class="line">    vocab_size=text_dataset.vocab_size,</span><br><span class="line">    dropout=<span class="number">0.1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">net.apply(net._init_weights)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 尝试加载已有模型（微调）</span></span><br><span class="line">net.load(<span class="string">&quot;./model/model.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line"><span class="comment"># 数据索引，用于随机采样</span></span><br><span class="line"><span class="comment"># ============================</span></span><br><span class="line">all_indices = np.arange(<span class="built_in">len</span>(text_dataset))</span><br><span class="line">test_indices = np.arange(<span class="built_in">len</span>(test_dataset))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个 epoch 从训练集中随机抽 8000 个样本</span></span><br><span class="line">num_samples_per_epoch = <span class="number">8000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================================</span></span><br><span class="line"><span class="comment"># 训练步骤（前向 + 反向 + 更新参数）</span></span><br><span class="line"><span class="comment"># =====================================</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">X, y, loss_fn, optimizer, net</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    一个训练 step：</span></span><br><span class="line"><span class="string">    - 前向传播</span></span><br><span class="line"><span class="string">    - 反向传播</span></span><br><span class="line"><span class="string">    - 更新模型参数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    X = X.to(device)</span><br><span class="line">    y = y.to(device)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    y_pred = net(X)          <span class="comment"># (batch, seq, vocab_size)</span></span><br><span class="line"></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">return</span> loss.item()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================================</span></span><br><span class="line"><span class="comment"># 测试步骤（计算 loss 和准确率）</span></span><br><span class="line"><span class="comment"># =====================================</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_step</span>(<span class="params">X, y, loss_fn, net</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    评估步骤：</span></span><br><span class="line"><span class="string">    - 仅前向传播</span></span><br><span class="line"><span class="string">    - 计算 loss 和准确率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    X = X.to(device)</span><br><span class="line">    y = y.to(device)</span><br><span class="line"></span><br><span class="line">    y_pred = net(X)</span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取预测 token（最大概率）</span></span><br><span class="line">    preds = y_pred.argmax(dim=-<span class="number">1</span>)  <span class="comment"># (batch, seq)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 忽略 PAD=0 的位置</span></span><br><span class="line">    mask = (y != <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    correct = (preds == y) &amp; mask</span><br><span class="line">    acc = correct.<span class="built_in">sum</span>().item()</span><br><span class="line">    total = mask.<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item(), acc, total</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================================</span></span><br><span class="line"><span class="comment"># 主训练循环</span></span><br><span class="line"><span class="comment"># =====================================</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    best_accuracy = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Adam 优化器</span></span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当 loss 无改进时自动降低学习率</span></span><br><span class="line">    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">        optimizer, mode=<span class="string">&#x27;min&#x27;</span>, factor=<span class="number">0.1</span>, patience=<span class="number">2</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    net = net.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ================</span></span><br><span class="line">        <span class="comment"># 训练集采样</span></span><br><span class="line">        <span class="comment"># ================</span></span><br><span class="line">        random_indices = np.random.choice(</span><br><span class="line">            all_indices, size=num_samples_per_epoch, replace=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        train_loader = DataLoader(</span><br><span class="line">            text_dataset,</span><br><span class="line">            batch_size=<span class="number">8</span>,</span><br><span class="line">            sampler=SubsetRandomSampler(random_indices),</span><br><span class="line">            shuffle=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ====== 开始训练 ======</span></span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        net.train()</span><br><span class="line"></span><br><span class="line">        idx = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_loader:</span><br><span class="line">            loss += train_step(X, y, Loss, optimizer, net)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> idx % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Batch <span class="subst">&#123;idx&#125;</span>, Loss: <span class="subst">&#123;loss/(idx+<span class="number">1</span>):<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Train Loss: <span class="subst">&#123;loss/<span class="built_in">len</span>(train_loader):<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ================</span></span><br><span class="line">        <span class="comment"># 测试集采样</span></span><br><span class="line">        <span class="comment"># ================</span></span><br><span class="line">        test_random_indices = np.random.choice(test_indices, size=<span class="number">1000</span>, replace=<span class="literal">False</span>)</span><br><span class="line">        test_loader = DataLoader(</span><br><span class="line">            test_dataset,</span><br><span class="line">            batch_size=<span class="number">8</span>,</span><br><span class="line">            sampler=SubsetRandomSampler(test_random_indices),</span><br><span class="line">            shuffle=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ====== 开始测试 ======</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            test_loss = <span class="number">0</span></span><br><span class="line">            total_correct = <span class="number">0</span></span><br><span class="line">            total_tokens = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            idx = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> X, y <span class="keyword">in</span> test_loader:</span><br><span class="line">                batch_loss, batch_correct, batch_tokens = test_step(X, y, Loss, net)</span><br><span class="line"></span><br><span class="line">                test_loss += batch_loss</span><br><span class="line">                total_correct += batch_correct</span><br><span class="line">                total_tokens += batch_tokens</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> idx % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Batch <span class="subst">&#123;idx&#125;</span>, Test Loss: <span class="subst">&#123;test_loss/(idx+<span class="number">1</span>):<span class="number">.5</span>f&#125;</span>, &quot;</span></span><br><span class="line">                          <span class="string">f&quot;Test Accuracy: <span class="subst">&#123;total_correct/total_tokens:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line">                idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Test Loss: <span class="subst">&#123;test_loss/<span class="built_in">len</span>(test_loader):<span class="number">.5</span>f&#125;</span>, &quot;</span></span><br><span class="line">                  <span class="string">f&quot;Test Accuracy: <span class="subst">&#123;total_correct/total_tokens:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ====== 模型保存（根据 accuracy 提升） ======</span></span><br><span class="line">        <span class="keyword">if</span> total_correct/total_tokens &gt; best_accuracy:</span><br><span class="line">            best_accuracy = total_correct/total_tokens</span><br><span class="line">            torch.save(net.state_dict(), <span class="string">&quot;model/model.pt&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Model improved. Saved.&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;No improvement. Skipping save.&quot;</span>)</span><br><span class="line">            scheduler.step(loss)</span><br></pre></td></tr></table></figure>
<h2 id="5-生成流程">5. 生成流程</h2>
<p>生成流程比较简单，就拿到用户输入，然后分解成token，然后传入模型跑出 logit，通过SoftMax拿到概率生成即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generate</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    文本生成类（基础版本：贪婪搜索）</span></span><br><span class="line"><span class="string">    功能：</span></span><br><span class="line"><span class="string">        - 加载模型</span></span><br><span class="line"><span class="string">        - 加载词表</span></span><br><span class="line"><span class="string">        - 对文本分词并转换为 token id</span></span><br><span class="line"><span class="string">        - 使用贪婪搜索逐 token 生成</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, device</span>):</span><br><span class="line">        <span class="comment"># 初始化 Transformer 模型</span></span><br><span class="line">        <span class="variable language_">self</span>.model = Model(</span><br><span class="line">            num_heads=CONFIG[<span class="string">&quot;num_heads&quot;</span>],</span><br><span class="line">            num_transformer_blocks=CONFIG[<span class="string">&quot;num_transformer_blocks&quot;</span>],</span><br><span class="line">            emb_dim=CONFIG[<span class="string">&quot;emb_dim&quot;</span>],</span><br><span class="line">            seq_length=CONFIG[<span class="string">&quot;seq_length&quot;</span>],</span><br><span class="line">            vocab_size=CONFIG[<span class="string">&quot;vocab_size&quot;</span>],</span><br><span class="line">            dropout=CONFIG[<span class="string">&quot;dropout&quot;</span>]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载已经训练好的权重</span></span><br><span class="line">        <span class="variable language_">self</span>.model.load(<span class="string">&quot;./model/model.pt&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 移动模型到 CPU/GPU</span></span><br><span class="line">        <span class="variable language_">self</span>.model = <span class="variable language_">self</span>.model.to(<span class="variable language_">self</span>.device)</span><br><span class="line">        <span class="variable language_">self</span>.model.<span class="built_in">eval</span>()  <span class="comment"># 推理时必须 eval()</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载词表映射</span></span><br><span class="line">        <span class="variable language_">self</span>.load_id_to_vocab_and_vocab_to_id()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 特殊 token ID</span></span><br><span class="line">        <span class="variable language_">self</span>.unk_id = <span class="variable language_">self</span>.vocab_to_id[<span class="string">&quot;&lt;unk&gt;&quot;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.eos_id = <span class="variable language_">self</span>.vocab_to_id[<span class="string">&quot;&lt;eos&gt;&quot;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.pad_id = <span class="variable language_">self</span>.vocab_to_id[<span class="string">&quot;&lt;pad&gt;&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_id_to_vocab_and_vocab_to_id</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        加载词表映射：</span></span><br><span class="line"><span class="string">        - id_to_vocab: id -&gt; 词</span></span><br><span class="line"><span class="string">        - vocab_to_id: 词 -&gt; id</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> json</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./model/id_to_vocab.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="variable language_">self</span>.id_to_vocab = json.load(f)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># json 读出来键是字符串，所以要转 int</span></span><br><span class="line">        <span class="variable language_">self</span>.id_to_vocab = &#123;<span class="built_in">int</span>(k): v <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="variable language_">self</span>.id_to_vocab.items()&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建反向词表</span></span><br><span class="line">        <span class="variable language_">self</span>.vocab_to_id = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="variable language_">self</span>.id_to_vocab.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_output</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        使用模型预测下一个 token（贪婪策略：取最大概率项）</span></span><br><span class="line"><span class="string">        input: (1, seq_length)</span></span><br><span class="line"><span class="string">        return: 最后一个位置的预测 token id</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="variable language_">self</span>.model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">            output = <span class="variable language_">self</span>.model(<span class="built_in">input</span>)          <span class="comment"># (1, seq_len, vocab_size)</span></span><br><span class="line">            output = output[:, -<span class="number">1</span>, :]           <span class="comment"># 只取最后一个 token 的输出</span></span><br><span class="line">            output_id = output.argmax(dim=-<span class="number">1</span>)   <span class="comment"># 取最大概率的 token</span></span><br><span class="line">            <span class="keyword">return</span> output_id.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_text</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将输入文本分词 → 转 token id</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(text, <span class="built_in">str</span>):</span><br><span class="line">            <span class="keyword">import</span> jieba</span><br><span class="line">            words = jieba.lcut(text)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            words = text</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将词映射到 ID，没有的映射到 &lt;unk&gt;</span></span><br><span class="line">        token_ids = [<span class="variable language_">self</span>.vocab_to_id.get(word, <span class="variable language_">self</span>.unk_id) <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">        <span class="keyword">return</span> token_ids</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_greedy</span>(<span class="params">self, text, max_length=<span class="number">100</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        贪婪搜索文本生成：</span></span><br><span class="line"><span class="string">            每一步都选概率最大的 token</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;question:&quot;</span>, text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分词 → id</span></span><br><span class="line">        input_tokens = <span class="variable language_">self</span>.tokenize_text(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 序列长度补齐到固定长度（左侧 pad）</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(input_tokens) &gt; CONFIG[<span class="string">&quot;seq_length&quot;</span>]:</span><br><span class="line">            input_tokens = input_tokens[-CONFIG[<span class="string">&quot;seq_length&quot;</span>]:]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            padding = [<span class="variable language_">self</span>.pad_id] * (CONFIG[<span class="string">&quot;seq_length&quot;</span>] - <span class="built_in">len</span>(input_tokens))</span><br><span class="line">            input_tokens = padding + input_tokens</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 转 tensor</span></span><br><span class="line">        input_tensor = torch.tensor([input_tokens], dtype=torch.long).to(<span class="variable language_">self</span>.device)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;&lt;answer&gt;&quot;</span>, <span class="string">&quot;我理解你的问题：&quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">        generated_text = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐 token 生成</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_length):</span><br><span class="line">            next_token_id = <span class="variable language_">self</span>.get_output(input_tensor)</span><br><span class="line">            next_token = <span class="variable language_">self</span>.id_to_vocab.get(next_token_id, <span class="string">&quot;&lt;unk&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> next_token_id == <span class="variable language_">self</span>.eos_id:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(next_token, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">            generated_text += next_token</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 滑动窗口：去掉最左 token，加上新 token</span></span><br><span class="line">            new_input = input_tensor[<span class="number">0</span>, <span class="number">1</span>:].tolist() + [next_token_id]</span><br><span class="line">            input_tensor = torch.tensor([new_input], dtype=torch.long).to(<span class="variable language_">self</span>.device)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;&lt;answer&gt;&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> generated_text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------- 高级版本：采样生成 -------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AdvancedGenerate</span>(<span class="title class_ inherited__">Generate</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于采样的文本生成类：</span></span><br><span class="line"><span class="string">        - 温度 temperature 控制随机性</span></span><br><span class="line"><span class="string">        - top-k 限制采样范围，使文本更合理</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, device</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_output_with_sampling</span>(<span class="params">self, input_tensor, temperature=<span class="number">1.0</span>, top_k=<span class="number">50</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        使用温度采样 + Top-k 选择下一 token</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="variable language_">self</span>.model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">            input_tensor = input_tensor.to(<span class="variable language_">self</span>.device)</span><br><span class="line">            output = <span class="variable language_">self</span>.model(input_tensor)           <span class="comment"># (1, seq_len, vocab_size)</span></span><br><span class="line">            next_token_logits = output[:, -<span class="number">1</span>, :]        <span class="comment"># 取最后一个 token 的预测分布</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 温度缩放</span></span><br><span class="line">            next_token_logits = next_token_logits / temperature</span><br><span class="line"></span><br><span class="line">            <span class="comment"># top-k 筛选低概率 token</span></span><br><span class="line">            <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                threshold = torch.topk(next_token_logits, top_k)[<span class="number">0</span>][..., -<span class="number">1</span>, <span class="literal">None</span>]</span><br><span class="line">                indices_to_remove = next_token_logits &lt; threshold</span><br><span class="line">                next_token_logits[indices_to_remove] = -<span class="built_in">float</span>(<span class="string">&#x27;Inf&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># softmax 得到概率</span></span><br><span class="line">            probs = torch.softmax(next_token_logits, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 按概率采样</span></span><br><span class="line">            next_token_id = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> next_token_id.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_with_sampling</span>(<span class="params">self, text, max_length=<span class="number">100</span>, temperature=<span class="number">0.8</span>, top_k=<span class="number">50</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        使用 top-k + 温度采样生成文本</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        input_tokens = <span class="variable language_">self</span>.tokenize_text(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 左 pad 保持长度</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(input_tokens) &gt; CONFIG[<span class="string">&quot;seq_length&quot;</span>]:</span><br><span class="line">            input_tokens = input_tokens[-CONFIG[<span class="string">&quot;seq_length&quot;</span>]:]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            padding = [<span class="variable language_">self</span>.pad_id] * (CONFIG[<span class="string">&quot;seq_length&quot;</span>] - <span class="built_in">len</span>(input_tokens))</span><br><span class="line">            input_tokens = padding + input_tokens</span><br><span class="line"></span><br><span class="line">        input_tensor = torch.tensor([input_tokens], dtype=torch.long).to(<span class="variable language_">self</span>.device)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;&lt;answer&gt;&quot;</span>, <span class="string">&quot;我理解你的问题：&quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">        generated_text = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_length):</span><br><span class="line">            next_token_id = <span class="variable language_">self</span>.get_output_with_sampling(</span><br><span class="line">                input_tensor,</span><br><span class="line">                temperature=temperature,</span><br><span class="line">                top_k=top_k</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            next_token = <span class="variable language_">self</span>.id_to_vocab.get(next_token_id, <span class="string">&quot;&lt;unk&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> next_token_id == <span class="variable language_">self</span>.eos_id:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(next_token, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">            generated_text += next_token</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 滑动窗口更新输入</span></span><br><span class="line">            new_input = input_tensor[<span class="number">0</span>, <span class="number">1</span>:].tolist() + [next_token_id]</span><br><span class="line">            input_tensor = torch.tensor([new_input], dtype=torch.long).to(<span class="variable language_">self</span>.device)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;&lt;answer&gt;&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> generated_text</span><br></pre></td></tr></table></figure>
<h1>结束</h1>
<p>本文章为作者的学习笔记，仅供参考，知识来自论文，B站讲解，Deepseek，ChatGpt和豆包。感谢指正。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/11/17/CfYA2b.md.png" alt=""></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://sakjijdidji55.github.io">欣冻</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://sakjijdidji55.github.io/posts/4260ab42.html">https://sakjijdidji55.github.io/posts/4260ab42.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://sakjijdidji55.github.io" target="_blank">迷路的小朋友</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="https://i.imgs.ovh/2025/11/17/CfYA2b.md.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/6f4fa4e7.html" title="快速幂、逆元与组合数学"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/12/19/CdIGkr.md.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">快速幂、逆元与组合数学</div></div><div class="info-2"><div class="info-item-1">快速幂、逆元与组合数学 快速幂 快速幂是一种用于计算 $a^b$ 的算法，其时间复杂度为 $O(\log b)$。 原理 快速幂的原理是利用二进制分解指数，将指数 $b$ 转化为二进制表示，然后通过不断平方和乘法来计算 $a^b$。 例如，计算 $a^{13}$，我们可以将其转化为 $a^{1101}_2$，然后通过以下步骤计算：   $a^1 = a$   $a^2 = a^1 \times a^1 = a \times a$   $a^4 = a^2 \times a^2 = (a \times a) \times (a \times a)$   $a^8 = a^4 \times a^4 = ((a \times a) \times (a \times a)) \times ((a \times a) \times (a \times a))$   $a^{13} = a^8 \times a^4 \times a^1 = ((a \times a) \times (a \times a)) \times ((a \times a) \times (a \times...</div></div></div></a><a class="pagination-related" href="/posts/e329799.html" title="C++后端服务器实现笔记"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/12/03/CsbqBg.md.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">C++后端服务器实现笔记</div></div><div class="info-2"><div class="info-item-1">C++后端服务器实现笔记 仓库 项目结构 123456789101112131415161718192021222324252627282930313233343536├── .github/             # GitHub配置目录│   └── workflows/       # GitHub Actions工作流├── .gitignore           # Git忽略配置├── 3rdparty/            # 第三方库目录│   ├── mysql/           # MySQL客户端库│   │   ├── include/     # MySQL头文件│   │   └── lib/         # MySQL库文件│   └── redis/           # Redis客户端库│       ├── include/     # Redis头文件│       └── lib/         # Redis库文件├── CMakeLists.txt       # CMake构建配置├── LICENSE     ...</div></div></div></a></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/my-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">欣冻</div><div class="author-info-description">博客, 技术, 生活</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Transformer大语言模型架构原理学习笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%8E%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">1.1.</span> <span class="toc-text">1. 模型架构与优化方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">1.2.</span> <span class="toc-text">2. 大语言模型架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86"><span class="toc-number">1.3.</span> <span class="toc-text">3. 模型架构原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-number">1.4.</span> <span class="toc-text">4. 训练流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%88%86%E8%AF%8D"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 分词</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-EMbedding"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 EMbedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Attention"><span class="toc-number">1.4.3.</span> <span class="toc-text">4.3 Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E8%BE%93%E5%87%BA%E9%A2%84%E6%B5%8B%E5%B1%82"><span class="toc-number">1.4.4.</span> <span class="toc-text">4.4 输出预测层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E7%BB%84%E5%90%88%E6%88%90%E5%AE%8C%E6%95%B4%E7%9A%84-Transformer-%E6%A8%A1%E5%9E%8B%EF%BC%88GTP2%E6%9E%B6%E6%9E%84%EF%BC%89"><span class="toc-number">1.4.5.</span> <span class="toc-text">4.5 组合成完整的 Transformer 模型（GTP2架构）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.4.6.</span> <span class="toc-text">4.6 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-%E8%AE%AD%E7%BB%83"><span class="toc-number">1.4.7.</span> <span class="toc-text">4.7 训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E7%94%9F%E6%88%90%E6%B5%81%E7%A8%8B"><span class="toc-number">1.5.</span> <span class="toc-text">5. 生成流程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">结束</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/bc9ae956.html" title="数据结构与算法实现"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/12/20/ClnnUM.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法实现"/></a><div class="content"><a class="title" href="/posts/bc9ae956.html" title="数据结构与算法实现">数据结构与算法实现</a><time datetime="2025-12-28T10:00:00.000Z" title="发表于 2025-12-28 18:00:00">2025-12-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/a6d4d6d1.html" title="JavaGame实现笔记"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/12/20/Cln2Wr.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="JavaGame实现笔记"/></a><div class="content"><a class="title" href="/posts/a6d4d6d1.html" title="JavaGame实现笔记">JavaGame实现笔记</a><time datetime="2025-12-25T04:24:02.000Z" title="发表于 2025-12-25 12:24:02">2025-12-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/e329799.html" title="C++后端服务器实现笔记"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/12/03/CsbqBg.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C++后端服务器实现笔记"/></a><div class="content"><a class="title" href="/posts/e329799.html" title="C++后端服务器实现笔记">C++后端服务器实现笔记</a><time datetime="2025-12-01T08:00:00.000Z" title="发表于 2025-12-01 16:00:00">2025-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/4260ab42.html" title="Transformer大语言模型架构原理学习笔记"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/11/17/CfYA2b.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer大语言模型架构原理学习笔记"/></a><div class="content"><a class="title" href="/posts/4260ab42.html" title="Transformer大语言模型架构原理学习笔记">Transformer大语言模型架构原理学习笔记</a><time datetime="2025-11-17T12:51:00.000Z" title="发表于 2025-11-17 20:51:00">2025-11-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/6f4fa4e7.html" title="快速幂、逆元与组合数学"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/12/19/CdIGkr.md.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="快速幂、逆元与组合数学"/></a><div class="content"><a class="title" href="/posts/6f4fa4e7.html" title="快速幂、逆元与组合数学">快速幂、逆元与组合数学</a><time datetime="2025-10-31T15:48:33.000Z" title="发表于 2025-10-31 23:48:33">2025-10-31</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://i.imgs.ovh/2025/07/03/qLFy9.png);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By 欣冻</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://blog-twikoo.xindon.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = (el = document, path = location.pathname) => {
    twikoo.init({
      el: el.querySelector('#twikoo-wrap'),
      envId: 'https://blog-twikoo.xindon.top/',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      },
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    

    isShuoshuo && (window.shuoshuoComment.destroyTwikoo = () => {
      if (el.children.length) {
        el.innerHTML = ''
        el.classList.add('no-comment')
      }
    })
  }

  const loadTwikoo = (el, path) => {
    if (typeof twikoo === 'object') setTimeout(() => init(el, path), 0)
    else btf.getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(() => init(el, path))
  }

  if (isShuoshuo) {
    'Twikoo' === 'Twikoo'
      ? window.shuoshuoComment = { loadComment: loadTwikoo }
      : window.loadOtherComment = loadTwikoo
    return
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><div class="aplayer no-destroy" data-id="13348674056" data-server="netease" data-type="playlist"   data-order="list" data-fixed="true" data-preload="auto" data-autoplay="false" data-mutex="true" ></div><script src="https://cdn.jsdelivr.net/npm/mermaid@10.2.4/dist/mermaid.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v 7.3.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v5.2.2" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://www.jsdelivr.com/" style="margin-inline:5px" data-title="本站使用JsDelivr为静态资源提供CDN加速" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('article-sort-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__slideInRight');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script><script async src="/js/ali_font.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/miku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>