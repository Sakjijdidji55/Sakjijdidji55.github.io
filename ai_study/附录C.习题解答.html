<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>附录C. 习题解答 | 迷路的小朋友</title><meta name="author" content="欣冻"><meta name="copyright" content="欣冻"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="附录C. 习题解答 附录C. 习题解答 第二掌 练习 2.1 练习 2.2   第三章 练习 3.1 练习 3.2 练习 3.3   第四章 练习 4.1 练习 4.2   第五章 练习 5.1 练习 5.2 练习 5.3 练习 5.4 练习 5.5 练习 5.6   第六章 练习 6.1 练习 6.2 练习 6.3   第七章 练习 7.1 练习 7.2 练习 7.3 练习 7.4">
<meta property="og:type" content="website">
<meta property="og:title" content="附录C. 习题解答">
<meta property="og:url" content="https://sakjijdidji55.github.io/ai_study/%E9%99%84%E5%BD%95C.%E4%B9%A0%E9%A2%98%E8%A7%A3%E7%AD%94.html">
<meta property="og:site_name" content="迷路的小朋友">
<meta property="og:description" content="附录C. 习题解答 附录C. 习题解答 第二掌 练习 2.1 练习 2.2   第三章 练习 3.1 练习 3.2 练习 3.3   第四章 练习 4.1 练习 4.2   第五章 练习 5.1 练习 5.2 练习 5.3 练习 5.4 练习 5.5 练习 5.6   第六章 练习 6.1 练习 6.2 练习 6.3   第七章 练习 7.1 练习 7.2 练习 7.3 练习 7.4">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sakjijdidji55.github.io/img/my-icon.png">
<meta property="article:published_time" content="2025-10-26T08:00:00.000Z">
<meta property="article:modified_time" content="2025-10-26T07:34:22.082Z">
<meta property="article:author" content="欣冻">
<meta property="article:tag" content="博客, 技术, 生活, tanxin, tanxin.me, 吃好喝好, 玩好, 睡好, 迷路的小朋友,tanxin55">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sakjijdidji55.github.io/img/my-icon.png"><link rel="shortcut icon" href="/img/logo.ico"><link rel="canonical" href="https://sakjijdidji55.github.io/ai_study/%E9%99%84%E5%BD%95C.%E4%B9%A0%E9%A2%98%E8%A7%A3%E7%AD%94.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '附录C. 习题解答',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'page'
}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"<!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="迷路的小朋友" type="application/atom+xml">
</head><body><div id="web_bg" style="background-image: url(https://i.imgs.ovh/2025/07/03/qLFy9.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/my-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-home"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-home-page" id="page-header" style="background-image: url(https://img.picgo.net/2025/04/05/2025-2-22fe10c0c4fb1bc202.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.png" alt="Logo"><span class="site-name">迷路的小朋友</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-home"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="page-site-info"><h1 id="site-title">附录C. 习题解答</h1></div></header><main class="layout" id="content-inner"><div id="page"><div class="container" id="article-container"><h1 id="附录C-习题解答"><a href="#附录C-习题解答" class="headerlink" title="附录C. 习题解答"></a>附录C. 习题解答</h1><ul>
<li><a href="#附录c-习题解答">附录C. 习题解答</a><ul>
<li><a href="#第二掌">第二掌</a><ul>
<li><a href="#练习-21">练习 2.1</a></li>
<li><a href="#练习-22">练习 2.2</a></li>
</ul>
</li>
<li><a href="#第三章">第三章</a><ul>
<li><a href="#练习-31">练习 3.1</a></li>
<li><a href="#练习-32">练习 3.2</a></li>
<li><a href="#练习-33">练习 3.3</a></li>
</ul>
</li>
<li><a href="#第四章">第四章</a><ul>
<li><a href="#练习-41">练习 4.1</a></li>
<li><a href="#练习-42">练习 4.2</a></li>
</ul>
</li>
<li><a href="#第五章">第五章</a><ul>
<li><a href="#练习-51">练习 5.1</a></li>
<li><a href="#练习-52">练习 5.2</a></li>
<li><a href="#练习-53">练习 5.3</a></li>
<li><a href="#练习-54">练习 5.4</a></li>
<li><a href="#练习-55">练习 5.5</a></li>
<li><a href="#练习-56">练习 5.6</a></li>
</ul>
</li>
<li><a href="#第六章">第六章</a><ul>
<li><a href="#练习-61">练习 6.1</a></li>
<li><a href="#练习-62">练习 6.2</a></li>
<li><a href="#练习-63">练习 6.3</a></li>
</ul>
</li>
<li><a href="#第七章">第七章</a><ul>
<li><a href="#练习-71">练习 7.1</a></li>
<li><a href="#练习-72">练习 7.2</a></li>
<li><a href="#练习-73">练习 7.3</a></li>
<li><a href="#练习-74">练习 7.4</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p><br /><br>练习答案的完整代码示例可以在补充 GitHub 仓库中找到：<a target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch。">https://github.com/rasbt/LLMs-from-scratch。</a></p>
<p><br /></p>
<h2 id="第二掌"><a href="#第二掌" class="headerlink" title="第二掌"></a>第二掌</h2><h3 id="练习-2-1"><a href="#练习-2-1" class="headerlink" title="练习 2.1"></a>练习 2.1</h3><p>你可以通过一次用一个字符串提示编码器来获取单独的 token ID：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tokenizer.encode(<span class="string">&quot;Ak&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(tokenizer.encode(<span class="string">&quot;w&quot;</span>))</span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<p>打印如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">33901</span>]</span><br><span class="line">[<span class="number">86</span>]</span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<p>然后你可以使用以下代码来组装原始字符串：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tokenizer.decode([<span class="number">33901</span>, <span class="number">86</span>, <span class="number">343</span>, <span class="number">86</span>, <span class="number">220</span>, <span class="number">959</span>]))</span><br></pre></td></tr></table></figure>
<p>打印如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;Akwirw ier&#x27;</span></span><br></pre></td></tr></table></figure>
<p><br /></p>
<h3 id="练习-2-2"><a href="#练习-2-2" class="headerlink" title="练习 2.2"></a>练习 2.2</h3><p>具有 max_length=2 和 stride=2 的数据加载器的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataloader = create_dataloader(raw_text, batch_size=<span class="number">4</span>, max_length=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>它产生以下格式的批次：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">40</span>, <span class="number">367</span>],</span><br><span class="line">        [<span class="number">2885</span>, <span class="number">1464</span>],</span><br><span class="line">        [<span class="number">1807</span>, <span class="number">3619</span>],</span><br><span class="line">        [ <span class="number">402</span>, <span class="number">271</span>]])</span><br></pre></td></tr></table></figure>
<p>第二个数据加载器的代码，其 max_length=8，stride=2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataloader = create_dataloader(raw_text, batch_size=<span class="number">4</span>, max_length=<span class="number">8</span>, stride=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>一个示例批次如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">40</span>, <span class="number">367</span>, <span class="number">2885</span>, <span class="number">1464</span>, <span class="number">1807</span>, <span class="number">3619</span>, <span class="number">402</span>, <span class="number">271</span>],</span><br><span class="line">        [ <span class="number">2885</span>, <span class="number">1464</span>, <span class="number">1807</span>, <span class="number">3619</span>, <span class="number">402</span>, <span class="number">271</span>, <span class="number">10899</span>, <span class="number">2138</span>],</span><br><span class="line">        [ <span class="number">1807</span>, <span class="number">3619</span>, <span class="number">402</span>, <span class="number">271</span>, <span class="number">10899</span>, <span class="number">2138</span>, <span class="number">257</span>, <span class="number">7026</span>],</span><br><span class="line">        [ <span class="number">402</span>, <span class="number">271</span>, <span class="number">10899</span>, <span class="number">2138</span>, <span class="number">257</span>, <span class="number">7026</span>, <span class="number">15632</span>, <span class="number">438</span>]])</span><br></pre></td></tr></table></figure>
<p><br /></p>
<h2 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h2><h3 id="练习-3-1"><a href="#练习-3-1" class="headerlink" title="练习 3.1"></a>练习 3.1</h3><p>正确的权重分配如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)</span><br><span class="line">sa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)</span><br><span class="line">sa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)</span><br></pre></td></tr></table></figure>
<p><br /></p>
<h3 id="练习-3-2"><a href="#练习-3-2" class="headerlink" title="练习 3.2"></a>练习 3.2</h3><p>为了获得与单头注意力中相同的 2 维输出维度，我们需要将投影维度 d_ou t更改为 1 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d_out = <span class="number">1</span></span><br><span class="line">mha = MultiHeadAttentionWrapper(d_in, d_out, block_size, <span class="number">0.0</span>, num_heads=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><br /></p>
<h3 id="练习-3-3"><a href="#练习-3-3" class="headerlink" title="练习 3.3"></a>练习 3.3</h3><p>最小 GPT-2 模型的初始化如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">block_size = <span class="number">1024</span></span><br><span class="line">d_in, d_out = <span class="number">768</span>, <span class="number">768</span></span><br><span class="line">num_heads = <span class="number">12</span></span><br><span class="line">mha = MultiHeadAttention(d_in, d_out, block_size, <span class="number">0.0</span>, num_heads)</span><br></pre></td></tr></table></figure>
<p><br /></p>
<h2 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h2><h3 id="练习-4-1"><a href="#练习-4-1" class="headerlink" title="练习 4.1"></a>练习 4.1</h3><p>我们可以按如下方式计算前馈模块和注意力模块中的参数数量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">block = TransformerBlock(GPT_CONFIG_124M)</span><br><span class="line">total_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> block.ff.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total number of parameters in feed forward module: <span class="subst">&#123;total_params:,&#125;</span>&quot;</span>)</span><br><span class="line">total_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> block.att.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total number of parameters in attention module: <span class="subst">&#123;total_params:,&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>正如我们所见，前馈模块包含的参数数量大约是注意力模块的两倍：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Total number of parameters <span class="keyword">in</span> feed forward module: <span class="number">4</span>,<span class="number">722</span>,<span class="number">432</span></span><br><span class="line">Total number of parameters <span class="keyword">in</span> attention module: <span class="number">2</span>,<span class="number">360</span>,064</span><br></pre></td></tr></table></figure>
<p><br /></p>
<h3 id="练习-4-2"><a href="#练习-4-2" class="headerlink" title="练习 4.2"></a>练习 4.2</h3><p>要实例化其他GPT模型尺寸，我们可以修改配置字典为如下所示（此处以GPT-2 XL为例）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">GPT_CONFIG = GPT_CONFIG_124M.copy()</span><br><span class="line">GPT_CONFIG[<span class="string">&quot;emb_dim&quot;</span>] = <span class="number">1600</span></span><br><span class="line">GPT_CONFIG[<span class="string">&quot;n_layers&quot;</span>] = <span class="number">48</span></span><br><span class="line">GPT_CONFIG[<span class="string">&quot;n_heads&quot;</span>] = <span class="number">25</span></span><br><span class="line">model = GPTModel(GPT_CONFIG)</span><br></pre></td></tr></table></figure>
<p>然后，重用第 4.6 节中的代码来计算参数数量和 RAM 需求，我们得到以下结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gpt2-xl:</span><br><span class="line">Total number of parameters: <span class="number">1</span>,<span class="number">637</span>,<span class="number">792</span>,<span class="number">000</span></span><br><span class="line">Number of trainable parameters considering weight tying: <span class="number">1</span>,<span class="number">557</span>,<span class="number">380</span>,<span class="number">800</span></span><br><span class="line">Total size of the model: <span class="number">6247.68</span> MB</span><br></pre></td></tr></table></figure>
<p><br /></p>
<h2 id="第五章"><a href="#第五章" class="headerlink" title="第五章"></a>第五章</h2><h3 id="练习-5-1"><a href="#练习-5-1" class="headerlink" title="练习 5.1"></a>练习 5.1</h3><p>我们可以使用本节中定义的 <code>print_sampled_tokens</code> 函数来打印 token（或单词）“pizza” 被采样的次数。让我们从我们在 5.3.1 节中定义的代码开始。</p>
<p>如果温度为 0 或 0.1，则 “pizza” token 被采样 0 次；如果温度升高到 5，则被采样 32 次。估计的概率是 32/1000 × 100% = 3.2%。实际概率是 4.3%，包含在重新缩放的 softmax 概率张量中 (scaled_probas[2][6])。</p>
<p><br /></p>
<h3 id="练习-5-2"><a href="#练习-5-2" class="headerlink" title="练习 5.2"></a>练习 5.2</h3><p>Top-k 采样和温度缩放是需要根据 LLM 以及输出中所需的 diversity 和随机性程度进行调整的设置。</p>
<p>当使用相对较小的 top-k 值（例如，小于 10）并且温度设置为低于 1 时，模型的输出变得不那么随机，更具确定性。当我们希望生成的文本更具可预测性、连贯性，并且更接近基于训练数据的最可能结果时，这种设置非常有用。</p>
<p>这种低 k 值和温度设置的应用包括生成正式文档或报告，在这些场景中，清晰度和准确性最为重要。其他应用示例包括技术分析或代码生成任务，在这些任务中，精确性至关重要。此外，问答和教育内容需要准确的答案，低于 1 的温度有助于实现这一点。</p>
<p>另一方面，较大的 top-k 值（例如，范围在 20 到 40 之间）和高于 1 的温度值很有用，当使用 LLM 进行头脑风暴或生成创意内容（如小说）时。</p>
<p><br /></p>
<h3 id="练习-5-3"><a href="#练习-5-3" class="headerlink" title="练习 5.3"></a>练习 5.3</h3><p>有多种方法可以使用 <code>generate</code> 函数强制确定性行为：</p>
<ol>
<li>将 top_k 设置为 None 且不应用温度缩放；</li>
<li>将 top_k 设置为 1。</li>
</ol>
<p><br /></p>
<h3 id="练习-5-4"><a href="#练习-5-4" class="headerlink" title="练习 5.4"></a>练习 5.4</h3><p>本质上，我们必须加载我们在主章节中保存的模型和优化器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">checkpoint = torch.load(<span class="string">&quot;model_and_optimizer.pth&quot;</span>)</span><br><span class="line">model = GPTModel(GPT_CONFIG_124M)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&quot;model_state_dict&quot;</span>])</span><br><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">5e-4</span>, weight_decay=<span class="number">0.1</span>)</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&quot;optimizer_state_dict&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>然后，调用 <code>train_simple_function</code> 并设置 <code>num_epochs=1</code>，以再次训练模型一个 epoch。</p>
<p><br /></p>
<h3 id="练习-5-5"><a href="#练习-5-5" class="headerlink" title="练习 5.5"></a>练习 5.5</h3><p>我们可以使用以下代码来计算 GPT 模型的训练集和验证集损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_loss = calc_loss_loader(train_loader, gpt, device)</span><br><span class="line">val_loss = calc_loss_loader(val_loader, gpt, device)</span><br></pre></td></tr></table></figure>
<p>具有 1.24 亿参数的模型得到的损失如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training loss: <span class="number">3.754748503367106</span></span><br><span class="line">Validation loss: <span class="number">3.559617757797241</span></span><br></pre></td></tr></table></figure>
<p>主要的观察结果是，训练集和验证集的性能处于同一水平。这可能有多种解释。</p>
<ol>
<li>当 OpenAI 训练 GPT-2 时，“The Verdict” 并非预训练数据集的一部分。因此，该模型并没有显式地过拟合训练集，并且在 “The Verdict” 的训练集和验证集部分上表现得同样出色。（验证集损失略低于训练集损失，这在深度学习中是不常见的。然而，这很可能是由于数据集相对较小而产生的随机噪声。在实践中，如果没有过拟合，训练集和验证集的性能预计大致相同。）</li>
<li>“The Verdict” 是 GPT-2 训练数据集的一部分。在这种情况下，我们无法判断模型是否过拟合训练数据，因为验证集也可能被用于训练。为了评估过拟合的程度，我们需要一个在 OpenAI 完成 GPT-2 的训练后生成的新数据集，以确保它不可能是预训练数据的一部分。</li>
</ol>
<p><br /></p>
<h3 id="练习-5-6"><a href="#练习-5-6" class="headerlink" title="练习 5.6"></a>练习 5.6</h3><p>在主章节中，我们使用了最小的 GPT-2 模型，它只有 1.24 亿个参数。原因是尽可能降低资源需求。然而，你可以通过极少的代码更改轻松地尝试更大的模型。例如，在第 5 章中，我们加载的是 15.58 亿个参数的模型权重而不是 1.24 亿个，我们只需要更改以下两行代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hparams, params = download_and_load_gpt2(model_size=<span class="string">&quot;124M&quot;</span>, models_dir=<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">model_name = <span class="string">&quot;gpt2-small (124M)&quot;</span></span><br></pre></td></tr></table></figure>
<p> 更新后的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hparams, params = download_and_load_gpt2(model_size=<span class="string">&quot;1558M&quot;</span>, models_dir=<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">model_name = <span class="string">&quot;gpt2-xl (1558M)&quot;</span></span><br></pre></td></tr></table></figure>
<p><br /></p>
<h2 id="第六章"><a href="#第六章" class="headerlink" title="第六章"></a>第六章</h2><h3 id="练习-6-1"><a href="#练习-6-1" class="headerlink" title="练习 6.1"></a>练习 6.1</h3><p>我们可以通过在初始化数据集时将最大长度设置为 <code>max_length = 1024</code>，来将输入填充到模型支持的最大 token 数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = SpamDataset(..., max_length=<span class="number">1024</span>, ...)</span><br><span class="line">val_dataset = SpamDataset(..., max_length=<span class="number">1024</span>, ...)</span><br><span class="line">test_dataset = SpamDataset(..., max_length=<span class="number">1024</span>, ...)</span><br></pre></td></tr></table></figure>
<p>然而，额外的填充导致测试准确率大幅下降，仅为 78.33%（相比之下，主章节中的准确率为 95.67%）。</p>
<p><br /></p>
<h3 id="练习-6-2"><a href="#练习-6-2" class="headerlink" title="练习 6.2"></a>练习 6.2</h3><p>与其仅微调最后一个 Transformer 模块，我们可以通过从代码中删除以下几行来微调整个模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">		param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>此修改使测试准确率提高了 1%，达到 96.67%（相比之下，主章节中的准确率为 95.67%）。</p>
<p><br /></p>
<h3 id="练习-6-3"><a href="#练习-6-3" class="headerlink" title="练习 6.3"></a>练习 6.3</h3><p>与其微调最后一个输出 token，我们可以通过将代码中所有出现的 <code>model(input_batch)[:, -1, :]</code> 更改为 <code>model(input_batch)[:, 0, :]</code> 来微调第一个输出 token。</p>
<p>正如预期的那样，由于第一个 token 包含的信息比最后一个 token 少，这一更改导致测试准确率大幅下降至 75.00%（相比之下，主章节中的准确率为 95.67%）。</p>
<p><br /></p>
<h2 id="第七章"><a href="#第七章" class="headerlink" title="第七章"></a>第七章</h2><h3 id="练习-7-1"><a href="#练习-7-1" class="headerlink" title="练习 7.1"></a>练习 7.1</h3><p>Phi-3 的提示格式，如图 7.4 在第 7 章中所示，对于给定的输入示例，看起来如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;user&gt;</span><br><span class="line">Identify the correct spelling of the following word: <span class="string">&#x27;Occasion&#x27;</span></span><br><span class="line">&lt;assistant&gt;</span><br><span class="line">The correct spelling <span class="keyword">is</span> <span class="string">&#x27;Occasion&#x27;</span>.</span><br></pre></td></tr></table></figure>
<p>要使用此模板，我们可以按如下方式修改 <code>format_input</code> 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">format_input</span>(<span class="params">entry</span>):</span><br><span class="line">    instruction_text = (</span><br><span class="line">    <span class="string">f&quot;&lt;|user|&gt;\n<span class="subst">&#123;entry[<span class="string">&#x27;instruction&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line">    )</span><br><span class="line">    input_text = <span class="string">f&quot;\n<span class="subst">&#123;entry[<span class="string">&#x27;input&#x27;</span>]&#125;</span>&quot;</span> <span class="keyword">if</span> entry[<span class="string">&quot;input&quot;</span>] <span class="keyword">else</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> instruction_text + input_text</span><br></pre></td></tr></table></figure>
<p>最后，当我们收集测试集响应时，我们还必须更新提取生成响应的方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, entry <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(test_data), total=<span class="built_in">len</span>(test_data)):</span><br><span class="line">    input_text = format_input(entry)</span><br><span class="line">    tokenizer=tokenizer</span><br><span class="line">    token_ids = generate(</span><br><span class="line">        model=model,</span><br><span class="line">        idx=text_to_token_ids(input_text, tokenizer).to(device),</span><br><span class="line">        max_new_tokens=<span class="number">256</span>,</span><br><span class="line">        context_size=BASE_CONFIG[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">        eos_id=<span class="number">50256</span></span><br><span class="line">    )</span><br><span class="line">    generated_text = token_ids_to_text(token_ids, tokenizer)</span><br><span class="line">    response_text = (                                          <span class="comment">#A</span></span><br><span class="line">        generated_text[<span class="built_in">len</span>(input_text):]</span><br><span class="line">        .replace(<span class="string">&quot;&lt;|assistant|&gt;:&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        .strip()</span><br><span class="line">    )</span><br><span class="line">    test_data[i][<span class="string">&quot;model_response&quot;</span>] = response_text</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">#A New: Adjust ###Response to &lt;|assistant|&gt;</span></span><br></pre></td></tr></table></figure>
<p>使用 Phi-3 模板对模型进行微调大约快 17%，因为它使得模型输入更短。得分接近 50，这与我们之前使用 Alpaca 风格的提示所获得的分数大致相同。</p>
<p><br /></p>
<h3 id="练习-7-2"><a href="#练习-7-2" class="headerlink" title="练习 7.2"></a>练习 7.2</h3><p>为了像第 7 章图 7.13 中所示那样屏蔽指令，我们需要对 <code>InstructionDataset</code> 类和 <code>custom_collate_fn</code> 函数进行一些小的修改。我们可以修改 <code>InstructionDataset</code> 类来收集指令的长度，我们将在 collate 函数中使用这些长度，以便在编写 collate 函数时定位目标中的指令内容位置，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InstructionDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, tokenizer</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.instruction_lengths = []                                     <span class="comment">#A</span></span><br><span class="line">        <span class="variable language_">self</span>.encoded_texts = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> entry <span class="keyword">in</span> data:</span><br><span class="line">            instruction_plus_input = format_input(entry)</span><br><span class="line">            response_text = <span class="string">f&quot;\n\n### Response:\n<span class="subst">&#123;entry[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line">            full_text = instruction_plus_input + response_text</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.encoded_texts.append(</span><br><span class="line">       		 tokenizer.encode(full_text)</span><br><span class="line">        )</span><br><span class="line">        instruction_length = <span class="built_in">len</span>(tokenizer.encode(instruction_plus_input))</span><br><span class="line">        <span class="variable language_">self</span>.instruction_lengths.append(instruction_length)                <span class="comment">#B</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):                                          <span class="comment">#C</span></span><br><span class="line">    		<span class="keyword">return</span> <span class="variable language_">self</span>.instruction_lengths[index], <span class="variable language_">self</span>.encoded_texts[index]</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">    		<span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line"><span class="comment">#A 用于指令长度的单独列表</span></span><br><span class="line"><span class="comment">#B 收集指令长度</span></span><br><span class="line"><span class="comment">#C 分别返回指令长度和文本</span></span><br></pre></td></tr></table></figure>
<p>接下来，我们更新 <code>custom_collate_fn</code>，由于 <code>InstructionDataset</code> 数据集的更改，现在的每个批次都是一个包含 <code>(instruction_length, item)</code> 的元组，而不仅仅是 <code>item</code>。此外，我们现在屏蔽目标 ID 列表中的相应指令 token：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">custom_collate_fn</span>(<span class="params"></span></span><br><span class="line"><span class="params">    batch,</span></span><br><span class="line"><span class="params">    pad_token_id=<span class="number">50256</span>,</span></span><br><span class="line"><span class="params">    ignore_index=-<span class="number">100</span>,</span></span><br><span class="line"><span class="params">    allowed_max_length=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    device=<span class="string">&quot;cpu&quot;</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line"></span><br><span class="line">batch_max_length = <span class="built_in">max</span>(<span class="built_in">len</span>(item)+<span class="number">1</span> <span class="keyword">for</span> instruction_length, item <span class="keyword">in</span> batch)      <span class="comment">#A</span></span><br><span class="line">inputs_lst, targets_lst = [], []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> instruction_length, item <span class="keyword">in</span> batch:                                         <span class="comment">#A</span></span><br><span class="line">    new_item = item.copy()</span><br><span class="line">    new_item += [pad_token_id]</span><br><span class="line">    padded = new_item + [pad_token_id] * (batch_max_length - <span class="built_in">len</span>(new_item))</span><br><span class="line">    inputs = torch.tensor(padded[:-<span class="number">1</span>])</span><br><span class="line">    targets = torch.tensor(padded[<span class="number">1</span>:])</span><br><span class="line">    mask = targets == pad_token_id</span><br><span class="line">    indices = torch.nonzero(mask).squeeze()</span><br><span class="line">    <span class="keyword">if</span> indices.numel() &gt; <span class="number">1</span>:</span><br><span class="line">    		targets[indices[<span class="number">1</span>:]] = ignore_index</span><br><span class="line">        </span><br><span class="line">    targets[:instruction_length-<span class="number">1</span>] = -<span class="number">100</span>                                       <span class="comment">#B</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> allowed_max_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        inputs = inputs[:allowed_max_length]</span><br><span class="line">        targets = targets[:allowed_max_length]</span><br><span class="line">    </span><br><span class="line">    inputs_lst.append(inputs)</span><br><span class="line">    targets_lst.append(targets)</span><br><span class="line">    </span><br><span class="line">inputs_tensor = torch.stack(inputs_lst).to(device)</span><br><span class="line">targets_tensor = torch.stack(targets_lst).to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> inputs_tensor, targets_tensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 批次现在是一个元组</span></span><br><span class="line"><span class="comment">#B 屏蔽目标中的所有输入和指令 token</span></span><br></pre></td></tr></table></figure>
<p>当评估使用这种指令屏蔽方法进行微调的模型时，它的性能略有下降（使用第 7 章中的 Ollama Llama 3 方法评估，大约下降 4 分）。这与论文《Instruction Tuning With Loss Over Instructions》（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.14394）中的观察结果一致。">https://arxiv.org/abs/2405.14394）中的观察结果一致。</a></p>
<p><br /></p>
<h3 id="练习-7-3"><a href="#练习-7-3" class="headerlink" title="练习 7.3"></a>练习 7.3</h3><p>为了在原始的 Stanford Alpaca 数据集（<a target="_blank" rel="noopener" href="https://github.com/tatsulab/stanford_alpaca）上微调模型，我们只需要将文件">https://github.com/tatsulab/stanford_alpaca）上微调模型，我们只需要将文件</a> URL 从：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_mainchapter-code/instruction-data.json&quot;</span></span><br></pre></td></tr></table></figure>
<p>修改成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&quot;https://raw.githubusercontent.com/tatsulab/stanford_alpaca/main/alpaca_data.json&quot;</span></span><br></pre></td></tr></table></figure>
<p>请注意，该数据集包含 5.2 万条记录（是第 7 章中的 50 倍），并且记录比我们在第 7 章中使用的更长。因此，强烈建议在 GPU 上运行训练。如果遇到内存不足的错误，请考虑将批处理大小从 8 减少到 4、2 或 1。除了降低批处理大小之外，您可能还需要考虑将 <code>allowed_max_length</code> 从 1024 降低到 512 或 256。</p>
<p>以下是 Alpaca 数据集中的一些示例，包括生成的模型回复。</p>
<p><br /></p>
<h3 id="练习-7-4"><a href="#练习-7-4" class="headerlink" title="练习 7.4"></a>练习 7.4</h3><p>要使用 LoRA 对模型进行指令微调，请使用附录 E 中的相关类和函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> appendix_E <span class="keyword">import</span> LoRALayer, LinearWithLoRA, replace_linear_with_lora</span><br></pre></td></tr></table></figure>
<p>接下来，在第 7.5 节的模型加载代码下方添加以下代码行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">total_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total trainable parameters before: <span class="subst">&#123;total_params:,&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">		param.requires_grad = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">total_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total trainable parameters after: <span class="subst">&#123;total_params:,&#125;</span>&quot;</span>)</span><br><span class="line">replace_linear_with_lora(model, rank=<span class="number">16</span>, alpha=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">total_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total trainable LoRA parameters: <span class="subst">&#123;total_params:,&#125;</span>&quot;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<p>请注意，在 Nvidia L4 GPU 上，使用 LoRA 进行微调在 L4 上运行需要 1.30 分钟。在同一 GPU 上，原始代码运行需要 1.80 分钟。因此，在这种情况下，LoRA 大约快 28%。使用第 7 章中的 Ollama Llama 3 方法评估的分数约为 50，这与原始模型的分数大致相同。</p>
</div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/my-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">欣冻</div><div class="author-info-description">博客, 技术, 生活</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/2f58633e.html" title="常用数据结构"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://origin.picgo.net/2025/10/11/792a8743-6d0d-43fe-91b8-0a5a77b529f4a296a597708421a1.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="常用数据结构"/></a><div class="content"><a class="title" href="/posts/2f58633e.html" title="常用数据结构">常用数据结构</a><time datetime="2025-10-11T11:00:00.000Z" title="发表于 2025-10-11 19:00:00">2025-10-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/7258f8a4.html" title="THYTHM 音游"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/08/01/HotT9.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="THYTHM 音游"/></a><div class="content"><a class="title" href="/posts/7258f8a4.html" title="THYTHM 音游">THYTHM 音游</a><time datetime="2025-08-01T04:00:00.000Z" title="发表于 2025-08-01 12:00:00">2025-08-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/73e7a68a.html" title="力扣每日一题讲解"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/07/24/QEaxN.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="力扣每日一题讲解"/></a><div class="content"><a class="title" href="/posts/73e7a68a.html" title="力扣每日一题讲解">力扣每日一题讲解</a><time datetime="2025-07-24T08:50:00.000Z" title="发表于 2025-07-24 16:50:00">2025-07-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/4729e793.html" title="数据结构入门"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/07/05/qp0G9.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构入门"/></a><div class="content"><a class="title" href="/posts/4729e793.html" title="数据结构入门">数据结构入门</a><time datetime="2025-07-04T16:11:10.000Z" title="发表于 2025-07-05 00:11:10">2025-07-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/c7c83056.html" title="四至五月学习笔记"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/07/03/qSwfa.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="四至五月学习笔记"/></a><div class="content"><a class="title" href="/posts/c7c83056.html" title="四至五月学习笔记">四至五月学习笔记</a><time datetime="2025-05-22T04:49:23.000Z" title="发表于 2025-05-22 12:49:23">2025-05-22</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/c%E8%AF%AD%E8%A8%80/"><span class="card-category-list-name">c语言</span><span class="card-category-list-count">1</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/%E5%A4%A7%E5%AE%B6%E5%A5%BD%EF%BC%8C%E6%88%91%E6%98%AF%E8%BF%B7%E8%B7%AF%E7%9A%84%E5%B0%8F%E6%9C%8B%E5%8F%8B/" style="font-size: 1.1em; color: #999">大家好，我是迷路的小朋友</a> <a href="/tags/hexo-github-blog-node-js-npm-git-%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2-hexo%E9%83%A8%E7%BD%B2/" style="font-size: 1.1em; color: #999">hexo, github, blog, node.js,npm,git,部署博客,hexo部署</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" style="font-size: 1.1em; color: #999">每日一题</a> <a href="/tags/c%E8%AF%AD%E8%A8%80-%E5%AD%A6%E4%B9%A0/" style="font-size: 1.1em; color: #999">c语言,学习</a> <a href="/tags/python-%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91-%E9%9F%B3%E6%B8%B8/" style="font-size: 1.1em; color: #999">python,游戏开发,音游</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 1.1em; color: #999">算法</a> <a href="/tags/%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%EF%BC%8Cbutterfly/" style="font-size: 1.1em; color: #999">一键部署，butterfly</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/10/">
            <span class="card-archive-list-date">
              十月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/08/">
            <span class="card-archive-list-date">
              八月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/07/">
            <span class="card-archive-list-date">
              七月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/05/">
            <span class="card-archive-list-date">
              五月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/04/">
            <span class="card-archive-list-date">
              四月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/03/">
            <span class="card-archive-list-date">
              三月 2025
            </span>
            <span class="card-archive-list-count">7</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/02/">
            <span class="card-archive-list-date">
              二月 2025
            </span>
            <span class="card-archive-list-count">7</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">21</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-10-26T08:00:41.080Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://i.imgs.ovh/2025/07/03/qLFy9.png);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By 欣冻</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://blog-twikoo.xindon.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = (el = document, path = location.pathname) => {
    twikoo.init({
      el: el.querySelector('#twikoo-wrap'),
      envId: 'https://blog-twikoo.xindon.top/',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      },
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    

    isShuoshuo && (window.shuoshuoComment.destroyTwikoo = () => {
      if (el.children.length) {
        el.innerHTML = ''
        el.classList.add('no-comment')
      }
    })
  }

  const loadTwikoo = (el, path) => {
    if (typeof twikoo === 'object') setTimeout(() => init(el, path), 0)
    else btf.getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(() => init(el, path))
  }

  if (isShuoshuo) {
    'Twikoo' === 'Twikoo'
      ? window.shuoshuoComment = { loadComment: loadTwikoo }
      : window.loadOtherComment = loadTwikoo
    return
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><div class="aplayer no-destroy" data-id="13348674056" data-server="netease" data-type="playlist"   data-order="list" data-fixed="true" data-preload="auto" data-autoplay="false" data-mutex="true" ></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var qweather_key = '0cca502ccc7341c2be6ba09309916622';
  var gaud_map_key = '5653914d2fc43aad14b253ab6cf762b9';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '112.982279,28.19409';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><script async src="/js/ali_font.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('article-sort-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__slideInRight');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v 7.3.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v5.2.2" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://www.jsdelivr.com/" style="margin-inline:5px" data-title="本站使用JsDelivr为静态资源提供CDN加速" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/miku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>