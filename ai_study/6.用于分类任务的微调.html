<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>6.用于分类任务的微调 | 迷路的小朋友</title><meta name="author" content="欣冻"><meta name="copyright" content="欣冻"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="6.用于分类任务的微调本章涵盖以下内容：  介绍不同的LLM微调方法 准备用于文本分类任务的数据集 调整预训练的 LLM 以便微调 微调 LLM 以识别垃圾短信 评估微调后的 LLM 分类器的准确性 使用微调后的 LLM 对新数据进行分类    6.用于分类任务的微调 6.1 不同类型的微调 6.2 准备数据集 6.3 创建数据加载器 6.4 使用预训练权重初始化模型 6.5 添加分类头 6.6">
<meta property="og:type" content="website">
<meta property="og:title" content="6.用于分类任务的微调">
<meta property="og:url" content="https://sakjijdidji55.github.io/ai_study/6.%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BE%AE%E8%B0%83.html">
<meta property="og:site_name" content="迷路的小朋友">
<meta property="og:description" content="6.用于分类任务的微调本章涵盖以下内容：  介绍不同的LLM微调方法 准备用于文本分类任务的数据集 调整预训练的 LLM 以便微调 微调 LLM 以识别垃圾短信 评估微调后的 LLM 分类器的准确性 使用微调后的 LLM 对新数据进行分类    6.用于分类任务的微调 6.1 不同类型的微调 6.2 准备数据集 6.3 创建数据加载器 6.4 使用预训练权重初始化模型 6.5 添加分类头 6.6">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sakjijdidji55.github.io/img/my-icon.png">
<meta property="article:published_time" content="2025-10-26T08:00:00.000Z">
<meta property="article:modified_time" content="2025-10-26T08:24:56.658Z">
<meta property="article:author" content="欣冻">
<meta property="article:tag" content="博客, 技术, 生活, tanxin, tanxin.me, 吃好喝好, 玩好, 睡好, 迷路的小朋友,tanxin55">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sakjijdidji55.github.io/img/my-icon.png"><link rel="shortcut icon" href="/img/logo.ico"><link rel="canonical" href="https://sakjijdidji55.github.io/ai_study/6.%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BE%AE%E8%B0%83.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '6.用于分类任务的微调',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'page'
}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"<!-- hexo injector head_end start --><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="迷路的小朋友" type="application/atom+xml">
</head><body><div id="web_bg" style="background-image: url(https://i.imgs.ovh/2025/07/03/qLFy9.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/my-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-home"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-home-page" id="page-header" style="background-image: url(https://img.picgo.net/2025/04/05/2025-2-22fe10c0c4fb1bc202.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.png" alt="Logo"><span class="site-name">迷路的小朋友</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-home"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="page-site-info"><h1 id="site-title">6.用于分类任务的微调</h1></div></header><main class="layout" id="content-inner"><div id="page"><div class="container" id="article-container"><h1 id="6-用于分类任务的微调"><a href="#6-用于分类任务的微调" class="headerlink" title="6.用于分类任务的微调"></a>6.用于分类任务的微调</h1><p>本章涵盖以下内容：</p>
<ul>
<li><strong>介绍不同的LLM微调方法</strong></li>
<li><strong>准备用于文本分类任务的数据集</strong></li>
<li><strong>调整预训练的 LLM 以便微调</strong></li>
<li><strong>微调 LLM 以识别垃圾短信</strong></li>
<li><strong>评估微调后的 LLM 分类器的准确性</strong></li>
<li><strong>使用微调后的 LLM 对新数据进行分类</strong></li>
</ul>
<hr>
<ul>
<li><a href="#6用于分类任务的微调">6.用于分类任务的微调</a><ul>
<li><a href="#61-不同类型的微调">6.1 不同类型的微调</a></li>
<li><a href="#62-准备数据集">6.2 准备数据集</a></li>
<li><a href="#63-创建数据加载器">6.3 创建数据加载器</a></li>
<li><a href="#64-使用预训练权重初始化模型">6.4 使用预训练权重初始化模型</a></li>
<li><a href="#65-添加分类头">6.5 添加分类头</a></li>
<li><a href="#66-计算分类损失和准确率">6.6 计算分类损失和准确率</a></li>
<li><a href="#67-使用监督数据对模型进行微调">6.7 使用监督数据对模型进行微调</a></li>
<li><a href="#68-将-llm-用于垃圾短信分类">6.8 将 LLM 用于垃圾短信分类</a></li>
<li><a href="#69-本章摘要">6.9 本章摘要</a></li>
</ul>
</li>
</ul>
<hr>
<p>在之前的章节中，我们实现了 LLM 的架构，进行了预训练，并学习了如何从外部来源（如 OpenAI）导入预训练权重。本章将在此基础上，通过微调 LLM 来完成特定目标任务，比如文本分类（见图 6.1）。我们将以一个具体的例子来说明如何将文本消息分类为垃圾短信或正常短信。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.1.png" alt=""></p>
<p>图 6.1 展示了微调 LLM 的两种主要方式：用于分类的微调（步骤 8）和用于指令遵循的微调（步骤 9）。在下一节中，我们将深入探讨这两种微调方式。</p>
<h2 id="6-1-不同类型的微调"><a href="#6-1-不同类型的微调" class="headerlink" title="6.1 不同类型的微调"></a>6.1 不同类型的微调</h2><p>微调语言模型最常见的方法是指令微调和分类微调。指令微调通过在一组任务上使用特定指令训练模型，用以提升模型对自然语言提示中任务描述的理解和执行能力，如图 6.2 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.2.png" alt=""></p>
<p>下一章将讨论指令微调，相关内容在图 6.2 中有所展示。而本章的重点是分类微调，如果您有机器学习基础，可能已经对这一概念比较熟悉。</p>
<p>在分类微调中，模型被训练用来识别特定的一组类别标签，比如“垃圾短信”和“非垃圾短信”。分类任务的应用不仅限于 LLM 和电子邮件过滤，还包括从图像中识别不同种类的植物、将新闻分类到体育、政治或科技等主题，以及在医学影像中区分良性和恶性肿瘤。</p>
<p>但有一个关键点需要注意，经过分类微调的模型只能预测训练中遇到的类别。例如，它可以判断某内容是‘垃圾短信’还是‘非垃圾短信’（如图 6.3 所示），但不能对输入文本提供其他方面的信息。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.3.png" alt=""></p>
<p>与图6.3中所示的分类微调模型不同，指令微调模型通常可以执行更广泛的任务。分类微调模型可以视为高度专业化的模型，而相比之下，开发一个适用于各种任务的通用型模型通常更具挑战性。</p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>选择合适的微调方式</strong></p>
<p>指令微调提升了模型基于用户指令进行理解和生成响应的能力。它适用于需要基于复杂用户指令处理多任务的模型，增强模型的灵活性和交互质量。而分类微调则适合需要将数据精确分类为预定义类别的任务，例如情感分析或垃圾短信检测。</p>
<p>虽然指令微调用途更广泛，但需要更大的数据集和更多的计算资源，才能训练出能胜任多种任务的模型。相比之下，分类微调所需的数据和计算量更少，但用途局限于模型已训练的特定类别。</p>
</blockquote>
<h2 id="6-2-准备数据集"><a href="#6-2-准备数据集" class="headerlink" title="6.2 准备数据集"></a>6.2 准备数据集</h2><p>在本章的剩余部分，我们将对之前章节中实现并预训练的 GPT 模型进行修改和分类微调。我们从下载并准备数据集开始，如图 6.4 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.4.png" alt=""></p>
<p>为了提供一个直观实用的分类微调示例，我们将采用一个包含垃圾消息和非垃圾消息的文本消息数据集。</p>
<p>注意，这里讨论的是通过手机发送的短信，而不是电子邮件。不过，相同的步骤也适用于电子邮件分类，感兴趣的读者可以在附录 B 的参考部分找到邮件垃圾分类数据集的链接。</p>
<p>首先，通过以下代码下载数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 6.1 Downloading and unzipping the dataset</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip&quot;</span></span><br><span class="line">zip_path = <span class="string">&quot;sms_spam_collection.zip&quot;</span></span><br><span class="line">extracted_path = <span class="string">&quot;sms_spam_collection&quot;</span></span><br><span class="line">data_file_path = Path(extracted_path) / <span class="string">&quot;SMSSpamCollection.tsv&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_and_unzip_spam_data</span>(<span class="params">url, zip_path, extracted_path, data_file_path</span>):</span><br><span class="line">    <span class="keyword">if</span> data_file_path.exists():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;data_file_path&#125;</span> already exists. Skipping download and extraction.&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">with</span> urllib.request.urlopen(url) <span class="keyword">as</span> response:          <span class="comment">#A</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(zip_path, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> out_file:</span><br><span class="line">            out_file.write(response.read())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(zip_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> zip_ref:        <span class="comment">#B</span></span><br><span class="line">        zip_ref.extractall(extracted_path)</span><br><span class="line"></span><br><span class="line">    original_file_path = Path(extracted_path) / <span class="string">&quot;SMSSpamCollection&quot;</span></span><br><span class="line">    os.rename(original_file_path, data_file_path)          <span class="comment">#C</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;File downloaded and saved as <span class="subst">&#123;data_file_path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 下载数据集</span></span><br><span class="line"><span class="comment">#B 解压数据集</span></span><br><span class="line"><span class="comment">#C 为解压的数据集文件设置.csv文件扩展名</span></span><br></pre></td></tr></table></figure>
<p>执行完上述代码后，数据集被保存为制表符分隔的文本文件“SMSSpamCollection.tsv”，位于“sms_spam_collection”文件夹中。我们可以将其加载到 pandas DataFrame 中，方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(data_file_path, sep=<span class="string">&quot;\t&quot;</span>, header=<span class="literal">None</span>, names=[<span class="string">&quot;Label&quot;</span>, <span class="string">&quot;Text&quot;</span>])</span><br><span class="line">df      <span class="comment">#A</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 在 Jupyter Notebook 中可以直接渲染数据，或者用 print(df) 命令显示数据内容</span></span><br></pre></td></tr></table></figure>
<p>保存的数据集如图 6.5 所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.5.png" alt=""></p>
<p>我们来看一下数据集中类别标签的分布情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(df[<span class="string">&quot;Label&quot;</span>].value_counts())</span><br></pre></td></tr></table></figure>
<p>执行上述代码后，我们发现数据集中‘ham’（正常短信）比‘spam’（垃圾短信）出现频率更高：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Label</span><br><span class="line">ham <span class="number">4825</span></span><br><span class="line">spam <span class="number">747</span></span><br><span class="line">Name: count, dtype: int64</span><br></pre></td></tr></table></figure>
<p>为了简化起见，同时也因为我们倾向于使用小数据集进行教学（这便于更快地微调 LLM），我们选择对数据集进行下采样，每个类别保留 747 个样本。尽管处理类别不平衡的方法有多种，但这超出了本书关于 LLM 的讨论范围。读者若有兴趣探索处理不平衡数据的方法，可以参考附录 B 的参考部分。</p>
<p>我们可以通过以下代码对数据集进行下采样，以创建一个平衡的数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 6.2 Creating a balanced dataset</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_balanced_dataset</span>(<span class="params">df</span>):</span><br><span class="line">    num_spam = df[df[<span class="string">&quot;Label&quot;</span>] == <span class="string">&quot;spam&quot;</span>].shape[<span class="number">0</span>]                                 <span class="comment">#A</span></span><br><span class="line">    ham_subset = df[df[<span class="string">&quot;Label&quot;</span>] == <span class="string">&quot;ham&quot;</span>].sample(num_spam, random_state=<span class="number">123</span>)      <span class="comment">#B</span></span><br><span class="line">    balanced_df = pd.concat([ham_subset, df[df[<span class="string">&quot;Label&quot;</span>] == <span class="string">&quot;spam&quot;</span>]])              <span class="comment">#C</span></span><br><span class="line">    <span class="keyword">return</span> balanced_df</span><br><span class="line"></span><br><span class="line">balanced_df = create_balanced_dataset(df)</span><br><span class="line"><span class="built_in">print</span>(balanced_df[<span class="string">&quot;Label&quot;</span>].value_counts())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 统计垃圾短信的实例数量</span></span><br><span class="line"><span class="comment">#B 随机抽取正常邮件实例，使其数量与垃圾短信实例相同。</span></span><br><span class="line"><span class="comment">#C 将正常短信子集与垃圾短信合并</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在执行了以上代码以平衡数据集后，我们可以看到现在垃圾短信和正常短信的数量相等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Label</span><br><span class="line">ham <span class="number">747</span></span><br><span class="line">spam <span class="number">747</span></span><br><span class="line">Name: count, dtype: int64</span><br></pre></td></tr></table></figure>
<p>接下来，我们将字符串类别标签 “ham” 和 “spam” 分别转换为整数类别标签 0 和 1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">balanced_df[<span class="string">&quot;Label&quot;</span>] = balanced_df[<span class="string">&quot;Label&quot;</span>].<span class="built_in">map</span>(&#123;<span class="string">&quot;ham&quot;</span>: <span class="number">0</span>, <span class="string">&quot;spam&quot;</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>这个过程类似于将文本转换为 token ID，但与使用包含 5 万多个词的 GPT 词汇表不同，这里我们仅处理两个 token ID：0 和 1。</p>
<p>我们还需创建一个<code>random_split</code>函数，将数据集划分为三部分：70%用于训练，10%用于验证，20%用于测试。这些比例是机器学习中用于训练、调整和评估模型的常见划分比例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 6.3 Splitting the dataset</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_split</span>(<span class="params">df, train_frac, validation_frac</span>):</span><br><span class="line">    df = df.sample(frac=<span class="number">1</span>, random_state=<span class="number">123</span>).reset_index(drop=<span class="literal">True</span>)     <span class="comment">#A</span></span><br><span class="line"></span><br><span class="line">    train_end = <span class="built_in">int</span>(<span class="built_in">len</span>(df) * train_frac)                               <span class="comment">#B</span></span><br><span class="line">    validation_end = train_end + <span class="built_in">int</span>(<span class="built_in">len</span>(df) * validation_frac)</span><br><span class="line"></span><br><span class="line">    train_df = df[:train_end]                                           <span class="comment">#C</span></span><br><span class="line">    validation_df = df[train_end:validation_end]</span><br><span class="line">    test_df = df[validation_end:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_df, validation_df, test_df</span><br><span class="line"></span><br><span class="line">train_df, validation_df, test_df = random_split(balanced_df, <span class="number">0.7</span>, <span class="number">0.1</span>)  <span class="comment">#D</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 将整个 DataFrame 随机打乱</span></span><br><span class="line"><span class="comment">#B 计算数据分割的索引</span></span><br><span class="line"><span class="comment">#C 分割 DataFrame</span></span><br><span class="line"><span class="comment">#D 测试集默认大小为 0.2（即剩余部分）</span></span><br></pre></td></tr></table></figure>
<p>此外，我们将数据集保存为 CSV 文件，以便后续复用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_df.to_csv(<span class="string">&quot;train.csv&quot;</span>, index=<span class="literal">None</span>)</span><br><span class="line">validation_df.to_csv(<span class="string">&quot;validation.csv&quot;</span>, index=<span class="literal">None</span>)</span><br><span class="line">test_df.to_csv(<span class="string">&quot;test.csv&quot;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>本节中，我们已经完成了数据集的下载、数据平衡处理，并将其划分为训练集和验证集。在接下来的部分中，我们将设置用于模型训练的 PyTorch 数据加载器。</p>
<h2 id="6-3-创建数据加载器"><a href="#6-3-创建数据加载器" class="headerlink" title="6.3 创建数据加载器"></a>6.3 创建数据加载器</h2><p>在本节中，我们将开发 PyTorch 数据加载器，其概念与第 2 章中实现的加载器类似。</p>
<p>在第2章中，我们使用滑动窗口技术生成了大小一致的文本块，并将它们分组成批次，以提高模型训练的效率。每个文本块都作为一个独立的训练实例。</p>
<p>然而，本章中我们使用的垃圾短信数据集包含长度不一的文本消息。为了像第 2 章中的文本块那样对这些消息进行批处理，我们有两种处理方式：</p>
<ol>
<li>将所有消息截断至数据集或批次中最短消息的长度。</li>
<li>将所有消息填充到数据集或批次中最长消息的长度。</li>
</ol>
<p>方案一的计算成本较低，但如果较短的消息远小于平均长度或最长消息长度，可能会导致显著的信息损失，从而降低模型的性能。因此，我们选择方案二，以完整保留所有消息的内容。</p>
<p>为实现方案二，我们需要将所有消息填充到与数据集中最长消息相同的长度，对所有较短的消息添加填充 token。为此，我们使用 <code>&quot;&lt;|endoftext|&gt;&quot;</code> 作为填充 token，正如第 2 章中所讨论的。</p>
<p>在实现细节上，我们可以在编码后的文本消息中添加与 <code>&quot;&lt;|endoftext|&gt;&quot;</code> 对应的 token ID，而不是直接将字符串 <code>&quot;&lt;|endoftext|&gt;&quot;</code> 附加到每条文本消息后，如图 6.6 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.6.png" alt=""></p>
<p>图 6.6 假定 50,256 是填充 token <code>&lt;|endoftext|&gt;</code> 的 token ID。我们可以通过使用 tiktoken 包中的 GPT-2 分词器对 <code>&lt;|endoftext|&gt;</code> 进行编码来进一步验证此 token ID 是否正确（该分词器在前几章中已使用过）:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line">tokenizer = tiktoken.get_encoding(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.encode(<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>, allowed_special=&#123;<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>&#125;))</span><br></pre></td></tr></table></figure>
<p>执行以上代码，我们发现确实返回了 <code>[50256]</code>。</p>
<p>接着，我们需要实例化数据加载器。但在此之前，我们首先需要实现一个 PyTorch Dataset，用于定义数据的加载和处理方式。</p>
<p>为此，我们定义了<code>SpamDataset</code>类，实现了图 6.6 中展示的概念。该类负责处理多个关键任务：它识别训练数据集中最长的序列，对文本消息进行编码，并确保通过填充 token 将其他序列补齐到与最长序列相同的长度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 6.4 Setting up a Pytorch Dataset class</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpamDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_file, tokenizer, max_length=<span class="literal">None</span>, pad_token_id=<span class="number">50256</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = pd.read_csv(csv_file)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.encoded_texts = [                                      <span class="comment">#A</span></span><br><span class="line">            tokenizer.encode(text) <span class="keyword">for</span> text <span class="keyword">in</span> <span class="variable language_">self</span>.data[<span class="string">&quot;Text&quot;</span>]</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> max_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.max_length = <span class="variable language_">self</span>._longest_encoded_length()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.max_length = max_length</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.encoded_texts = [                                  <span class="comment">#B</span></span><br><span class="line">                encoded_text[:<span class="variable language_">self</span>.max_length]</span><br><span class="line">                <span class="keyword">for</span> encoded_text <span class="keyword">in</span> <span class="variable language_">self</span>.encoded_texts</span><br><span class="line">            ]</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.encoded_texts = [                                      <span class="comment">#C</span></span><br><span class="line">            encoded_text + [pad_token_id] * (<span class="variable language_">self</span>.max_length - <span class="built_in">len</span>(encoded_text))</span><br><span class="line">            <span class="keyword">for</span> encoded_text <span class="keyword">in</span> <span class="variable language_">self</span>.encoded_texts</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        encoded = <span class="variable language_">self</span>.encoded_texts[index]</span><br><span class="line">        label = <span class="variable language_">self</span>.data.iloc[index][<span class="string">&quot;Label&quot;</span>]</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            torch.tensor(encoded, dtype=torch.long),</span><br><span class="line">            torch.tensor(label, dtype=torch.long)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_longest_encoded_length</span>(<span class="params">self</span>):</span><br><span class="line">        max_length = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> encoded_text <span class="keyword">in</span> <span class="variable language_">self</span>.encoded_texts:</span><br><span class="line">            encoded_length = <span class="built_in">len</span>(encoded_text)</span><br><span class="line">            <span class="keyword">if</span> encoded_length &gt; max_length:</span><br><span class="line">                max_length = encoded_length</span><br><span class="line">        <span class="keyword">return</span> max_length</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 对文本进行预分词</span></span><br><span class="line"><span class="comment">#B 若序列超过最大长度则进行截断</span></span><br><span class="line"><span class="comment">#C 将序列填充至最长序列长度</span></span><br></pre></td></tr></table></figure>
<p><code>SpamDataset</code>类从之前创建的 CSV 文件中加载数据，使用 tiktoken 库中的 GPT-2 分词器对文本进行分词，并支持将序列填充或截断为统一长度（由最长序列或预定义的最大长度决定）。这样可以确保每个输入张量大小一致，从而满足接下来数据加载器创建批量训练数据的需求：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = SpamDataset(</span><br><span class="line">    csv_file=<span class="string">&quot;train.csv&quot;</span>,</span><br><span class="line">    max_length=<span class="literal">None</span>,</span><br><span class="line">    tokenizer=tokenizer</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>请注意，数据集的 <code>max_length</code> 属性中存储了最大序列长度。如果想要查看最长序列的 token 数量，可以使用以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_dataset.max_length)</span><br></pre></td></tr></table></figure>
<p>代码输出了 120，表明最长的序列不超过 120 个 token，这也是文本消息的常见长度。值得注意的是，我们之前预训练的模型的上下文长度限制为 1,024 个 token，因此可以处理最长 1,024 个 token 的序列。如果数据集中包含更长的文本，可以在创建训练数据集时传入 <code>max_length=1024</code> 参数，以确保数据不会超出模型支持的输入（上下文）长度。</p>
<p>接下来，我们将验证集和测试集的序列填充到与训练集中最长序列相同的长度。需要注意的是，如果验证集和测试集中的某些样本长度超过了训练集中最长样本的长度，会在先前定义的 <code>SpamDataset</code> 代码中通过 <code>encoded_text[:self.max_length]</code> 进行截断。这种截断是可选的；如果确保验证集和测试集中没有超过 1,024 个 token 的序列，也可以将 <code>max_length</code> 设置为 <code>None</code> 来避免截断。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val_dataset = SpamDataset(</span><br><span class="line">    csv_file=<span class="string">&quot;validation.csv&quot;</span>,</span><br><span class="line">    max_length=train_dataset.max_length,</span><br><span class="line">    tokenizer=tokenizer</span><br><span class="line">)</span><br><span class="line">test_dataset = SpamDataset(</span><br><span class="line">    csv_file=<span class="string">&quot;test.csv&quot;</span>,</span><br><span class="line">    max_length=train_dataset.max_length,</span><br><span class="line">    tokenizer=tokenizer</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>[!NOTE]</p>
<p>练习6.1 扩展上下文长度</p>
<p>将输入补齐到模型支持的最大 token 数量，并观察其对预测性能的影响。</p>
</blockquote>
<p>将以上的数据集作为输入，我们就可以实例化数据加载器（可以回顾第 2 章中的操作）。然而，在本例中，目标表示的是类别标签，而非文本中的下一个 token。例如，选择批量大小为 8 时，每个批次包含 8 个长度为 120 的训练样本和相应的类别标签，如图 6.7 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.7.png" alt=""></p>
<p>以下代码创建了训练集、验证集和测试集的数据加载器，以批量大小为 8 加载文本消息及其标签（如图 6.7 所示）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 6.5 Creating PyTorch data loaders</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">num_workers = <span class="number">0</span>                  <span class="comment">#A</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(</span><br><span class="line">    dataset=train_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    num_workers=num_workers,</span><br><span class="line">    drop_last=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">val_loader = DataLoader(</span><br><span class="line">    dataset=val_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    num_workers=num_workers,</span><br><span class="line">    drop_last=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line">test_loader = DataLoader(</span><br><span class="line">    dataset=test_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    num_workers=num_workers,</span><br><span class="line">    drop_last=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#A 此设置可确保与大多数计算机兼容</span></span><br></pre></td></tr></table></figure>
<p>为了确保数据加载器正常工作并确实返回了预期大小的批次数据，我们可以遍历训练集数据加载器，并打印最后一个批次的张量维度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> input_batch, target_batch <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Input batch dimensions:&quot;</span>, input_batch.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Label batch dimensions&quot;</span>, target_batch.shape)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input batch dimensions: torch.Size([<span class="number">8</span>, <span class="number">120</span>])</span><br><span class="line">Label batch dimensions torch.Size([<span class="number">8</span>])</span><br></pre></td></tr></table></figure>
<p>如上所示，输入批次包含 8 个训练样本，每个样本包含 120 个token。标签张量存储了对应 8 个训练样本的类别标签。</p>
<p>最后，为了了解数据集的大小，可以打印每个数据集的批次数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;<span class="built_in">len</span>(train_loader)&#125;</span> training batches&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;<span class="built_in">len</span>(val_loader)&#125;</span> validation batches&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;<span class="built_in">len</span>(test_loader)&#125;</span> test batches&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>各数据集的批次数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">130</span> training batches</span><br><span class="line"><span class="number">19</span> validation batches</span><br><span class="line"><span class="number">38</span> test batches</span><br></pre></td></tr></table></figure>
<p>本章的数据准备工作到此结束，接下来我们将初始化模型以准备进行微调。</p>
<h2 id="6-4-使用预训练权重初始化模型"><a href="#6-4-使用预训练权重初始化模型" class="headerlink" title="6.4 使用预训练权重初始化模型"></a>6.4 使用预训练权重初始化模型</h2><p>在本节中，我们将准备用于垃圾短信分类微调的模型。首先，我们初始化上一章使用过的预训练模型，如图 6.8 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.8.png" alt=""></p>
<p>现在我们通过复用第 5 章的配置，开始进行模型准备过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">CHOOSE_MODEL = <span class="string">&quot;gpt2-small (124M)&quot;</span></span><br><span class="line">INPUT_PROMPT = <span class="string">&quot;Every effort moves&quot;</span></span><br><span class="line">BASE_CONFIG = &#123;</span><br><span class="line">    <span class="string">&quot;vocab_size&quot;</span>: <span class="number">50257</span>, <span class="comment"># Vocabulary size</span></span><br><span class="line">    <span class="string">&quot;context_length&quot;</span>: <span class="number">1024</span>, <span class="comment"># Context length</span></span><br><span class="line">    <span class="string">&quot;drop_rate&quot;</span>: <span class="number">0.0</span>, <span class="comment"># Dropout rate</span></span><br><span class="line">    <span class="string">&quot;qkv_bias&quot;</span>: <span class="literal">True</span> <span class="comment"># Query-key-value bias</span></span><br><span class="line">&#125;</span><br><span class="line">model_configs = &#123;</span><br><span class="line">    <span class="string">&quot;gpt2-small (124M)&quot;</span>: &#123;<span class="string">&quot;emb_dim&quot;</span>: <span class="number">768</span>, <span class="string">&quot;n_layers&quot;</span>: <span class="number">12</span>, <span class="string">&quot;n_heads&quot;</span>: <span class="number">12</span>&#125;,</span><br><span class="line">    <span class="string">&quot;gpt2-medium (355M)&quot;</span>: &#123;<span class="string">&quot;emb_dim&quot;</span>: <span class="number">1024</span>, <span class="string">&quot;n_layers&quot;</span>: <span class="number">24</span>, <span class="string">&quot;n_heads&quot;</span>: <span class="number">16</span>&#125;,</span><br><span class="line">    <span class="string">&quot;gpt2-large (774M)&quot;</span>: &#123;<span class="string">&quot;emb_dim&quot;</span>: <span class="number">1280</span>, <span class="string">&quot;n_layers&quot;</span>: <span class="number">36</span>, <span class="string">&quot;n_heads&quot;</span>: <span class="number">20</span>&#125;,</span><br><span class="line">    <span class="string">&quot;gpt2-xl (1558M)&quot;</span>: &#123;<span class="string">&quot;emb_dim&quot;</span>: <span class="number">1600</span>, <span class="string">&quot;n_layers&quot;</span>: <span class="number">48</span>, <span class="string">&quot;n_heads&quot;</span>: <span class="number">25</span>&#125;,</span><br><span class="line">&#125;</span><br><span class="line">BASE_CONFIG.update(model_configs[CHOOSE_MODEL])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> train_dataset.max_length &lt;= BASE_CONFIG[<span class="string">&quot;context_length&quot;</span>], (</span><br><span class="line">    <span class="string">f&quot;Dataset length <span class="subst">&#123;train_dataset.max_length&#125;</span> exceeds model&#x27;s context &quot;</span></span><br><span class="line">    <span class="string">f&quot;length <span class="subst">&#123;BASE_CONFIG[<span class="string">&#x27;context_length&#x27;</span>]&#125;</span>. Reinitialize data sets with &quot;</span></span><br><span class="line">    <span class="string">f&quot;`max_length=<span class="subst">&#123;BASE_CONFIG[<span class="string">&#x27;context_length&#x27;</span>]&#125;</span>`&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>接下来，我们从第 5 章下载的 <code>gpt_download.py</code> 文件中导入 <code>download_and_load_gpt2</code> 函数。同时，我们还可以复用第 5 章中的 <code>GPTModel</code> 类和 <code>load_weights_into_gpt</code> 函数，将下载的权重加载到 GPT 模型中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 6.6 Loading a pretrained GPT model</span></span><br><span class="line"><span class="keyword">from</span> gpt_download <span class="keyword">import</span> download_and_load_gpt2</span><br><span class="line"><span class="keyword">from</span> chapter05 <span class="keyword">import</span> GPTModel, load_weights_into_gpt</span><br><span class="line"></span><br><span class="line">model_size = CHOOSE_MODEL.split(<span class="string">&quot; &quot;</span>)[-<span class="number">1</span>].lstrip(<span class="string">&quot;(&quot;</span>).rstrip(<span class="string">&quot;)&quot;</span>)</span><br><span class="line">settings, params = download_and_load_gpt2(model_size=model_size, models_dir=<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line"></span><br><span class="line">model = GPTModel(BASE_CONFIG)</span><br><span class="line">load_weights_into_gpt(model, params)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<p>在将模型权重加载到<code>GPTModel</code>后，我们使用前面章节的文本生成工具函数，确保模型能够生成连贯的文本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> chapter04 <span class="keyword">import</span> generate_text_simple</span><br><span class="line"><span class="keyword">from</span> chapter05 <span class="keyword">import</span> text_to_token_ids, token_ids_to_text</span><br><span class="line"></span><br><span class="line">text_1 = <span class="string">&quot;Every effort moves you&quot;</span></span><br><span class="line">token_ids = generate_text_simple(</span><br><span class="line">    model=model,</span><br><span class="line">    idx=text_to_token_ids(text_1, tokenizer),</span><br><span class="line">    max_new_tokens=<span class="number">15</span>,</span><br><span class="line">    context_size=BASE_CONFIG[<span class="string">&quot;context_length&quot;</span>]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(token_ids_to_text(token_ids, tokenizer))</span><br></pre></td></tr></table></figure>
<p>从以下输出可以看出，模型生成了连贯的文本，这表明模型权重已正确加载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Every effort moves you forward.</span><br><span class="line">The first step <span class="keyword">is</span> to understand the importance of your work</span><br></pre></td></tr></table></figure>
<p>现在，在我们开始将模型微调为垃圾短信分类器之前，我们先来看看这个模型是否能通过给它提供指令来对垃圾短信进行分类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">text_2 = (</span><br><span class="line">    <span class="string">&quot;Is the following text &#x27;spam&#x27;? Answer with &#x27;yes&#x27; or &#x27;no&#x27;:&quot;</span></span><br><span class="line">    <span class="string">&quot; &#x27;You are a winner you have been specially&quot;</span></span><br><span class="line">    <span class="string">&quot; selected to receive $1000 cash or a $2000 award.&#x27;&quot;</span></span><br><span class="line">)</span><br><span class="line">token_ids = generate_text_simple(</span><br><span class="line">    model=model,</span><br><span class="line">    idx=text_to_token_ids(text_2, tokenizer),</span><br><span class="line">    max_new_tokens=<span class="number">23</span>,</span><br><span class="line">    context_size=BASE_CONFIG[<span class="string">&quot;context_length&quot;</span>]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(token_ids_to_text(token_ids, tokenizer))</span><br></pre></td></tr></table></figure>
<p>模型输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Is the following text <span class="string">&#x27;spam&#x27;</span>? Answer <span class="keyword">with</span> <span class="string">&#x27;yes&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;no&#x27;</span>: <span class="string">&#x27;You are a winner you have been</span></span><br><span class="line"><span class="string">specially selected to receive $1000 cash or a $2000 award.&#x27;</span></span><br><span class="line">The following text <span class="string">&#x27;spam&#x27;</span>? Answer <span class="keyword">with</span> <span class="string">&#x27;yes&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;no&#x27;</span>: <span class="string">&#x27;You are a winner</span></span><br></pre></td></tr></table></figure>
<p>根据输出结果，可以看到模型还不具备遵循指令方面的能力。</p>
<p>这是预料之中的，因为它仅经过了预训练，缺乏指令微调，我们将在下一章探讨这个问题。</p>
<p>下一节开始为模型的分类微调做准备。</p>
<h2 id="6-5-添加分类头"><a href="#6-5-添加分类头" class="headerlink" title="6.5 添加分类头"></a>6.5 添加分类头</h2><p>本节我们将修改预训练的模型，为分类任务的微调做准备。为此，我们需要替换原始输出层，原输出层将隐层表示映射到50,257个词汇的词汇表，而我们用一个较小的输出层将其映射到两个类别：0（‘非垃圾短信’）和1（‘垃圾短信’），如图6.9所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.9.png" alt=""></p>
<p>如图 6.9 所示，我们使用与前几章相同的模型，唯一的不同是替换了输出层。</p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>输出层节点</strong></p>
<p>理论上，由于我们处理的是二分类任务，可以使用单个输出节点。然而，这需要修改损失函数，具体内容可以参见附录B的参考部分。因此，我们选择一个更通用的方法，即输出节点数与类别数相匹配。例如，对于一个三分类问题，如将新闻文章分类为“技术”、“体育”或“政治”，我们使用三个输出节点，以此类推。</p>
</blockquote>
<p>在我们尝试图 6.9 中展示的修改之前，先通过 <code>print(model)</code> 打印模型架构，结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">GPTModel(</span><br><span class="line">  (tok_emb): Embedding(<span class="number">50257</span>, <span class="number">768</span>)</span><br><span class="line">  (pos_emb): Embedding(<span class="number">1024</span>, <span class="number">768</span>)</span><br><span class="line">  (drop_emb): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  (trf_blocks): Sequential(</span><br><span class="line">...</span><br><span class="line">    (<span class="number">11</span>): TransformerBlock(</span><br><span class="line">      (att): MultiHeadAttention(</span><br><span class="line">        (W_query): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (W_key): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (W_value): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (out_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (dropout): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">     (ff): FeedForward(</span><br><span class="line">       (layers): Sequential(</span><br><span class="line">         (<span class="number">0</span>): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">         (<span class="number">1</span>): GELU()</span><br><span class="line">         (<span class="number">2</span>): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">       )</span><br><span class="line">     )</span><br><span class="line">     (norm1): LayerNorm()</span><br><span class="line">     (norm2): LayerNorm()</span><br><span class="line">     (drop_resid): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (final_norm): LayerNorm()</span><br><span class="line">  (out_head): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">50257</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>上图清晰展示了我们在第 4 章实现的架构：GPT 模型由嵌入层、12 个相同的 Transformer 模块（出于简洁考虑，只展示了最后一个模块）构成，接着是最终的 LayerNorm 层和输出层（out_head）。</p>
<p>接下来我们将用一个新的输出层替换原始输出层（见图 6.9），并对其进行微调。</p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>微调部分层与全部层的对比</strong></p>
<p>由于我们从预训练模型开始，并不需要对所有模型层进行微调。这是因为，在基于神经网络的语言模型中，低层通常捕捉到的是基本的语言结构和语义，这些特征适用于多种任务和数据集。因此，只微调最后几层（接近输出层），它们更专注于细致的语言模式和任务特定的特征，通常就足够使模型适应新任务。此外，微调较少的层在计算上也更加高效。对于有兴趣的读者，可以在附录B的参考部分找到更多关于微调哪些层的详细信息，包括相关实验。</p>
</blockquote>
<p>为了让模型准备好进行分类微调，我们首先通过将所有层设为不可训练来冻结模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>接着，按照图 6.9 所示，我们替换掉输出层（model.out_head），该层原本将层输入映射到 50,257 维空间（即词汇表大小）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 6.7 Adding a classification layer</span></span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">num_classes = <span class="number">2</span></span><br><span class="line">model.out_head = torch.nn.Linear(</span><br><span class="line">    in_features=BASE_CONFIG[<span class="string">&quot;emb_dim&quot;</span>],</span><br><span class="line">    out_features=num_classes</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>请注意，在上述代码中我们使用了 <code>BASE_CONFIG[&quot;emb_dim&quot;]</code>，在 <code>gpt2-small (124M)</code> 模型中它的值为 768，这样可以让后续代码更加通用，便于适配更大的 GPT-2 模型变体。</p>
<p>这个新的输出层 <code>model.out_head</code> 的 <code>requires_grad</code> 属性默认为 <code>True</code>，意味着它是模型训练过程中唯一会被更新的层。</p>
<p>从技术上讲，训练我们刚添加的输出层已经足够。然而，通过实验我发现，微调更多层能够显著提升微调后模型的预测性能（更多细节请参考附录 C 中的参考文献）。</p>
<p>此外，我们还需将最后一个 Transformer 模块以及连接该模块和输出层的 LayerNorm 模块配置为可训练，如图6.10所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.10.png" alt=""></p>
<p>为了让最终的 LayerNorm 和最后一个 Transformer 模块参与训练（如图 6.10 所示），我们将它们的 <code>requires_grad</code> 设置为 <code>True：</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.trf_blocks[-<span class="number">1</span>].parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">True</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.final_norm.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>[!NOTE]</p>
<p><strong>微调整个模型</strong></p>
<p>与仅微调最后一个 Transformer 模块相比，可以微调整个模型并评估其对预测性能的影响。</p>
</blockquote>
<p>尽管我们增加了一个新的输出层，并标记了某些层为可训练或不可训练，我们仍然可以像前几章那样使用这个模型。例如，我们可以像以前一样向模型输入一个示例文本。考虑以下示例文本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inputs = tokenizer.encode(<span class="string">&quot;Do you have time&quot;</span>)</span><br><span class="line">inputs = torch.tensor(inputs).unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Inputs:&quot;</span>, inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Inputs dimensions:&quot;</span>, inputs.shape) <span class="comment"># shape: (batch_size, num_tokens)</span></span><br></pre></td></tr></table></figure>
<p>从输出结果可以看出，前面的代码将输入编码成了一个包含 4 个输入 token 的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Inputs: tensor([[<span class="number">5211</span>, <span class="number">345</span>, <span class="number">423</span>, <span class="number">640</span>]])</span><br><span class="line">Inputs dimensions: torch.Size([<span class="number">1</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<p>接着，我们将编码后的 token ID 直接传入模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">outputs = model(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Outputs:\n&quot;</span>, outputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Outputs dimensions:&quot;</span>, outputs.shape)  <span class="comment"># shape: (batch_size, num_tokens, num_classes)</span></span><br></pre></td></tr></table></figure>
<p>输出张量如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Outputs:</span><br><span class="line">  tensor([[[-<span class="number">1.5854</span>, <span class="number">0.9904</span>],</span><br><span class="line">           [-<span class="number">3.7235</span>, <span class="number">7.4548</span>],</span><br><span class="line">           [-<span class="number">2.2661</span>, <span class="number">6.6049</span>],</span><br><span class="line">           [-<span class="number">3.5983</span>, <span class="number">3.9902</span>]]])</span><br><span class="line">Outputs dimensions: torch.Size([<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>在第 4 章和第 5 章中，相似的输入会生成形状为 [1, 4, 50257] 的输出张量，其中 50,257 表示词汇表大小。与前几章相同，输出张量的行数对应输入的 token 数量（在这里是 4 个）。不过，由于替换了模型的输出层，现在每个输出的嵌入维度（即列数）从 50,257 缩减为 2。</p>
<p>请注意，我们希望微调该模型，使其能够输出一个分类标签，用于判断输入是否为垃圾短信。为实现这一点，我们不需要微调所有 4 行输出，只需聚焦于单个输出  token。具体来说，我们将重点关注最后一行对应的输出 token，如图 6.11 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.11.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To extract the last output token, illustrated in figure 6.11, from the output tensor, we use the following code:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Last output token:&quot;</span>, outputs[:, -<span class="number">1</span>, :])</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Last output token: tensor([[-<span class="number">3.5983</span>, <span class="number">3.9902</span>]])</span><br></pre></td></tr></table></figure>
<p>接下来，我们将重点讨论如何将这些值转换为类别标签预测。但在此之前，我们需要理解，为什么我们特别关注最后一个输出的token，而不是第一个、第二个或第三个输出token。</p>
<p>在第 3 章中，我们探讨了注意力机制，该机制在每个输入 token 与其他所有输入 token 之间建立关系。随后，我们引入了因果注意力掩码的概念，这在 GPT 类模型中被广泛使用。这种掩码限制每个 token 的关注范围，使其只能关注当前位置及之前的内容，从而确保每个 token 只能受到自身及前面 token 的影响，如图 6.12 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.12.png" alt=""></p>
<p>在图 6.12 所示的因果注意力掩码设置中，序列中的最后一个 token 聚合了所有前面 token 的信息。因此，在垃圾短信分类任务的微调过程中，我们会重点关注这个最后的 token。</p>
<p>在修改模型后，接下来将详细介绍如何将最后一个 token 转换为分类标签预测，并计算模型的初始预测准确率。之后，我们将在后续部分对模型进行垃圾短信分类任务的微调。</p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>第一个 token 与最后一个 token 的微调对比</strong></p>
<p>尝试微调第一个输出 token，而不是最后一个输出 token，并在后续章节的模型微调实验中观察预测性能的变化。</p>
</blockquote>
<h2 id="6-6-计算分类损失和准确率"><a href="#6-6-计算分类损失和准确率" class="headerlink" title="6.6 计算分类损失和准确率"></a>6.6 计算分类损失和准确率</h2><p>本章到目前为止，我们已完成了数据集准备、预训练模型的加载，以及对模型进行分类微调的修改。在微调正式开始前，还剩下一小部分工作：实现微调过程中使用的模型评估函数（如图 6.13 所示）。我们将在本节完成这一部分。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.13.png" alt=""></p>
<p>在实现评估工具之前，我们先简单讨论一下如何将模型输出转换为类别标签预测。</p>
<p>在上一章中，我们通过 softmax 函数将 50,257 个输出转换为概率分布，然后通过 argmax 函数返回概率最高的位置，从而得到 LLM 生成的下一个 token 的 token ID。本章中，我们采用相同的方法来计算模型对于给定输入的预测结果是‘垃圾短信’还是‘正常短信’。唯一的区别是，这次的输出维度是 2，而不是 50,257 维。</p>
<p>模型对每个输入文本的最后一个 token 生成的输出被转换为概率得分。然后，通过查找概率得分中最高值的位置来确定对应的分类标签。请注意，由于模型尚未经过训练，目前对垃圾短信标签的预测是不准确的。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.14.png" alt=""></p>
<p>为了通过具体示例来说明图 6.14，我们来看一下前一节代码示例中的最后一个输出 token：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Last output token:&quot;</span>, outputs[:, -<span class="number">1</span>, :])</span><br></pre></td></tr></table></figure>
<p>以下是最后一个 token 对应的张量值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Last output token: tensor([[-<span class="number">3.5983</span>, <span class="number">3.9902</span>]])</span><br></pre></td></tr></table></figure>
<p>我们可以通过以下代码获取分类标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">probas = torch.softmax(outputs[:, -<span class="number">1</span>, :], dim=-<span class="number">1</span>)</span><br><span class="line">label = torch.argmax(probas)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Class label:&quot;</span>, label.item())</span><br></pre></td></tr></table></figure>
<p>在这种情况下，代码返回 1，表示模型预测输入文本为‘垃圾短信’。这里使用 Softmax 函数是可选的，因为最大的输出值已经对应最高的概率分数（参见第 5 章）。因此，我们可以省略 Softmax 函数，简化代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">logits = outputs[:, -<span class="number">1</span>, :]</span><br><span class="line">label = torch.argmax(logits)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Class label:&quot;</span>, label.item())</span><br></pre></td></tr></table></figure>
<p>这个概念可以用来计算分类准确率，它衡量的是数据集上正确预测的比例。</p>
<p>为了计算分类准确率，我们对数据集中的所有样本进行 argmax 预测，并通过定义一个 <code>calc_accuracy_loader</code> 函数来计算预测正确的比例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 6.8 Calculating the classification accuracy</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_accuracy_loader</span>(<span class="params">data_loader, model, device, num_batches=<span class="literal">None</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct_predictions, num_examples = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> num_batches <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        num_batches = <span class="built_in">len</span>(data_loader)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        num_batches = <span class="built_in">min</span>(num_batches, <span class="built_in">len</span>(data_loader))</span><br><span class="line">    <span class="keyword">for</span> i, (input_batch, target_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        <span class="keyword">if</span> i &lt; num_batches:</span><br><span class="line">            input_batch, target_batch = input_batch.to(device), target_batch.to(device)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                logits = model(input_batch)[:, -<span class="number">1</span>, :]                   <span class="comment">#A</span></span><br><span class="line">            predicted_labels = torch.argmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            num_examples += predicted_labels.shape[<span class="number">0</span>]</span><br><span class="line">            correct_predictions += (predicted_labels == target_batch).<span class="built_in">sum</span>().item()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> correct_predictions / num_examples</span><br><span class="line"></span><br><span class="line"><span class="comment">#A 最后一个输出 token 的 logits 值</span></span><br></pre></td></tr></table></figure>
<p>我们可以使用这个函数来估算多个数据集上的分类准确率，为提高效率，这里基于 10 个批次的结果进行估算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=<span class="number">10</span>)</span><br><span class="line">val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=<span class="number">10</span>)</span><br><span class="line">test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training accuracy: <span class="subst">&#123;train_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Validation accuracy: <span class="subst">&#123;val_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test accuracy: <span class="subst">&#123;test_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>通过设置<code>device</code>属性，如果检测到支持 Nvidia CUDA 的 GPU，模型会自动在 GPU 上运行，否则会在 CPU 上运行。输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Training accuracy: <span class="number">46.25</span>%</span><br><span class="line">Validation accuracy: <span class="number">45.00</span>%</span><br><span class="line">Test accuracy: <span class="number">48.75</span>%</span><br></pre></td></tr></table></figure>
<p>可以看到，当前模型的预测准确率接近随机预测（在本例中为 50%）。为了提高预测准确率，我们需要对模型进行微调。</p>
<p>在微调模型之前，我们需要定义损失函数，以便在训练过程中对其进行优化。我们的目标是最大化模型的垃圾短信分类准确率，因此代码输出应为正确的类别标签：0 表示正常短信，1 表示垃圾短信。</p>
<p>然而，由于分类准确率不是一个可微分的函数，因此我们使用交叉熵损失作为替代来优化准确率。这里所说的交叉熵损失与第 5 章讨论的一致。</p>
<p>因此，<code>calc_loss_batch</code> 函数与第五章中的版本基本相同，唯一的调整是：我们只优化最后一个 token（<code>model(input_batch)[:, -1, :]</code>），而不是整个序列中的所有 token（<code>model(input_batch)</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss_batch</span>(<span class="params">input_batch, target_batch, model, device</span>):</span><br><span class="line">    input_batch, target_batch = input_batch.to(device), target_batch.to(device)</span><br><span class="line">    logits = model(input_batch)[:, -<span class="number">1</span>, :] <span class="comment"># Logits of last output token</span></span><br><span class="line">    loss = torch.nn.functional.cross_entropy(logits, target_batch)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>我们使用 <code>calc_loss_batch</code> 函数来计算从前面定义的数据加载器获取的单个批次的损失。为了计算数据加载器中所有批次的损失，我们定义了 <code>calc_loss_loader</code> 函数，其功能与第五章中的描述相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 6.9 Calculating the classification loss</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss_loader</span>(<span class="params">data_loader, model, device, num_batches=<span class="literal">None</span></span>):</span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_loader) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">float</span>(<span class="string">&quot;nan&quot;</span>)</span><br><span class="line">    <span class="keyword">elif</span> num_batches <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        num_batches = <span class="built_in">len</span>(data_loader)</span><br><span class="line">    <span class="keyword">else</span>:                                      <span class="comment">#A</span></span><br><span class="line">        num_batches = <span class="built_in">min</span>(num_batches, <span class="built_in">len</span>(data_loader))</span><br><span class="line">    <span class="keyword">for</span> i, (input_batch, target_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        <span class="keyword">if</span> i &lt; num_batches:</span><br><span class="line">            loss = calc_loss_batch(input_batch, target_batch, model, device)</span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> total_loss / num_batches</span><br><span class="line"></span><br><span class="line"><span class="comment"># Similar to calculating the training accuracy, we now compute the initial loss for each data set:</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():                           <span class="comment">#B</span></span><br><span class="line">    train_loss = calc_loss_loader(train_loader, model, device, num_batches=<span class="number">5</span>)</span><br><span class="line">    val_loss = calc_loss_loader(val_loader, model, device, num_batches=<span class="number">5</span>)</span><br><span class="line">    test_loss = calc_loss_loader(test_loader, model, device, num_batches=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 确保批次数不超过数据加载器中的总批次数</span></span><br><span class="line"><span class="comment">#B 关闭梯度追踪以提高效率，因为当前未进行训练</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Validation loss: <span class="subst">&#123;val_loss:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line">The initial loss values are <span class="keyword">as</span> follows:</span><br><span class="line">Training loss: <span class="number">3.095</span></span><br><span class="line">Validation loss: <span class="number">2.583</span></span><br><span class="line">Test loss: <span class="number">2.322</span></span><br></pre></td></tr></table></figure>
<p>在下一节，我们将实现一个训练函数来微调模型，实现最小化训练集损失。最小化训练集损失将有助于提高分类准确性，这是我们的总体目标。</p>
<h2 id="6-7-使用监督数据对模型进行微调"><a href="#6-7-使用监督数据对模型进行微调" class="headerlink" title="6.7 使用监督数据对模型进行微调"></a>6.7 使用监督数据对模型进行微调</h2><p>在本节中，我们定义并使用训练函数，对预训练的 LLM 进行微调，以提升其垃圾短信分类的准确率。训练循环的整体结构与第 5 章中的相同（详见图 6.15），唯一的区别在于，这里计算的是分类准确率，而不是通过生成文本来评估模型。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.15.png" alt=""></p>
<p>可以看到，图 6.15 中所示的训练函数逻辑，与第 5 章中用于模型预训练的 <code>train_model_simple</code> 函数非常相似。</p>
<p>唯一的两个区别在于：现在记录的是训练样本数量（examples_seen），而不是 token 数量；并且在每个 epoch 后计算准确率，而不再打印示例文本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 6.10 Finetuning the model to classify spam</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_classifier_simple</span>(<span class="params">model, train_loader, val_loader, optimizer, device,</span></span><br><span class="line"><span class="params">num_epochs, eval_freq, eval_iter, tokenizer</span>):</span><br><span class="line">    <span class="comment"># Initialize lists to track losses and examples seen</span></span><br><span class="line">    train_losses, val_losses, train_accs, val_accs = [], [], [], []</span><br><span class="line">    examples_seen, global_step = <span class="number">0</span>, -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Main training loop</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        model.train()                                      <span class="comment">#A</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> input_batch, target_batch <span class="keyword">in</span> train_loader:</span><br><span class="line">            optimizer.zero_grad()                          <span class="comment">#B</span></span><br><span class="line">            loss = calc_loss_batch(input_batch, target_batch, model, device)</span><br><span class="line">            loss.backward()                                <span class="comment">#C</span></span><br><span class="line">            optimizer.step()                               <span class="comment">#D</span></span><br><span class="line">            examples_seen += input_batch.shape[<span class="number">0</span>]          <span class="comment">#E</span></span><br><span class="line">            global_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> global_step % eval_freq == <span class="number">0</span>:               <span class="comment">#F</span></span><br><span class="line">                train_loss, val_loss = evaluate_model(</span><br><span class="line">                    model, train_loader, val_loader, device, eval_iter)</span><br><span class="line">                train_losses.append(train_loss)</span><br><span class="line">                val_losses.append(val_loss)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Ep <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> (Step <span class="subst">&#123;global_step:06d&#125;</span>): &quot;</span></span><br><span class="line">                      <span class="string">f&quot;Train loss <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span>, Val loss <span class="subst">&#123;val_loss:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        train_accuracy = calc_accuracy_loader(             <span class="comment">#G</span></span><br><span class="line">            train_loader, model, device, num_batches=eval_iter</span><br><span class="line">        )</span><br><span class="line">        val_accuracy = calc_accuracy_loader(</span><br><span class="line">            val_loader, model, device, num_batches=eval_iter</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Training accuracy: <span class="subst">&#123;train_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>% | &quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Validation accuracy: <span class="subst">&#123;val_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">        train_accs.append(train_accuracy)</span><br><span class="line">        val_accs.append(val_accuracy)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_losses, val_losses, train_accs, val_accs, examples_seen</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 设置模型为训练模式</span></span><br><span class="line"><span class="comment">#B 重置上一批次的损失梯度</span></span><br><span class="line"><span class="comment">#C 计算损失梯度</span></span><br><span class="line"><span class="comment">#D 使用损失梯度更新模型权重</span></span><br><span class="line"><span class="comment">#E 更改逻辑：跟踪样本数量而非 token 数量</span></span><br><span class="line"><span class="comment">#F 可选评估步骤</span></span><br><span class="line"><span class="comment">#G 每个 epoch 后计算准确率</span></span><br></pre></td></tr></table></figure>
<p>以上 <code>train_classifier_simple</code> 中使用的 <code>evaluate_model</code> 函数与我们在第 5 章中使用的函数相同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_model</span>(<span class="params">model, train_loader, val_loader, device, eval_iter</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)</span><br><span class="line">        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">return</span> train_loss, val_loss</span><br></pre></td></tr></table></figure>
<p>接下来，我们初始化优化器，设置训练轮数，并通过 <code>train_classifier_simple</code> 函数启动训练。关于训练轮数的选择将在评估结果后讨论。在 M3 MacBook Air 上训练大约需要 6 分钟，而在 V100 或 A100 GPU 上则不到半分钟：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">5e-5</span>, weight_decay=<span class="number">0.1</span>)</span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(</span><br><span class="line">    model, train_loader, val_loader, optimizer, device,</span><br><span class="line">    num_epochs=num_epochs, eval_freq=<span class="number">50</span>, eval_iter=<span class="number">5</span>,</span><br><span class="line">    tokenizer=tokenizer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">end_time = time.time()</span><br><span class="line">execution_time_minutes = (end_time - start_time) / <span class="number">60</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training completed in <span class="subst">&#123;execution_time_minutes:<span class="number">.2</span>f&#125;</span> minutes.&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>训练过程中的输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Ep <span class="number">1</span> (Step <span class="number">000000</span>): Train loss <span class="number">2.153</span>, Val loss <span class="number">2.392</span></span><br><span class="line">Ep <span class="number">1</span> (Step 000050): Train loss <span class="number">0.617</span>, Val loss <span class="number">0.637</span></span><br><span class="line">Ep <span class="number">1</span> (Step <span class="number">000</span>100): Train loss <span class="number">0.523</span>, Val loss <span class="number">0.557</span></span><br><span class="line">Training accuracy: <span class="number">70.00</span>% | Validation accuracy: <span class="number">72.50</span>%</span><br><span class="line">Ep <span class="number">2</span> (Step 000150): Train loss <span class="number">0.561</span>, Val loss <span class="number">0.489</span></span><br><span class="line">Ep <span class="number">2</span> (Step 000200): Train loss <span class="number">0.419</span>, Val loss <span class="number">0.397</span></span><br><span class="line">Ep <span class="number">2</span> (Step 000250): Train loss <span class="number">0.409</span>, Val loss <span class="number">0.353</span></span><br><span class="line">Training accuracy: <span class="number">82.50</span>% | Validation accuracy: <span class="number">85.00</span>%</span><br><span class="line">Ep <span class="number">3</span> (Step 000300): Train loss <span class="number">0.333</span>, Val loss <span class="number">0.320</span></span><br><span class="line">Ep <span class="number">3</span> (Step 000350): Train loss <span class="number">0.340</span>, Val loss <span class="number">0.306</span></span><br><span class="line">Training accuracy: <span class="number">90.00</span>% | Validation accuracy: <span class="number">90.00</span>%</span><br><span class="line">Ep <span class="number">4</span> (Step 000400): Train loss <span class="number">0.136</span>, Val loss <span class="number">0.200</span></span><br><span class="line">Ep <span class="number">4</span> (Step 000450): Train loss <span class="number">0.153</span>, Val loss <span class="number">0.132</span></span><br><span class="line">Ep <span class="number">4</span> (Step 000500): Train loss <span class="number">0.222</span>, Val loss <span class="number">0.137</span></span><br><span class="line">Training accuracy: <span class="number">100.00</span>% | Validation accuracy: <span class="number">97.50</span>%</span><br><span class="line">Ep <span class="number">5</span> (Step 000550): Train loss <span class="number">0.207</span>, Val loss <span class="number">0.143</span></span><br><span class="line">Ep <span class="number">5</span> (Step 000600): Train loss <span class="number">0.083</span>, Val loss <span class="number">0.074</span></span><br><span class="line">Training accuracy: <span class="number">100.00</span>% | Validation accuracy: <span class="number">97.50</span>%</span><br><span class="line">Training completed <span class="keyword">in</span> <span class="number">5.65</span> minutes.</span><br></pre></td></tr></table></figure>
<p>类似于第 5 章的做法，我们使用 matplotlib 绘制训练集和验证集的损失函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 6.11 Plotting the classification loss</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">plot_values</span>(<span class="params">epochs_seen, examples_seen, train_values, val_values, label=<span class="string">&quot;loss&quot;</span></span>):</span><br><span class="line">    fig, ax1 = plt.subplots(figsize=(<span class="number">5</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    ax1.plot(epochs_seen, train_values, label=<span class="string">f&quot;Training <span class="subst">&#123;label&#125;</span>&quot;</span>)    <span class="comment">#A</span></span><br><span class="line">    ax1.plot(epochs_seen, val_values, linestyle=<span class="string">&quot;-.&quot;</span>, label=<span class="string">f&quot;Validation <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br><span class="line">    ax1.set_xlabel(<span class="string">&quot;Epochs&quot;</span>)</span><br><span class="line">    ax1.set_ylabel(label.capitalize())</span><br><span class="line">    ax1.legend()</span><br><span class="line"></span><br><span class="line">    ax2 = ax1.twiny()                                                 <span class="comment">#B</span></span><br><span class="line">    ax2.plot(examples_seen, train_values, alpha=<span class="number">0</span>) <span class="comment"># Invisible plot for aligning ticks</span></span><br><span class="line">    ax2.set_xlabel(<span class="string">&quot;Examples seen&quot;</span>)</span><br><span class="line"></span><br><span class="line">    fig.tight_layout()                                                <span class="comment">#C</span></span><br><span class="line">    plt.savefig(<span class="string">f&quot;<span class="subst">&#123;label&#125;</span>-plot.pdf&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 绘制训练轮次与训练和验证损失的变化图</span></span><br><span class="line"><span class="comment">#B 创建一个新的 x 轴，用于显示已处理样本数</span></span><br><span class="line"><span class="comment">#C 调整布局以留出空间</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epochs_tensor = torch.linspace(<span class="number">0</span>, num_epochs, <span class="built_in">len</span>(train_losses))</span><br><span class="line">examples_seen_tensor = torch.linspace(<span class="number">0</span>, examples_seen, <span class="built_in">len</span>(train_losses))</span><br><span class="line">plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)</span><br></pre></td></tr></table></figure>
<p>图6.16展示了最终的损失曲线。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.16.png" alt=""></p>
<p>从图 6.16 中陡峭的下降曲线可以看出，模型在训练数据上的学习效果很好，且没有明显的过拟合迹象，训练集和验证集的损失值几乎没有差距。</p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>选择训练轮数</strong></p>
<p>在训练开始时，我们将 epoch 数量设置为 5。epoch 的具体数量取决于数据集和任务的难度，并没有通用的解决方案或推荐值。5 个 epoch 通常是一个合适的起点。如果在前几个 epoch 后模型出现过拟合迹象（如图 6.16 所示的损失曲线显示验证损失上升），我们可能需要减少 epoch 数量。相反，如果趋势线显示验证损失随着训练仍有下降空间，我们则应增加 epoch 数量。在本例中，5 个 epoch 是合理的选择，因为没有早期过拟合的迹象，且验证损失接近 0。</p>
</blockquote>
<p>接下来，继续使用 plot_values 函数绘制分类准确率的图表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epochs_tensor = torch.linspace(<span class="number">0</span>, num_epochs, <span class="built_in">len</span>(train_accs))</span><br><span class="line">examples_seen_tensor = torch.linspace(<span class="number">0</span>, examples_seen, <span class="built_in">len</span>(train_accs))</span><br><span class="line"></span><br><span class="line">plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line">The resulting accuracy graphs are shown <span class="keyword">in</span> figure <span class="number">6.17</span>.</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.17.png" alt=""></p>
<p>从图 6.17 的准确率曲线可以看出，模型在第 4 到 5 个训练周期后，训练和验证准确率均达到了较高水平。</p>
<p>需要注意的是，我们之前在使用 <code>train_classifier_simple</code> 函数时将 <code>eval_iter</code> 设置为 5，这意味着我们的训练和验证性能估计仅基于 5 个批次，目的是为了提高训练效率。</p>
<p>现在，我们将通过运行以下代码，计算整个数据集在训练集、验证集和测试集上的性能指标，这次不需要定义 <code>eval_iter</code> 值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_accuracy = calc_accuracy_loader(train_loader, model, device)</span><br><span class="line">val_accuracy = calc_accuracy_loader(val_loader, model, device)</span><br><span class="line">test_accuracy = calc_accuracy_loader(test_loader, model, device)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training accuracy: <span class="subst">&#123;train_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Validation accuracy: <span class="subst">&#123;val_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test accuracy: <span class="subst">&#123;test_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>由此得到的准确率值如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Training accuracy: <span class="number">97.21</span>%</span><br><span class="line">Validation accuracy: <span class="number">97.32</span>%</span><br><span class="line">Test accuracy: <span class="number">95.67</span>%</span><br></pre></td></tr></table></figure>
<p>可以看到，训练集和测试集的表现几乎相同。</p>
<p>训练集和测试集准确率之间的轻微差异表明训练数据的过拟合程度较低。通常，验证集的准确率会略高于测试集的准确率，这是因为模型开发过程中通常会通过调整超参数来优化验证集上的表现，而这种优化未必能有效地泛化到测试集上。</p>
<p>这种情况很常见，但可以通过调整模型设置来减小这种差距，比如增加 dropout 率（<code>drop_rate</code>）或优化器配置中的权重衰减（<code>weight_decay</code>）参数。</p>
<h2 id="6-8-将-LLM-用于垃圾短信分类"><a href="#6-8-将-LLM-用于垃圾短信分类" class="headerlink" title="6.8 将 LLM 用于垃圾短信分类"></a>6.8 将 LLM 用于垃圾短信分类</h2><p>在前几节对模型进行微调和评估后，我们现在进入本章的最后阶段（见图 6.18）：使用模型进行垃圾短信分类。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter6/figure6.18.png" alt=""></p>
<p>最后，我们将使用微调后的基于 GPT 的垃圾短信分类模型。以下的 <code>classify_review</code> 函数遵循了与本章之前实现的 <code>SpamDataset</code> 类似的数据预处理步骤。函数先将文本处理为 token ID，然后使用模型预测一个整数类别标签（与 6.6 节中的实现类似），并返回对应的类别名称：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 6.12 Using the model to classify new texts</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify_review</span>(<span class="params">text, model, tokenizer, device, max_length=<span class="literal">None</span>, pad_token_id=<span class="number">50256</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    input_ids = tokenizer.encode(text)                                   <span class="comment">#A</span></span><br><span class="line">    supported_context_length = model.pos_emb.weight.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    input_ids = input_ids[:<span class="built_in">min</span>(max_length, supported_context_length)]    <span class="comment">#B</span></span><br><span class="line"></span><br><span class="line">    input_ids += [pad_token_id] * (max_length - <span class="built_in">len</span>(input_ids))          <span class="comment">#C</span></span><br><span class="line">    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(<span class="number">0</span>)   <span class="comment">#D</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():                                                <span class="comment">#E</span></span><br><span class="line">        logits = model(input_tensor)[:, -<span class="number">1</span>, :]                           <span class="comment">#F</span></span><br><span class="line">    predicted_label = torch.argmax(logits, dim=-<span class="number">1</span>).item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;spam&quot;</span> <span class="keyword">if</span> predicted_label == <span class="number">1</span> <span class="keyword">else</span> <span class="string">&quot;not spam&quot;</span>                <span class="comment">#G</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 准备模型输入</span></span><br><span class="line"><span class="comment">#B 截断过长序列</span></span><br><span class="line"><span class="comment">#C 填充序列至最长长度</span></span><br><span class="line"><span class="comment">#D 增加批次维度</span></span><br><span class="line"><span class="comment">#E 关闭梯度跟踪，进行模型推理</span></span><br><span class="line"><span class="comment">#F 获取最后一个输出 token 的 logits</span></span><br><span class="line"><span class="comment">#G 返回分类结果</span></span><br></pre></td></tr></table></figure>
<p>我们来试试用示例文本测试 classify_review 函数的效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">text_1 = (</span><br><span class="line">    <span class="string">&quot;You are a winner you have been specially&quot;</span></span><br><span class="line">    <span class="string">&quot; selected to receive $1000 cash or a $2000 award.&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classify_review(</span><br><span class="line">    text_1, model, tokenizer, device, max_length=train_dataset.max_length</span><br><span class="line">))</span><br></pre></td></tr></table></figure>
<p>训练得到的模型正确预测了‘spam’。接下来，让我们尝试另一个示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">text_2 = (</span><br><span class="line">    <span class="string">&quot;Hey, just wanted to check if we&#x27;re still on&quot;</span></span><br><span class="line">    <span class="string">&quot; for dinner tonight? Let me know!&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(classify_review(</span><br><span class="line">    text_2, model, tokenizer, device, max_length=train_dataset.max_length</span><br><span class="line">))</span><br></pre></td></tr></table></figure>
<p>这个实例也一样，模型做出了正确预测并返回了‘非垃圾短信’标签。</p>
<p>最后，为了方便后续重复使用模型，避免再次训练，我们可以使用上一章介绍的 <code>torch.save</code> 方法来保存模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&quot;review_classifier.pth&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>保存后，可以按如下方式加载模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_state_dict = torch.load(<span class="string">&quot;review_classifier.pth&quot;</span>)</span><br><span class="line">model.load_state_dict(model_state_dict)</span><br></pre></td></tr></table></figure>
<h2 id="6-9-本章摘要"><a href="#6-9-本章摘要" class="headerlink" title="6.9 本章摘要"></a>6.9 本章摘要</h2><ul>
<li>微调 LLM 有不同的策略，包括分类微调（本章）和指令微调（下一章）。</li>
<li>分类微调是指将 LLM 的输出层替换为一个小型的分类层。</li>
<li>在将文本消息分类为‘垃圾短信’或‘非垃圾短信’的任务中，新的分类层只需要 2 个输出节点；而在之前的章节中，输出节点的数量等于词汇表中的唯一 token 数量，即 50,256。</li>
<li>分类微调任务不是像预训练那样预测下一个词，而是训练模型输出正确的类别标签，例如‘垃圾短信’或‘非垃圾短信’。</li>
<li>在微调阶段，模型的输入是转换为 token ID 的文本，这与预训练阶段类似。</li>
<li>在微调 LLM 之前，我们会加载预训练模型作为基础。</li>
<li>评估分类模型需要计算分类准确率，即正确预测的比例。</li>
<li>微调分类模型时使用的交叉熵损失函数与预训练 LLM 时相同。</li>
</ul>
</div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/my-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">欣冻</div><div class="author-info-description">博客, 技术, 生活</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/6f4fa4e7.html" title="快速幂、逆元与组合数学"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/10/31/7I8gnp.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="快速幂、逆元与组合数学"/></a><div class="content"><a class="title" href="/posts/6f4fa4e7.html" title="快速幂、逆元与组合数学">快速幂、逆元与组合数学</a><time datetime="2025-10-31T15:48:33.000Z" title="发表于 2025-10-31 23:48:33">2025-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/2f58633e.html" title="常用数据结构"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://origin.picgo.net/2025/10/11/792a8743-6d0d-43fe-91b8-0a5a77b529f4a296a597708421a1.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="常用数据结构"/></a><div class="content"><a class="title" href="/posts/2f58633e.html" title="常用数据结构">常用数据结构</a><time datetime="2025-10-11T11:00:00.000Z" title="发表于 2025-10-11 19:00:00">2025-10-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/7258f8a4.html" title="THYTHM 音游"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/08/01/HotT9.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="THYTHM 音游"/></a><div class="content"><a class="title" href="/posts/7258f8a4.html" title="THYTHM 音游">THYTHM 音游</a><time datetime="2025-08-01T04:00:00.000Z" title="发表于 2025-08-01 12:00:00">2025-08-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/73e7a68a.html" title="力扣每日一题讲解"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/07/24/QEaxN.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="力扣每日一题讲解"/></a><div class="content"><a class="title" href="/posts/73e7a68a.html" title="力扣每日一题讲解">力扣每日一题讲解</a><time datetime="2025-07-24T08:50:00.000Z" title="发表于 2025-07-24 16:50:00">2025-07-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/4729e793.html" title="数据结构入门"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/07/05/qp0G9.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构入门"/></a><div class="content"><a class="title" href="/posts/4729e793.html" title="数据结构入门">数据结构入门</a><time datetime="2025-07-04T16:11:10.000Z" title="发表于 2025-07-05 00:11:10">2025-07-05</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/c%E8%AF%AD%E8%A8%80/"><span class="card-category-list-name">c语言</span><span class="card-category-list-count">1</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%EF%BC%8Cbutterfly/" style="font-size: 1.1em; color: #999">一键部署，butterfly</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" style="font-size: 1.1em; color: #999">每日一题</a> <a href="/tags/python-%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91-%E9%9F%B3%E6%B8%B8/" style="font-size: 1.1em; color: #999">python,游戏开发,音游</a> <a href="/tags/hexo-github-blog-node-js-npm-git-%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2-hexo%E9%83%A8%E7%BD%B2/" style="font-size: 1.1em; color: #999">hexo, github, blog, node.js,npm,git,部署博客,hexo部署</a> <a href="/tags/c%E8%AF%AD%E8%A8%80-%E5%AD%A6%E4%B9%A0/" style="font-size: 1.1em; color: #999">c语言,学习</a> <a href="/tags/%E5%A4%A7%E5%AE%B6%E5%A5%BD%EF%BC%8C%E6%88%91%E6%98%AF%E8%BF%B7%E8%B7%AF%E7%9A%84%E5%B0%8F%E6%9C%8B%E5%8F%8B/" style="font-size: 1.1em; color: #999">大家好，我是迷路的小朋友</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 1.1em; color: #999">算法</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/10/">
            <span class="card-archive-list-date">
              十月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/08/">
            <span class="card-archive-list-date">
              八月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/07/">
            <span class="card-archive-list-date">
              七月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/05/">
            <span class="card-archive-list-date">
              五月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/04/">
            <span class="card-archive-list-date">
              四月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/03/">
            <span class="card-archive-list-date">
              三月 2025
            </span>
            <span class="card-archive-list-count">7</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/02/">
            <span class="card-archive-list-date">
              二月 2025
            </span>
            <span class="card-archive-list-count">7</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">22</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-10-31T13:42:25.148Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://i.imgs.ovh/2025/07/03/qLFy9.png);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By 欣冻</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://blog-twikoo.xindon.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = (el = document, path = location.pathname) => {
    twikoo.init({
      el: el.querySelector('#twikoo-wrap'),
      envId: 'https://blog-twikoo.xindon.top/',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      },
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    

    isShuoshuo && (window.shuoshuoComment.destroyTwikoo = () => {
      if (el.children.length) {
        el.innerHTML = ''
        el.classList.add('no-comment')
      }
    })
  }

  const loadTwikoo = (el, path) => {
    if (typeof twikoo === 'object') setTimeout(() => init(el, path), 0)
    else btf.getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(() => init(el, path))
  }

  if (isShuoshuo) {
    'Twikoo' === 'Twikoo'
      ? window.shuoshuoComment = { loadComment: loadTwikoo }
      : window.loadOtherComment = loadTwikoo
    return
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><div class="aplayer no-destroy" data-id="13348674056" data-server="netease" data-type="playlist"   data-order="list" data-fixed="true" data-preload="auto" data-autoplay="false" data-mutex="true" ></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v 7.3.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v5.2.2" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://www.jsdelivr.com/" style="margin-inline:5px" data-title="本站使用JsDelivr为静态资源提供CDN加速" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.js"></script><script async src="/js/ali_font.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('article-sort-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__slideInRight');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var qweather_key = '0cca502ccc7341c2be6ba09309916622';
  var gaud_map_key = '5653914d2fc43aad14b253ab6cf762b9';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '112.982279,28.19409';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/miku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>