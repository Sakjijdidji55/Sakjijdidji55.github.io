<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>7.指令遵循微调 | 迷路的小朋友</title><meta name="author" content="欣冻"><meta name="copyright" content="欣冻"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="7.指令遵循微调 本章涵盖以下内容：  LLM 指令微调过程概述 为监督式指令微调准备数据集 批量组织指令数据 评估 LLM 通过指令遵循生成的内容质量 评估一个经过指令微调的 LLM    7.指令遵循微调  7.1 指令遵循微调简介 7.2 为监督指令微调准备数据集 7.3 将数据组织成训练批次 7.4 为指令数据集创建数据加载器 7.5 加载预训练的 LLM 7.6 指令微调 LLM 7.7">
<meta property="og:type" content="website">
<meta property="og:title" content="7.指令遵循微调">
<meta property="og:url" content="https://sakjijdidji55.github.io/ai_study/7.%E6%8C%87%E4%BB%A4%E9%81%B5%E5%BE%AA%E5%BE%AE%E8%B0%83.html">
<meta property="og:site_name" content="迷路的小朋友">
<meta property="og:description" content="7.指令遵循微调 本章涵盖以下内容：  LLM 指令微调过程概述 为监督式指令微调准备数据集 批量组织指令数据 评估 LLM 通过指令遵循生成的内容质量 评估一个经过指令微调的 LLM    7.指令遵循微调  7.1 指令遵循微调简介 7.2 为监督指令微调准备数据集 7.3 将数据组织成训练批次 7.4 为指令数据集创建数据加载器 7.5 加载预训练的 LLM 7.6 指令微调 LLM 7.7">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sakjijdidji55.github.io/img/my-icon.png">
<meta property="article:published_time" content="2025-10-26T08:00:00.000Z">
<meta property="article:modified_time" content="2025-10-26T08:24:53.179Z">
<meta property="article:author" content="欣冻">
<meta property="article:tag" content="博客, 技术, 生活, tanxin, tanxin.me, 吃好喝好, 玩好, 睡好, 迷路的小朋友,tanxin55">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sakjijdidji55.github.io/img/my-icon.png"><link rel="shortcut icon" href="/img/logo.ico"><link rel="canonical" href="https://sakjijdidji55.github.io/ai_study/7.%E6%8C%87%E4%BB%A4%E9%81%B5%E5%BE%AA%E5%BE%AE%E8%B0%83.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '7.指令遵循微调',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'page'
}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="迷路的小朋友" type="application/atom+xml">
</head><body><div id="web_bg" style="background-image: url(https://i.imgs.ovh/2025/07/03/qLFy9.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/my-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-home"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-home-page" id="page-header" style="background-image: url(https://img.picgo.net/2025/04/05/2025-2-22fe10c0c4fb1bc202.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.png" alt="Logo"><span class="site-name">迷路的小朋友</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-home"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="page-site-info"><h1 id="site-title">7.指令遵循微调</h1></div></header><main class="layout" id="content-inner"><div id="page"><div class="container" id="article-container"><h1>7.指令遵循微调</h1>
<p>本章涵盖以下内容：</p>
<ul>
<li><strong>LLM 指令微调过程概述</strong></li>
<li><strong>为监督式指令微调准备数据集</strong></li>
<li><strong>批量组织指令数据</strong></li>
<li><strong>评估 LLM 通过指令遵循生成的内容质量</strong></li>
<li><strong>评估一个经过指令微调的 LLM</strong></li>
</ul>
<hr>
<ul>
<li><a href="#7%E6%8C%87%E4%BB%A4%E9%81%B5%E5%BE%AA%E5%BE%AE%E8%B0%83">7.指令遵循微调</a>
<ul>
<li><a href="#71-%E6%8C%87%E4%BB%A4%E9%81%B5%E5%BE%AA%E5%BE%AE%E8%B0%83%E7%AE%80%E4%BB%8B">7.1 指令遵循微调简介</a></li>
<li><a href="#72-%E4%B8%BA%E7%9B%91%E7%9D%A3%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86">7.2 为监督指令微调准备数据集</a></li>
<li><a href="#73-%E5%B0%86%E6%95%B0%E6%8D%AE%E7%BB%84%E7%BB%87%E6%88%90%E8%AE%AD%E7%BB%83%E6%89%B9%E6%AC%A1">7.3 将数据组织成训练批次</a></li>
<li><a href="#74-%E4%B8%BA%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%99%A8">7.4 为指令数据集创建数据加载器</a></li>
<li><a href="#75-%E5%8A%A0%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84-llm">7.5 加载预训练的 LLM</a></li>
<li><a href="#76-%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83-llm">7.6 指令微调 LLM</a></li>
<li><a href="#77-%E6%8F%90%E5%8F%96%E5%B9%B6%E4%BF%9D%E5%AD%98%E5%93%8D%E5%BA%94">7.7 提取并保存响应</a></li>
<li><a href="#78-%E8%AF%84%E4%BC%B0%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84-llm">7.8 评估指令微调后的 LLM</a></li>
<li><a href="#79-%E7%BB%93%E8%AF%AD">7.9 结语</a>
<ul>
<li><a href="#791-%E6%8E%A5%E4%B8%8B%E6%9D%A5%E5%A6%82%E4%BD%95%E5%81%9A">7.9.1 接下来如何做？</a></li>
<li><a href="#792-%E5%A6%82%E4%BD%95%E5%9C%A8%E5%BF%AB%E9%80%9F%E5%8F%98%E5%8C%96%E7%9A%84%E5%89%8D%E6%B2%BF%E9%A2%86%E5%9F%9F%E4%B8%AD%E4%BF%9D%E6%8C%81%E9%A2%86%E5%85%88">7.9.2 如何在快速变化的前沿领域中保持领先</a></li>
</ul>
</li>
<li><a href="#710-%E6%9C%AC%E7%AB%A0%E6%91%98%E8%A6%81">7.10 本章摘要</a></li>
</ul>
</li>
</ul>
<hr>
<p>在之前的章节中，我们实现了 LLM 架构，完成了预训练，并将外部的预训练权重导入模型。接着，在上一章中，我们专注于对 LLM 进行特定分类任务的微调，即区分出正常短信和垃圾短信。在本章中，我们将介绍如何微调 LLM 以遵循人类指令（见图 7.1），这是开发用于聊天机器人、个人助理和其他对话任务的 LLM 的主要技术之一。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.1.png" alt=""></p>
<p>图 7.1 展示了微调 LLM 的两种主要方式：用于分类任务的微调（步骤 8）和用于指令遵循的微调（步骤 9）。上一章中我们已实现了步骤 8，本章将重点讲解如何使用指令数据集微调 LLM，具体过程将在下一节进一步说明。</p>
<h2 id="7-1-指令遵循微调简介">7.1 指令遵循微调简介</h2>
<p>我们在第 5 章中已了解到对 LLM 的预训练是一种逐词生成的学习过程。预训练后，LLM 将具备根据输入片段补全文本的能力，可以完成句子或生成段落。</p>
<p>然而，预训练的 LLM 在处理如“修正该文本的语法”或“将该文本转换为被动语态”等特定指令时往往表现不佳。我们将在第 7.5 节中详细讨论一个具体示例，演示如何加载预训练模型并基于其进行指令微调。</p>
<p>本章将专注于提升 LLM 遵循指令并生成理想回答的能力，如图 7.2 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.2.png" alt=""></p>
<p>在本章的剩余部分，我们将逐步实现指令微调过程，首先从数据集准备开始，如图 7.3 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.3.png" alt=""></p>
<p>数据集准备是指令微调中的关键环节，本章的大部分内容都将围绕这一过程展开。下一节将开始实现下载和格式化数据集的代码，这是数据集准备过程的第一步（如图 7.3 所示）。</p>
<h2 id="7-2-为监督指令微调准备数据集">7.2 为监督指令微调准备数据集</h2>
<p>在本节中，我们将下载并格式化指令数据集，以便对预训练的 LLM 进行指令微调。该数据集包含 1100 组指令-响应对，类似于图 7.2 中所示的示例。该数据集专为本书创建，有兴趣的读者可以在附录 B 中找到其他公开的指令数据集。</p>
<p>以下代码通过实现并执行一个函数来下载这个数据集。该数据集体积较小（仅 204 KB），采用 JSON 格式，其结构与 Python 字典类似，便于人类阅读和机器处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 7.1 Downloading the dataset</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_and_load_file</span>(<span class="params">file_path, url</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">        <span class="keyword">with</span> urllib.request.urlopen(url) <span class="keyword">as</span> response:</span><br><span class="line">            text_data = response.read().decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">            file.write(text_data)</span><br><span class="line">    <span class="keyword">else</span>: <span class="comment">#A</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">            text_data = file.read()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        data = json.load(file)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">file_path = <span class="string">&quot;instruction-data.json&quot;</span></span><br><span class="line">url = <span class="string">&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_mainchapter-code/instruction-data.json&quot;</span></span><br><span class="line"></span><br><span class="line">data = download_and_load_file(file_path, url)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of entries:&quot;</span>, <span class="built_in">len</span>(data))</span><br><span class="line"></span><br><span class="line"><span class="comment">#A 如果文件已经下载，就跳过下载过程</span></span><br></pre></td></tr></table></figure>
<p>执行代码后输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Number of entries: <span class="number">1100</span></span><br></pre></td></tr></table></figure>
<p>可以看到，我们从 JSON 文件中加载的‘数据列表’包含 1100 条指令数据集记录。让我们打印其中一条记录，看看每条记录的结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Example entry:\n&quot;</span>, data[<span class="number">50</span>])</span><br></pre></td></tr></table></figure>
<p>输出内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Example entry:</span><br><span class="line">&#123;<span class="string">&#x27;instruction&#x27;</span>: <span class="string">&#x27;Identify the correct spelling of the following word.&#x27;</span>, <span class="string">&#x27;input&#x27;</span>:</span><br><span class="line"><span class="string">&#x27;Ocassion&#x27;</span>, <span class="string">&#x27;output&#x27;</span>: <span class="string">&quot;The correct spelling is &#x27;Occasion.&#x27;&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>如我们所见，打印出的记录是一个包含 ‘instruction’、‘input’ 和 ‘output’ 键值的 Python 字典对象。我们来看另一条记录：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Another example entry:\n&quot;</span>, data[<span class="number">999</span>])</span><br></pre></td></tr></table></figure>
<p>根据该记录的内容，‘input’ 字段可能偶尔为空。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Another example entry:</span><br><span class="line">&#123;<span class="string">&#x27;instruction&#x27;</span>: <span class="string">&quot;What is an antonym of &#x27;complicated&#x27;?&quot;</span>, <span class="string">&#x27;input&#x27;</span>: <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;output&#x27;</span>: <span class="string">&quot;An</span></span><br><span class="line"><span class="string">antonym of &#x27;complicated&#x27; is &#x27;simple&#x27;.&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>指令微调（instruction finetuning），也称为监督式指令微调（supervised instruction finetuning），是指在包含明确输入-输出对的数据集上对模型进行训练（例如从 JSON 文件中提取的输入-输出对）。在为大语言模型（LLM）格式化这些条目时，通常会使用多种不同的方法。图 7.4 展示了两种不同的示例格式（通常称为提示风格），这些格式常用于训练一些知名的 LLM，例如 Alpaca 和 Phi-3。Alpaca 是最早公开指令微调过程的 LLM 之一，而由微软开发的 Phi-3 则展示了提示风格的多样性。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.4.png" alt=""></p>
<p>本章其余部分将使用 Alpaca 风格的提示方式，这是最受欢迎的提示风格之一，主要是因为它帮助定义了最初的微调方法。</p>
<blockquote>
<p><strong>练习 7.1 改变提示词风格</strong></p>
<p>在使用 Alpaca 提示语风格对模型进行微调之后，尝试图 7.4 中展示的 Phi-3 提示语风格，并观察其是否会影响模型的响应效果。</p>
</blockquote>
<p>我们首先定义一个<code>format_input</code>函数，用于将数据列表中的条目转换为如图 7.4 所示的 Alpaca 风格输入格式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 7.2 Implementing the prompt formatting function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">format_input</span>(<span class="params">entry</span>):</span><br><span class="line">    instruction_text = (</span><br><span class="line">        <span class="string">f&quot;Below is an instruction that describes a task. &quot;</span></span><br><span class="line">        <span class="string">f&quot;Write a response that appropriately completes the request.&quot;</span></span><br><span class="line">        <span class="string">f&quot;\n\n### Instruction:\n<span class="subst">&#123;entry[<span class="string">&#x27;instruction&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line">    )</span><br><span class="line">    input_text = <span class="string">f&quot;\n\n### Input:\n<span class="subst">&#123;entry[<span class="string">&#x27;input&#x27;</span>]&#125;</span>&quot;</span> <span class="keyword">if</span> entry[<span class="string">&quot;input&quot;</span>] <span class="keyword">else</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> instruction_text + input_text</span><br></pre></td></tr></table></figure>
<p><code>format_input</code> 函数接受一个字典条目作为输入，并构建格式化字符串。我们来用之前查看过的数据集条目 <code>data[50]</code> 测试一下这个函数的效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_input = format_input(data[<span class="number">50</span>])</span><br><span class="line">desired_response = <span class="string">f&quot;\n\n### Response:\n<span class="subst">&#123;data[<span class="number">50</span>][<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line"><span class="built_in">print</span>(model_input + desired_response)</span><br></pre></td></tr></table></figure>
<p>格式化后的输入示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Below <span class="keyword">is</span> an instruction that describes a task. Write a response that appropriately</span><br><span class="line">completes the request.</span><br><span class="line"></span><br><span class="line"><span class="comment">### Instruction:</span></span><br><span class="line">Identify the correct spelling of the following word.</span><br><span class="line"></span><br><span class="line"><span class="comment">### Input:</span></span><br><span class="line">Ocassion</span><br><span class="line"></span><br><span class="line"><span class="comment">### Response:</span></span><br><span class="line">The correct spelling <span class="keyword">is</span> <span class="string">&#x27;Occasion.&#x27;</span></span><br></pre></td></tr></table></figure>
<p>请注意，当 ‘input’ 字段为空时，<code>format_input</code>函数会跳过可选的 ‘### Input:’ 部分。我们可以通过将<code>format_input</code>函数应用于之前检查过的数据项 data[999] 来测试这一点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_input = format_input(data[<span class="number">999</span>])</span><br><span class="line">desired_response = <span class="string">f&quot;\n\n### Response:\n<span class="subst">&#123;data[<span class="number">999</span>][<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line"><span class="built_in">print</span>(model_input + desired_response)</span><br></pre></td></tr></table></figure>
<p>从以下输出可以看出，当 ‘input’ 字段为空时，格式化后的输入内容中不会包含 ‘### Input:’ 部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Below <span class="keyword">is</span> an instruction that describes a task. Write a response that appropriately</span><br><span class="line">completes the request.</span><br><span class="line"></span><br><span class="line"><span class="comment">### Instruction:</span></span><br><span class="line">What <span class="keyword">is</span> an antonym of <span class="string">&#x27;complicated&#x27;</span>?</span><br><span class="line"></span><br><span class="line"><span class="comment">### Response:</span></span><br><span class="line">An antonym of <span class="string">&#x27;complicated&#x27;</span> <span class="keyword">is</span> <span class="string">&#x27;simple&#x27;</span>.</span><br></pre></td></tr></table></figure>
<p>在进入下一节的 PyTorch 数据加载器设置之前，先将数据集划分为训练集、验证集和测试集，这与我们在上一章处理垃圾短信分类数据集时的划分方式类似。下面是具体的划分比例计算方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 7.3 Partitioning the dataset</span></span><br><span class="line">train_portion = <span class="built_in">int</span>(<span class="built_in">len</span>(data) * <span class="number">0.85</span>) <span class="comment"># 85% for training</span></span><br><span class="line">test_portion = <span class="built_in">int</span>(<span class="built_in">len</span>(data) * <span class="number">0.1</span>) <span class="comment"># 10% for testing</span></span><br><span class="line">val_portion = <span class="built_in">len</span>(data) - train_portion - test_portion <span class="comment"># Remaining 5% for validation</span></span><br><span class="line"></span><br><span class="line">train_data = data[:train_portion]</span><br><span class="line">test_data = data[train_portion:train_portion + test_portion]</span><br><span class="line">val_data = data[train_portion + test_portion:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set length:&quot;</span>, <span class="built_in">len</span>(train_data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Validation set length:&quot;</span>, <span class="built_in">len</span>(val_data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set length:&quot;</span>, <span class="built_in">len</span>(test_data))</span><br></pre></td></tr></table></figure>
<p>这种划分方式得到的数据集大小如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Training <span class="built_in">set</span> length: <span class="number">935</span></span><br><span class="line">Validation <span class="built_in">set</span> length: <span class="number">55</span></span><br><span class="line">Test <span class="built_in">set</span> length: <span class="number">110</span></span><br></pre></td></tr></table></figure>
<p>在成功下载并划分数据集，同时清晰地理解了数据集的提示格式后，我们现在准备开始指令微调过程的核心实现。在接下来的部分中，我们将重点讨论如何构建用于微调 LLM 的训练批次。</p>
<h2 id="7-3-将数据组织成训练批次">7.3 将数据组织成训练批次</h2>
<p>随着我们进入指令微调过程的实施阶段，接下来的步骤（如图 7.5 所示）将重点介绍如何高效地构建训练批次。这一步需要定义一种方法，以确保模型在微调过程中能够接收到格式化的训练数据。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.5.png" alt=""></p>
<p>在上一章中，训练批次是通过 PyTorch 的 <code>DataLoader</code> 类自动创建的，该类使用默认的<code>collate</code>函数将样本列表合并为批次。<code>collate </code> 函数的作用是将单个数据样本列表合并成一个批次，以便模型在训练过程中能够高效处理。</p>
<p>然而，为了适应指令微调的需求，本章的批处理过程更为复杂，需要我们创建一个自定义的 collate 函数，并将其嵌入到 <code>DataLoader</code> 中，以便处理指令微调数据集的特定需求和格式。</p>
<p>本节将分几步介绍批处理过程（包括自定义<code>collate</code>函数的编写），具体内容如图 7.6 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.6.png" alt=""></p>
<p>首先，为实现图 7.6 中展示的步骤 2.1 和 2.2，我们编写了一个 <code>InstructionDataset</code> 类，它应用了上一节中的 <code>format_input</code> 函数，并对数据集中的所有输入进行了预分词，类似于第 6 章中的 <code>SpamDataset</code>。这两个步骤的详细说明见图 7.7。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.7.png" alt=""></p>
<p>图 7.7 中展示的 两步操作通过 <code>InstructionDataset</code> 类的 <code>__init__</code> 构造函数实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 7.4 Implementing an instruction dataset class</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InstructionDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, tokenizer</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.encoded_texts = []</span><br><span class="line">        <span class="keyword">for</span> entry <span class="keyword">in</span> data:                                           <span class="comment">#A</span></span><br><span class="line">            instruction_plus_input = format_input(entry)</span><br><span class="line">            response_text = <span class="string">f&quot;\n\n### Response:\n<span class="subst">&#123;entry[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line">            full_text = instruction_plus_input + response_text</span><br><span class="line">            <span class="variable language_">self</span>.encoded_texts.append(</span><br><span class="line">                tokenizer.encode(full_text)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.encoded_texts[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line"><span class="comment">#A 预分词文本</span></span><br></pre></td></tr></table></figure>
<p>与第 6 章的方法类似，我们通过将多个训练样本收集到一个批次中来加速训练，这需要将所有输入填充到相似的长度。对此，我们使用与前一章一样的 <code>&lt;|endoftext|&gt;</code> 作为填充标记。</p>
<p>我们可以直接将 <code>&lt;|endoftext|&gt;</code> token 的 token ID 追加到预处理后的输入中，而不是将 <code>&lt;|endoftext|&gt;</code> token 本身追加到文本输入中。为了明确应该使用哪个 token ID，我们可以对 <code>&lt;|endoftext|&gt;</code> token 使用分词器的 <code>.encode</code> 方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line">tokenizer = tiktoken.get_encoding(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.encode(<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>, allowed_special=&#123;<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>&#125;))</span><br><span class="line">The resulting token ID <span class="keyword">is</span> <span class="number">50256.</span></span><br></pre></td></tr></table></figure>
<p>在第 6 章中，我们使用的填充方式是将数据集中的所有示例填充到相同长度。在本章中，我们将采用一种更为精细的方法，开发一个自定义的<code>collate</code>函数并传递给数据加载器。该自定义<code>collate</code>函数会将每个批次中的训练样本填充到相同长度，同时允许不同批次中的样本具有不同的长度，如图 7.8 所示。这种方法通过仅将序列扩展到每个批次中最长的序列长度，从而减少了不必要的填充，避免了对整个数据集进行冗余填充。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.8.png" alt=""></p>
<p>我们可以通过以下自定义<code>collate</code>函数来实现图 7.8 所示的填充过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">custom_collate_draft_1</span>(<span class="params"></span></span><br><span class="line"><span class="params">    batch,</span></span><br><span class="line"><span class="params">    pad_token_id=<span class="number">50256</span>,</span></span><br><span class="line"><span class="params">    device=<span class="string">&quot;cpu&quot;</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    batch_max_length = <span class="built_in">max</span>(<span class="built_in">len</span>(item)+<span class="number">1</span> <span class="keyword">for</span> item <span class="keyword">in</span> batch)          <span class="comment">#A</span></span><br><span class="line">    inputs_lst = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> batch:                                             <span class="comment">#B</span></span><br><span class="line">        new_item = item.copy()</span><br><span class="line">        new_item += [pad_token_id]</span><br><span class="line"></span><br><span class="line">        padded = new_item + [pad_token_id] * (batch_max_length - <span class="built_in">len</span>(new_item))</span><br><span class="line"></span><br><span class="line">        inputs = torch.tensor(padded[:-<span class="number">1</span>])                         <span class="comment">#C</span></span><br><span class="line">        inputs_lst.append(inputs)</span><br><span class="line"></span><br><span class="line">    inputs_tensor = torch.stack(inputs_lst).to(device)             <span class="comment">#D</span></span><br><span class="line">    <span class="keyword">return</span> inputs_tensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 找出批量中的最长序列</span></span><br><span class="line"><span class="comment">#B 对输入进行填充并准备好输入数据</span></span><br><span class="line"><span class="comment">#C 删除之前添加的多余填充 token</span></span><br><span class="line"><span class="comment">#D 将输入列表转换为张量，并转移到目标设备</span></span><br></pre></td></tr></table></figure>
<p>我们实现的 <code>custom_collate_draft_1</code> 虽然设计用于集成到 PyTorch 的 DataLoader 中，但它也可以独立使用。在这里，我们单独使用它来测试和验证其功能是否符合预期。我们将在三个不同输入上进行测试，目标是将它们合并为一个批次，并对每个样本进行填充以保证长度一致：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inputs_1 = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">inputs_2 = [<span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">inputs_3 = [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line">batch = (</span><br><span class="line">    inputs_1,</span><br><span class="line">    inputs_2,</span><br><span class="line">    inputs_3</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(custom_collate_draft_1(batch))</span><br></pre></td></tr></table></figure>
<p>生成的批次格式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [ <span class="number">5</span>, <span class="number">6</span>, <span class="number">50256</span>, <span class="number">50256</span>, <span class="number">50256</span>],</span><br><span class="line">        [ <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">50256</span>, <span class="number">50256</span>]])</span><br></pre></td></tr></table></figure>
<p>如输出所示，所有输入序列都已被填充到最长输入序列的长度，其中<code>inputs_1</code>包含了 5 个 token ID。</p>
<p>我们刚刚实现了自定义 <code>collate</code> 函数的第一个版本，用于从输入列表创建批次。然而，正如在第 5 章和第 6 章中所学的那样，我们还需要创建与输入 ID 批次相对应的目标 token ID 批次。图 7.9 显示了这些目标 ID，它们非常重要，因为它们代表我们希望模型生成的内容，并且在训练时用于计算损失，从而指导模型更新权重。这与之前章节的做法类似。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.9.png" alt=""></p>
<p>如图 7.9 所示，我们需要修改自定义的<code>collate</code>函数，使其在返回输入 token ID 的基础上，同时返回目标 token ID。</p>
<p>与第 5 章中描述的 LLM 预训练过程类似，目标 token ID 与输入 token ID 一一对应，但会右移一个位置，这种设置（如图 7.10 所示）使得 LLM 能够学习如何预测序列中的下一个 token。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.10.png" alt=""></p>
<p>以下为更新后的<code>collate</code>函数，它根据输入 token ID 生成目标 token ID（流程如图 7.10 所示）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">custom_collate_draft_2</span>(<span class="params"></span></span><br><span class="line"><span class="params">    batch,</span></span><br><span class="line"><span class="params">    pad_token_id=<span class="number">50256</span>,</span></span><br><span class="line"><span class="params">    device=<span class="string">&quot;cpu&quot;</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    batch_max_length = <span class="built_in">max</span>(<span class="built_in">len</span>(item)+<span class="number">1</span> <span class="keyword">for</span> item <span class="keyword">in</span> batch)</span><br><span class="line">    inputs_lst, targets_lst = [], []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> batch:</span><br><span class="line">        new_item = item.copy()</span><br><span class="line">        new_item += [pad_token_id]</span><br><span class="line">        padded = new_item + [pad_token_id] * (batch_max_length - <span class="built_in">len</span>(new_item))</span><br><span class="line">        inputs = torch.tensor(padded[:-<span class="number">1</span>])             <span class="comment">#A</span></span><br><span class="line">        targets = torch.tensor(padded[<span class="number">1</span>:])             <span class="comment">#B</span></span><br><span class="line">        inputs_lst.append(inputs)</span><br><span class="line">        targets_lst.append(targets)</span><br><span class="line"></span><br><span class="line">    inputs_tensor = torch.stack(inputs_lst).to(device)</span><br><span class="line">    targets_tensor = torch.stack(targets_lst).to(device)</span><br><span class="line">    <span class="keyword">return</span> inputs_tensor, targets_tensor</span><br><span class="line"></span><br><span class="line">inputs, targets = custom_collate_draft_2(batch)</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line"><span class="built_in">print</span>(targets)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 截断输入序列的最后一个 token。</span></span><br><span class="line"><span class="comment">#B 将目标序列中的每个 token 向右移动一个位置。</span></span><br></pre></td></tr></table></figure>
<p>对于之前定义的包含 3 个输入列表的示例批次，更新后的<code>custom_collate_draft_2</code>函数会返回输入和目标批次数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],                   <span class="comment">#A</span></span><br><span class="line">        [ <span class="number">5</span>, <span class="number">6</span>, <span class="number">50256</span>, <span class="number">50256</span>, <span class="number">50256</span>],</span><br><span class="line">        [ <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">50256</span>, <span class="number">50256</span>]])</span><br><span class="line">tensor([[ <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">50256</span>],               <span class="comment">#B</span></span><br><span class="line">        [ <span class="number">6</span>, <span class="number">50256</span>, <span class="number">50256</span>, <span class="number">50256</span>, <span class="number">50256</span>],</span><br><span class="line">        [ <span class="number">8</span>, <span class="number">9</span>, <span class="number">50256</span>, <span class="number">50256</span>, <span class="number">50256</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#A 第一个张量表示输入数据</span></span><br><span class="line"><span class="comment">#B 第二个张量表示目标数据</span></span><br></pre></td></tr></table></figure>
<p>在接下来的步骤中，我们会将所有填充 token 设置为占位值 -100。这个特殊值可以让填充 token 不参与训练损失的计算，从而确保只有有效数据会影响模型的学习。</p>
<p>关于这个过程的更多细节将在实施此修改后讨论。（在第 6 章中，我们无需担心这个问题，因为当时只训练了最后一个输出 token。）</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.11.png" alt=""></p>
<p>如图 7.11 所示，在步骤 2.4 中，我们将文本结束 token（之前用作填充 token，token ID 为 50256）在目标 token 列表中替换为 -100（选择 -100 作为替代值的原因将在后续说明）。</p>
<p>然而，请注意，我们在目标列表中仍保留了一个文本结束 token（ID 为 50256），如图 7.12 所示。这使得 LLM 能够学习在接收到指令时何时生成结束 token，以指示生成的响应已完成。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.12.png" alt=""></p>
<p>在以下代码中，我们修改了自定义的 <code>collate</code> 函数，将目标列表中 ID 为 50256 的 token 替换为 -100，图 7.12 展示了这一操作。此外，我们引入了一个 <code>allowed_max_length</code> 参数，用于选择性地限制样本的长度。当你使用的数据集超过 GPT-2 模型支持的 1024 个 token 的上下文长度时，这一调整将非常有用。更新后的 <code>collate</code> 函数代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 7.5 Implementing a custom batch collate function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">custom_collate_fn</span>(<span class="params"></span></span><br><span class="line"><span class="params">    batch,</span></span><br><span class="line"><span class="params">    pad_token_id=<span class="number">50256</span>,</span></span><br><span class="line"><span class="params">    ignore_index=-<span class="number">100</span>,</span></span><br><span class="line"><span class="params">    allowed_max_length=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    device=<span class="string">&quot;cpu&quot;</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    batch_max_length = <span class="built_in">max</span>(<span class="built_in">len</span>(item)+<span class="number">1</span> <span class="keyword">for</span> item <span class="keyword">in</span> batch)</span><br><span class="line">    inputs_lst, targets_lst = [], []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> batch:</span><br><span class="line">        new_item = item.copy()</span><br><span class="line">        new_item += [pad_token_id]</span><br><span class="line">        <span class="comment"># Pad sequences to max_length</span></span><br><span class="line">        padded = new_item + [pad_token_id] * (batch_max_length - <span class="built_in">len</span>(new_item))</span><br><span class="line">        inputs = torch.tensor(padded[:-<span class="number">1</span>]) <span class="comment"># Truncate the last token for inputs</span></span><br><span class="line">        targets = torch.tensor(padded[<span class="number">1</span>:]) <span class="comment"># Shift +1 to the right for targets</span></span><br><span class="line"></span><br><span class="line">        mask = targets == pad_token_id                  <span class="comment">#A</span></span><br><span class="line">        indices = torch.nonzero(mask).squeeze()         <span class="comment">#A</span></span><br><span class="line">        <span class="keyword">if</span> indices.numel() &gt; <span class="number">1</span>:                         <span class="comment">#A</span></span><br><span class="line">            targets[indices[<span class="number">1</span>:]] = ignore_index         <span class="comment">#A</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> allowed_max_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            inputs = inputs[:allowed_max_length]        <span class="comment">#B</span></span><br><span class="line">            targets = targets[:allowed_max_length]      <span class="comment">#B</span></span><br><span class="line"></span><br><span class="line">        inputs_lst.append(inputs)</span><br><span class="line">        targets_lst.append(targets)</span><br><span class="line"></span><br><span class="line">    inputs_tensor = torch.stack(inputs_lst).to(device)</span><br><span class="line">    targets_tensor = torch.stack(targets_lst)</span><br><span class="line">    <span class="keyword">return</span> inputs_tensor, targets_tensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 在 targets 中，将除第一个以外的所有填充标记替换为 ignore_index</span></span><br><span class="line"><span class="comment">#B 可选择性地将序列截断到最大长度</span></span><br></pre></td></tr></table></figure>
<p>我们再来尝试用最新的<code>custom_collate_fn</code>函数处理之前创建的样本批次，确认其是否按预期工作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inputs, targets = custom_collate_fn(batch)</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line"><span class="built_in">print</span>(targets)</span><br></pre></td></tr></table></figure>
<p>结果如下：第一个张量表示输入，第二个张量表示目标：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [ <span class="number">5</span>, <span class="number">6</span>, <span class="number">50256</span>, <span class="number">50256</span>, <span class="number">50256</span>],</span><br><span class="line">        [ <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">50256</span>, <span class="number">50256</span>]])</span><br><span class="line">tensor([[ <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">50256</span>],</span><br><span class="line">        [ <span class="number">6</span>, <span class="number">50256</span>, -<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>],</span><br><span class="line">        [ <span class="number">8</span>, <span class="number">9</span>, <span class="number">50256</span>, -<span class="number">100</span>, -<span class="number">100</span>]])</span><br></pre></td></tr></table></figure>
<p>通过打印出的结果可知，修改后的<code>custom_collate_fn</code>函数按预期工作，通过插入 token ID -100 来改变目标列表。那么，这种调整背后的逻辑是什么呢？接下来我们将深入探讨此修改的具体作用。</p>
<p>我们可以通过一个简单、独立的示例来说明，示例中每个输出的 logit 都可以对应模型词汇表中的一个潜在 token。在训练过程中，当模型预测一系列 token 时，我们可以计算交叉熵损失（类似于我们在第 5 章中进行预训练或第 6 章中对模型进行分类微调时的做法）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">logits_1 = torch.tensor(</span><br><span class="line">    [[-<span class="number">1.0</span>, <span class="number">1.0</span>], <span class="comment"># predictions for 1st token</span></span><br><span class="line">     [-<span class="number">0.5</span>, <span class="number">1.5</span>]] <span class="comment"># predictions for 2nd token</span></span><br><span class="line">)</span><br><span class="line">targets_1 = torch.tensor([<span class="number">0</span>, <span class="number">1</span>]) <span class="comment"># Correct token indices to generate</span></span><br><span class="line">loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)</span><br><span class="line"><span class="built_in">print</span>(loss_1)</span><br></pre></td></tr></table></figure>
<p>以上代码计算出的损失值为 1.1269。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">1.1269</span>)</span><br></pre></td></tr></table></figure>
<p>添加额外的 token ID 会影响损失计算，这是预料之中的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">logits_2 = torch.tensor(</span><br><span class="line">    [[-<span class="number">1.0</span>, <span class="number">1.0</span>],</span><br><span class="line">     [-<span class="number">0.5</span>, <span class="number">1.5</span>],</span><br><span class="line">     [-<span class="number">0.5</span>, <span class="number">1.5</span>]]                        <span class="comment">#A</span></span><br><span class="line">)</span><br><span class="line">targets_2 = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)</span><br><span class="line"><span class="built_in">print</span>(loss_2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#A 添加第三个 token ID</span></span><br></pre></td></tr></table></figure>
<p>添加第三个 token 后，损失值变为 0.7936。</p>
<p>到目前为止，我们已经使用 PyTorch 中的交叉熵损失函数进行了若干较为直观的示例计算，这也是我们在第 5 章和第 6 章的训练函数中使用的损失函数，接下来我们将在本章继续使用它。</p>
<p>现在，进入有趣的部分，看看如果我们将第三个目标 token ID 替换为 -100，会发生什么：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">targets_3 = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, -<span class="number">100</span>])</span><br><span class="line">loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)</span><br><span class="line"><span class="built_in">print</span>(loss_3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss_1 == loss_3:&quot;</span>, loss_1 == loss_3)</span><br><span class="line"><span class="comment">#The resulting output is as follows:</span></span><br><span class="line">tensor(<span class="number">1.1269</span>)</span><br><span class="line">loss_1 == loss_3: tensor(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>根据这个结果，我们可以看到，在这 3 个训练样本上的损失与之前计算的 2 个训练样本的损失相同。换句话说，交叉熵损失函数忽略了 targets_3 向量中的第三个条目，即对应 token ID 为 -100 的位置。（有兴趣的读者可以尝试将 -100 替换为其他非零、非一的 token ID，结果会导致错误。）</p>
<p>那么，为什么 -100 会被交叉熵损失函数忽略呢？在 PyTorch 中，cross_entropy 函数的默认设置是 <code>cross_entropy(..., ignore_index=-100)</code>，这意味着它会忽略标签为 -100 的目标。</p>
<p>在本章中，我们利用 <code>ignore_index</code> 来忽略训练示例中额外的结束token（填充token），这些 token 用于将训练样本填充至相同的长度，以便每个批次中的序列具有相同的长度。</p>
<p>如图 7.12 所示，我们希望在目标序列中保留一个50256（结束符）token ID，因为这有助于 LLM 学习生成文本结束的标记，进而作为判断回复是否完成的标志。</p>
<p>在实践中，除了遮蔽填充 token 外，还常常将指令部分对应的目标 token ID 一并遮蔽，如图 7.13 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.13.png" alt=""></p>
<p>通过对指令部分对应的目标 token ID 进行掩码（如图 7.13 所示），交叉熵损失仅计算生成响应的目标 token ID，模型在训练时也会专注于生成准确的回答，而不是去记住指令内容，从而有助于减少过拟合。</p>
<p>目前，研究人员对于在指令微调过程中遮蔽指令是否具有普遍效果存在分歧。例如，最近有一篇题为《Instruction Tuning With Loss Over Instructions》的论文表明，不遮蔽指令有助于提升大语言模型的性能（更多细节请参考附录 B）。在本章中，我们不选择遮蔽指令，但是将其作为读者的可选练习。</p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>练习 7.2 指令与输入的掩码处理</strong></p>
<p>在完成本章内容，并用本节实现的 <code>InstructionDataset</code> 对模型进行微调后，将指令和输入 token 替换为 -100 掩码，以实现图 7.13 展示的指令掩码方法。然后，评估该方法是否对模型性能有积极影响。</p>
</blockquote>
<h2 id="7-4-为指令数据集创建数据加载器">7.4 为指令数据集创建数据加载器</h2>
<p>在前一节中，我们完成了 <code>InstructionDataset</code> 类和 <code>custom_collate_fn</code> 函数的多个实现步骤。本节中，我们可以将 <code>InstructionDataset</code> 对象和 <code>custom_collate_fn</code> 函数直接传入 PyTorch 的数据加载器中（如图 7.14 所示）。加载器将自动对批次数据进行随机化和组织，为 LLM 的指令微调过程提供支持。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.14.png" alt=""></p>
<p>在我们实现图 7.14 中所示的数据加载器创建步骤之前，我们需要先简要讨论在前一节中实现的 <code>custom_collate_fn</code> 中的<code>device</code>参数设置。</p>
<p><code>custom_collate_fn</code> 包含将输入和目标张量（例如，torch.stack(inputs_lst).to(device)）移动到指定设备的代码，该设备可以是 “cpu”、“cuda”（GPU）或可选的 “mps”（适用于 Apple Silicon 芯片的 Mac）。(需要注意的是，使用 “mps” 设备可能会导致与本章内容存在数值差异，因为 PyTorch 对 Apple Silicon 的支持仍处于实验阶段。)</p>
<p>在前几章中，我们习惯在主训练循环中将数据转移到目标设备上（例如，当 device=“cuda” 时，数据转移到 GPU 内存）。将这个数据传输步骤移入 <code>collate</code> 函数的好处在于，能够在训练循环之外的后台进程中完成数据传输，避免在模型训练时阻塞 GPU。</p>
<p>以下代码用于初始化 device 变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># if torch.backends.mps.is_available():       #A</span></span><br><span class="line"><span class="comment">#     device = torch.device(&quot;mps&quot;)&quot;           #A</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Device:&quot;</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="comment">#A 取消这两行注释以在 Apple Silicon 芯片上启用 GPU</span></span><br></pre></td></tr></table></figure>
<p>接下来，为了在稍后将 <code>custom_collate_fn</code> 函数传入 PyTorch 的 <code>DataLoader</code> 类时复用<code>device</code>参数设置，我们使用 Python 标准库 <code>functools</code> 中的 <code>partial</code> 函数，为该函数创建一个预先填充 <code>device</code> 参数的新版本。另外，我们将 <code>allowed_max_length</code> 设置为 1024，以将数据截断至 GPT-2 模型（我们将在本章后续部分进行微调）所支持的最大上下文长度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line">customized_collate_fn = partial(custom_collate_fn, device=device,</span><br><span class="line">allowed_max_length=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure>
<p>接着，我们可以像前几章那样设置数据加载器，但这次我们将使用自定义的 <code>collate</code> 函数来处理批次数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 7.6 Initializing the data loaders</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">num_workers = <span class="number">0</span>            <span class="comment">#A</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line">train_dataset = InstructionDataset(train_data, tokenizer)</span><br><span class="line">train_loader = DataLoader(</span><br><span class="line">    train_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    collate_fn=customized_collate_fn,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    drop_last=<span class="literal">True</span>,</span><br><span class="line">    num_workers=num_workers</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">val_dataset = InstructionDataset(val_data, tokenizer)</span><br><span class="line">val_loader = DataLoader(</span><br><span class="line">    val_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    collate_fn=customized_collate_fn,</span><br><span class="line">    shuffle=<span class="literal">False</span>,</span><br><span class="line">    drop_last=<span class="literal">False</span>,</span><br><span class="line">    num_workers=num_workers</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_dataset = InstructionDataset(test_data, tokenizer)</span><br><span class="line"></span><br><span class="line">test_loader = DataLoader(</span><br><span class="line">    test_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    collate_fn=customized_collate_fn,</span><br><span class="line">    shuffle=<span class="literal">False</span>,</span><br><span class="line">    drop_last=<span class="literal">False</span>,</span><br><span class="line">    num_workers=num_workers</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#A 如果操作系统支持并行的 Python 进程，你可以尝试增加此数值。</span></span><br></pre></td></tr></table></figure>
<p>让我们检查一下由训练数据加载器生成的输入和目标批次的维度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Train loader:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> inputs, targets <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="built_in">print</span>(inputs.shape, targets.shape)</span><br></pre></td></tr></table></figure>
<p>输出如下（因篇幅限制，已做截断）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Train loader:</span><br><span class="line">torch.Size([<span class="number">8</span>, <span class="number">61</span>]) torch.Size([<span class="number">8</span>, <span class="number">61</span>])</span><br><span class="line">torch.Size([<span class="number">8</span>, <span class="number">76</span>]) torch.Size([<span class="number">8</span>, <span class="number">76</span>])</span><br><span class="line">torch.Size([<span class="number">8</span>, <span class="number">73</span>]) torch.Size([<span class="number">8</span>, <span class="number">73</span>])</span><br><span class="line">...</span><br><span class="line">torch.Size([<span class="number">8</span>, <span class="number">74</span>]) torch.Size([<span class="number">8</span>, <span class="number">74</span>])</span><br><span class="line">torch.Size([<span class="number">8</span>, <span class="number">69</span>]) torch.Size([<span class="number">8</span>, <span class="number">69</span>])</span><br></pre></td></tr></table></figure>
<p>在上面的输出中，我们可以看到第一个输入和目标批次的维度是 8×61，其中 8 表示批次大小（batch size），61 表示每个样本的 token 数。第二个输入和目标批次的 token 数则不同（76 个token）。</p>
<p>正如我们在前面的代码输出中所见，得益于自定义的<code>collate</code>函数，数据加载器可以创建包含不同长度数据的批次。在下一节，我们将加载一个预训练的 LLM ，并使用该数据加载器对模型进行微调。</p>
<h2 id="7-5-加载预训练的-LLM">7.5 加载预训练的 LLM</h2>
<p>在之前的部分中，我们花费了大量时间准备指令微调所需的数据集，这是监督微调过程的关键环节。除此之外，许多其他步骤也与预训练过程相同，因此我们可以复用前几章的大部分代码。</p>
<p>在正式开始指令微调之前，我们首先需要加载一个预训练的 GPT 模型，正如图 7.15 所示，该模型是我们希望进行微调的对象。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.15.png" alt=""></p>
<p>如 7.15 概述了完整的指令微调流程，本节重点介绍第 4 步，即加载预训练的 LLM ，作为指令微调的起点，过程与前几章类似。然而，这次我们加载的是 3.55 亿参数的中等模型，而非之前使用的 1.24 亿参数的小模型。选择更大模型的原因是 1.24 亿参数的小模型容量有限，难以通过指令微调获得令人满意的效果。”</p>
<p>本节使用与第 5 章第 5.5 节和第 6 章第 6.4 节中相同的代码，不同之处在于我们这次指定了“gpt2-medium (355M)”而不是“gpt2-small (124M)”。请注意，执行下面的代码将会启动下载中等规模的 GPT 模型，该模型的存储需求约为 1.42 GB，约是小型模型所需存储空间的三倍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 7.7 Loading the pretrained model</span></span><br><span class="line"><span class="keyword">from</span> gpt_download <span class="keyword">import</span> download_and_load_gpt2</span><br><span class="line"><span class="keyword">from</span> chapter04 <span class="keyword">import</span> GPTModel</span><br><span class="line"><span class="keyword">from</span> chapter05 <span class="keyword">import</span> load_weights_into_gpt</span><br><span class="line"></span><br><span class="line">BASE_CONFIG = &#123;</span><br><span class="line">    <span class="string">&quot;vocab_size&quot;</span>: <span class="number">50257</span>, <span class="comment"># Vocabulary size</span></span><br><span class="line">    <span class="string">&quot;context_length&quot;</span>: <span class="number">1024</span>, <span class="comment"># Context length</span></span><br><span class="line">    <span class="string">&quot;drop_rate&quot;</span>: <span class="number">0.0</span>, <span class="comment"># Dropout rate</span></span><br><span class="line">    <span class="string">&quot;qkv_bias&quot;</span>: <span class="literal">True</span> <span class="comment"># Query-key-value bias</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">model_configs = &#123;</span><br><span class="line">    <span class="string">&quot;gpt2-small (124M)&quot;</span>: &#123;<span class="string">&quot;emb_dim&quot;</span>: <span class="number">768</span>, <span class="string">&quot;n_layers&quot;</span>: <span class="number">12</span>, <span class="string">&quot;n_heads&quot;</span>: <span class="number">12</span>&#125;,</span><br><span class="line">    <span class="string">&quot;gpt2-medium (355M)&quot;</span>: &#123;<span class="string">&quot;emb_dim&quot;</span>: <span class="number">1024</span>, <span class="string">&quot;n_layers&quot;</span>: <span class="number">24</span>, <span class="string">&quot;n_heads&quot;</span>: <span class="number">16</span>&#125;,</span><br><span class="line">    <span class="string">&quot;gpt2-large (774M)&quot;</span>: &#123;<span class="string">&quot;emb_dim&quot;</span>: <span class="number">1280</span>, <span class="string">&quot;n_layers&quot;</span>: <span class="number">36</span>, <span class="string">&quot;n_heads&quot;</span>: <span class="number">20</span>&#125;,</span><br><span class="line">    <span class="string">&quot;gpt2-xl (1558M)&quot;</span>: &#123;<span class="string">&quot;emb_dim&quot;</span>: <span class="number">1600</span>, <span class="string">&quot;n_layers&quot;</span>: <span class="number">48</span>, <span class="string">&quot;n_heads&quot;</span>: <span class="number">25</span>&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">CHOOSE_MODEL = <span class="string">&quot;gpt2-medium (355M)&quot;</span></span><br><span class="line">BASE_CONFIG.update(model_configs[CHOOSE_MODEL])</span><br><span class="line"></span><br><span class="line">model_size = CHOOSE_MODEL.split(<span class="string">&quot; &quot;</span>)[-<span class="number">1</span>].lstrip(<span class="string">&quot;(&quot;</span>).rstrip(<span class="string">&quot;)&quot;</span>)</span><br><span class="line">settings, params = download_and_load_gpt2(model_size=model_size, models_dir=<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line"></span><br><span class="line">model = GPTModel(BASE_CONFIG)</span><br><span class="line">load_weights_into_gpt(model, params)</span><br><span class="line">model.<span class="built_in">eval</span>();</span><br></pre></td></tr></table></figure>
<p>在执行上述代码后，将下载多个文件，这与前面章节中的过程相似。下载的文件包括：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">checkpoint: <span class="number">100</span>%|██████████| <span class="number">77.0</span>/<span class="number">77.0</span> [<span class="number">00</span>:<span class="number">00</span>&lt;<span class="number">00</span>:<span class="number">00</span>, 156kiB/s]</span><br><span class="line">encoder.json: <span class="number">100</span>%|██████████| <span class="number">1.04</span>M/<span class="number">1.04</span>M [<span class="number">00</span>:02&lt;<span class="number">00</span>:<span class="number">00</span>, 467kiB/s]</span><br><span class="line">hparams.json: <span class="number">100</span>%|██████████| <span class="number">91.0</span>/<span class="number">91.0</span> [<span class="number">00</span>:<span class="number">00</span>&lt;<span class="number">00</span>:<span class="number">00</span>, 198kiB/s]</span><br><span class="line">model.ckpt.data-<span class="number">00000</span>-of-00001: <span class="number">100</span>%|██████████| <span class="number">1.42</span>G/<span class="number">1.42</span>G [05:<span class="number">50</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">4.05</span>MiB/s]</span><br><span class="line">model.ckpt.index: <span class="number">100</span>%|██████████| <span class="number">10.4</span>k/<span class="number">10.4</span>k [<span class="number">00</span>:<span class="number">00</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">18.1</span>MiB/s]</span><br><span class="line">model.ckpt.meta: <span class="number">100</span>%|██████████| 927k/927k [<span class="number">00</span>:02&lt;<span class="number">00</span>:<span class="number">00</span>, 454kiB/s]</span><br><span class="line">vocab.bpe: <span class="number">100</span>%|██████████| 456k/456k [<span class="number">00</span>:01&lt;<span class="number">00</span>:<span class="number">00</span>, 283kiB/s]</span><br></pre></td></tr></table></figure>
<p>在进入模型微调之前，让我们先评估一下预训练的 LLM 在某个验证集任务上的表现。具体来说，我们通过将模型的输出与预期回答进行比较，这样可以让我们在不进行微调的情况下，对模型的指令执行能力有一个基本了解，这也有助于我们理解微调的效果。我们使用验证集中的第一个例子来进行评估：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">input_text = format_input(val_data[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(input_text)</span><br></pre></td></tr></table></figure>
<p>指令的内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Below <span class="keyword">is</span> an instruction that describes a task. Write a response that appropriately</span><br><span class="line">completes the request.</span><br><span class="line"></span><br><span class="line"><span class="comment">### Instruction:</span></span><br><span class="line">Convert the active sentence to passive: <span class="string">&#x27;The chef cooks the meal every day.&#x27;</span></span><br><span class="line">Next, we generate the model<span class="string">&#x27;s response using the generate function from chapter 5:</span></span><br><span class="line"><span class="string">from chapter05 import generate, text_to_token_ids, token_ids_to_text</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">token_ids = generate(</span></span><br><span class="line"><span class="string">    model=model,</span></span><br><span class="line"><span class="string">    idx=text_to_token_ids(input_text, tokenizer),</span></span><br><span class="line"><span class="string">    max_new_tokens=35,</span></span><br><span class="line"><span class="string">    context_size=BASE_CONFIG[&quot;context_length&quot;],</span></span><br><span class="line"><span class="string">    eos_id=50256,</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">generated_text = token_ids_to_text(token_ids, tokenizer)</span></span><br></pre></td></tr></table></figure>
<p>需要注意的是，<code>generate</code> 函数返回的是输入文本和输出文本的组合。这种输出方式在前几章中由于易读性被频繁使用，因为预训练的大语言模型主要设计为文本补全模型，其中输入和输出会被拼接在一起，生成连贯且易读的文本。然而，在评估模型在特定任务上的表现时，我们通常只关注模型生成的响应部分。</p>
<p>为了提取模型的响应文本，我们需要从生成的文本起始部分减去输入指令的长度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">response_text = generated_text[<span class="built_in">len</span>(input_text):].strip()</span><br><span class="line"><span class="built_in">print</span>(response_text)</span><br></pre></td></tr></table></figure>
<p>这段代码将移除生成文本开头的输入部分，只留下模型生成的响应。接着，应用 <code>strip()</code> 函数去除文本两端的空白字符，输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Response:</span></span><br><span class="line"></span><br><span class="line">The chef cooks the meal every day.</span><br><span class="line"></span><br><span class="line"><span class="comment">### Instruction:</span></span><br><span class="line"></span><br><span class="line">Convert the active sentence to passive: <span class="string">&#x27;The chef cooks the</span></span><br></pre></td></tr></table></figure>
<p>从输出结果来看，预训练模型尚未能够正确地执行给定的指令。虽然它确实创建了一个“Response”部分，但只是重复了原始输入句子和部分指令，并未如要求那样将主动语态转换为被动语态。</p>
<p>在接下来的部分，我们将实现微调过程，以提升模型理解并恰当回应此类请求的能力。</p>
<h2 id="7-6-指令微调-LLM">7.6 指令微调 LLM</h2>
<p>图 7.16 中的章节概述展示了本节的重点：对大语言模型（LLM）进行微调。我们将在上一节加载的预训练模型基础上，利用本章前面准备的指令数据集进一步训练该模型。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.16.png" alt=""></p>
<p>如前所述，我们在本章开头实现指令数据集处理时，已经完成了所有关键工作。对于微调过程本身，我们可以复用第 5 章中实现的损失计算和训练函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> chapter05 <span class="keyword">import</span> (</span><br><span class="line">    calc_loss_loader,</span><br><span class="line">    train_model_simple</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在我们开始训练之前，让我们计算一下训练集和验证集的初始损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.to(device)</span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    train_loss = calc_loss_loader(train_loader, model, device, num_batches=<span class="number">5</span>)</span><br><span class="line">    val_loss = calc_loss_loader(val_loader, model, device, num_batches=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training loss:&quot;</span>, train_loss)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Validation loss:&quot;</span>, val_loss)</span><br></pre></td></tr></table></figure>
<p>初始损失值如下（与前几章一样，我们的目标是最小化这个损失）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training loss: <span class="number">3.825908660888672</span></span><br><span class="line">Validation loss: <span class="number">3.7619335651397705</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>[!NOTE]</p>
<p><strong>应对硬件限制</strong></p>
<p>需要注意的是，使用和训练像 GPT-2 medium（355 百万个参数）这样的大型模型相比于先前章节中使用的小型 GPT-2 模型（1.24 亿参数）在计算上更加密集。如果你因硬件限制遇到问题，可以通过将 CHOOSE_MODEL = “gpt2-medium (355M)” 更改为 CHOOSE_MODEL = “gpt2-small (124M)” 来切换到较小的模型。另一种加速模型训练的方式是使用 GPU。有关使用云 GPU 的选项，请参考本书代码仓库中的补充部分：<a target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch/tree/main/setup">https://github.com/rasbt/LLMs-from-scratch/tree/main/setup</a></p>
</blockquote>
<p>表格 7.1 提供了在不同设备（包括 CPU 和 GPU）上训练每个模型的参考运行时间。在兼容的 GPU 上运行此代码无需修改代码，并且能够显著加快训练速度。对于本章展示的结果，我使用了 GPT-2 中型模型，并在 A100 GPU 上进行了训练。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/table_7.1.png" alt=""></p>
<p>模型和数据加载器准备好后，我们可以开始训练模型。以下代码设置了训练过程的各项配置，包括初始化优化器、设置训练轮次、定义评估频率，并基于之前提到的第一个验证集样本（val_data[0]）来评估训练过程中生成的 LLM 响应：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 7.8 Instruction finetuning the pretrained LLM</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">0.00005</span>, weight_decay=<span class="number">0.1</span>)</span><br><span class="line">num_epochs = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">train_losses, val_losses, tokens_seen = train_model_simple(</span><br><span class="line">    model, train_loader, val_loader, optimizer, device,</span><br><span class="line">    num_epochs=num_epochs, eval_freq=<span class="number">5</span>, eval_iter=<span class="number">5</span>,</span><br><span class="line">    start_context=format_input(val_data[<span class="number">0</span>]), tokenizer=tokenizer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">end_time = time.time()</span><br><span class="line">execution_time_minutes = (end_time - start_time) / <span class="number">60</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training completed in <span class="subst">&#123;execution_time_minutes:<span class="number">.2</span>f&#125;</span> minutes.&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>以下输出显示了经过两个训练周期的进展，稳步下降的损失值表明模型在理解指令和生成合适回答方面的能力正在提升：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Ep <span class="number">1</span> (Step <span class="number">000000</span>): Train loss <span class="number">2.637</span>, Val loss <span class="number">2.626</span></span><br><span class="line">Ep <span class="number">1</span> (Step 000005): Train loss <span class="number">1.174</span>, Val loss <span class="number">1.103</span></span><br><span class="line">Ep <span class="number">1</span> (Step <span class="number">0000</span>10): Train loss <span class="number">0.872</span>, Val loss <span class="number">0.944</span></span><br><span class="line">Ep <span class="number">1</span> (Step 000015): Train loss <span class="number">0.857</span>, Val loss <span class="number">0.906</span></span><br><span class="line">...</span><br><span class="line">Ep <span class="number">1</span> (Step 000115): Train loss <span class="number">0.520</span>, Val loss <span class="number">0.665</span></span><br><span class="line">Below <span class="keyword">is</span> an instruction that describes a task. Write a response that appropriately</span><br><span class="line">completes the request. <span class="comment">### Instruction: Convert the active sentence to passive: &#x27;The</span></span><br><span class="line">chef cooks the meal every day.<span class="string">&#x27; ### Response: The meal is prepared every day by the</span></span><br><span class="line"><span class="string">chef.&lt;|endoftext|&gt;The following is an instruction that describes a task. Write a</span></span><br><span class="line"><span class="string">response that appropriately completes the request. ### Instruction: Convert the active</span></span><br><span class="line"><span class="string">sentence to passive:</span></span><br><span class="line"><span class="string">Ep 2 (Step 000120): Train loss 0.438, Val loss 0.670</span></span><br><span class="line"><span class="string">Ep 2 (Step 000125): Train loss 0.453, Val loss 0.685</span></span><br><span class="line"><span class="string">Ep 2 (Step 000130): Train loss 0.448, Val loss 0.681</span></span><br><span class="line"><span class="string">Ep 2 (Step 000135): Train loss 0.408, Val loss 0.677</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">Ep 2 (Step 000230): Train loss 0.300, Val loss 0.657</span></span><br><span class="line"><span class="string">Below is an instruction that describes a task. Write a response that appropriately</span></span><br><span class="line"><span class="string">completes the request. ### Instruction: Convert the active sentence to passive: &#x27;</span>The</span><br><span class="line">chef cooks the meal every day.<span class="string">&#x27; ### Response: The meal is cooked every day by the chef.</span></span><br><span class="line"><span class="string">&lt;|endoftext|&gt;The following is an instruction that describes a task. Write a response</span></span><br><span class="line"><span class="string">that appropriately completes the request. ### Instruction: What is the capital of the</span></span><br><span class="line"><span class="string">United Kingdom</span></span><br><span class="line"><span class="string">Training completed in 0.87 minutes.</span></span><br></pre></td></tr></table></figure>
<p>训练输出表明模型正在有效学习，我们可以通过训练和验证损失值在两个周期中的持续下降看出这一点。这表明模型正在逐渐提高其理解和执行提供的指令的能力。 （由于模型在这两个周期内展示了有效的学习，延长训练周期到第三个周期或更多并非必要，反而可能适得其反，因为这可能导致过拟合。）</p>
<p>此外，每一轮训练结束时生成的响应可以帮助我们检查模型在验证集示例上正确执行任务的进展。在这个例子中，模型成功地将主动语态句子‘The chef cooks the meal every day.’ 转换为被动语态‘The meal is cooked every day by the chef.’</p>
<p>我们将在后续部分更详细地回顾并评估模型的响应质量。现在，为了总结本节内容，我们将分析训练和验证损失曲线，从中获得有关模型学习过程的更多见解。为此，我们使用第 5 章中的 <code>plot_losses</code> 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> chapter05 <span class="keyword">import</span> plot_losses</span><br><span class="line">epochs_tensor = torch.linspace(<span class="number">0</span>, num_epochs, <span class="built_in">len</span>(train_losses))</span><br><span class="line">plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)</span><br></pre></td></tr></table></figure>
<p>由此生成的损失曲线如图 7.17 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.17.png" alt=""></p>
<p>如图 7.17 的损失图所示，模型在训练集和验证集上的表现随着训练的进行显著提高。在初期阶段，损失的快速下降表明模型正在迅速学习数据中的有意义的模式和表示。随着训练进入第二个 epoch，损失继续减少，但速度放缓，表明模型正在微调其学习到的表示，并逐渐收敛到一个稳定的解。</p>
<p>尽管图 7.17 中的损失曲线表明模型正在有效训练，但最关键的方面是其在响应质量和正确性上的表现。在本章接下来的部分，我们将提取响应，并将其存储为一种便于评估和量化响应质量的格式。</p>
<blockquote>
<p><strong>练习 7.3 在原始 Alpaca 数据集上进行微调</strong></p>
<p>斯坦福大学研究人员创建的 Alpaca 数据集是最早且最受欢迎的公开共享指令数据集之一，包含了 52,002 条数据。作为本章中使用的<code>instruction-data.json</code>文件的替代，可以考虑在这个数据集上对 LLM 进行微调。该数据集可通过以下网址获取：<a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json%E3%80%82">https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json。</a></p>
<p>该数据集包含 52,002 条记录，约为本章使用数据集的 50 倍，且大部分记录的长度也较长。因此，强烈建议使用 GPU 来加速微调过程。如果遇到内存不足错误，可以考虑将批量大小（batch_size）从 8 降至 4、2，甚至 1。此外，降低最大长度（allowed_max_length）从 1024 调整为 512 或 256，也有助于缓解内存问题。</p>
</blockquote>
<h2 id="7-7-提取并保存响应">7.7 提取并保存响应</h2>
<p>在之前内容中，我们已经对 LLM 在指令数据集的训练部分进行微调，现在我们开始评估其在测试集上的表现。为此，我们首先对测试集中的每个输入生成模型的回答，并收集这些结果以便人工分析，详见图 7.18。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.18.png" alt=""></p>
<p>我们从步骤 7 开始（详见图 7.18），通过<code>generate</code>函数输出模型回答，并将其与预期的前三个测试集答案并排展示，便于进行对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line"><span class="keyword">for</span> entry <span class="keyword">in</span> test_data[:<span class="number">3</span>]:                <span class="comment">#A</span></span><br><span class="line">    input_text = format_input(entry)</span><br><span class="line">    token_ids = generate(                  <span class="comment">#B</span></span><br><span class="line">        model=model,</span><br><span class="line">        idx=text_to_token_ids(input_text, tokenizer).to(device),</span><br><span class="line">        max_new_tokens=<span class="number">256</span>,</span><br><span class="line">        context_size=BASE_CONFIG[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">        eos_id=<span class="number">50256</span></span><br><span class="line">    )</span><br><span class="line">    generated_text = token_ids_to_text(token_ids, tokenizer)</span><br><span class="line">    response_text = generated_text[<span class="built_in">len</span>(input_text):].replace(<span class="string">&quot;### Response:&quot;</span>,</span><br><span class="line"><span class="string">&quot;&quot;</span>).strip()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(input_text)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\nCorrect response:\n&gt;&gt; <span class="subst">&#123;entry[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\nModel response:\n&gt;&gt; <span class="subst">&#123;response_text.strip()&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-------------------------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 遍历测试集中的前三个样本</span></span><br><span class="line"><span class="comment">#B 使用在第 7.5 节导入的 generate 函数</span></span><br></pre></td></tr></table></figure>
<p>如前所述，<code>generate</code>函数会返回合并后的输入和输出文本，因此我们可以对 <code>generated_text</code> 内容使用切片和 <code>.replace()</code> 方法，提取出模型的回复。以下展示了指令、测试集中的预期回复以及模型的实际回复：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">Below <span class="keyword">is</span> an instruction that describes a task. Write a response that appropriately</span><br><span class="line">completes the request.</span><br><span class="line"></span><br><span class="line"><span class="comment">### Instruction:</span></span><br><span class="line">Rewrite the sentence using a simile.</span><br><span class="line"></span><br><span class="line"><span class="comment">### Input:</span></span><br><span class="line">The car <span class="keyword">is</span> very fast.</span><br><span class="line"></span><br><span class="line">Correct response:</span><br><span class="line">&gt;&gt; The car <span class="keyword">is</span> <span class="keyword">as</span> fast <span class="keyword">as</span> lightning.</span><br><span class="line"></span><br><span class="line">Model response:</span><br><span class="line">&gt;&gt; The car <span class="keyword">is</span> <span class="keyword">as</span> fast <span class="keyword">as</span> a bullet.</span><br><span class="line">-------------------------------------</span><br><span class="line">Below <span class="keyword">is</span> an instruction that describes a task. Write a response that appropriately</span><br><span class="line">completes the request.</span><br><span class="line"></span><br><span class="line"><span class="comment">### Instruction:</span></span><br><span class="line">What <span class="built_in">type</span> of cloud <span class="keyword">is</span> typically associated <span class="keyword">with</span> thunderstorms?</span><br><span class="line"></span><br><span class="line">Correct response:</span><br><span class="line">&gt;&gt; The <span class="built_in">type</span> of cloud typically associated <span class="keyword">with</span> thunderstorms <span class="keyword">is</span> cumulonimbus.</span><br><span class="line"></span><br><span class="line">Model response:</span><br><span class="line">&gt;&gt; The <span class="built_in">type</span> of cloud associated <span class="keyword">with</span> thunderstorms <span class="keyword">is</span> a cumulus cloud.</span><br><span class="line">-------------------------------------</span><br><span class="line">Below <span class="keyword">is</span> an instruction that describes a task. Write a response that appropriately</span><br><span class="line">completes the request.</span><br><span class="line"></span><br><span class="line"><span class="comment">### Instruction:</span></span><br><span class="line">Name the author of <span class="string">&#x27;Pride and Prejudice&#x27;</span>.</span><br><span class="line"></span><br><span class="line">Correct response:</span><br><span class="line">&gt;&gt; Jane Austen.</span><br><span class="line"></span><br><span class="line">Model response:</span><br><span class="line">&gt;&gt; The author of <span class="string">&#x27;Pride and Prejudice&#x27;</span> <span class="keyword">is</span> Jane Austen.</span><br><span class="line">-------------------------------------</span><br></pre></td></tr></table></figure>
<p>从测试集的指令、给定的参考回答以及模型生成的回答来看，模型整体表现相对较好。第一个和最后一个指令的回答明显正确，而第二个回答虽然接近正确，但并非完全准确。模型将‘积云’回答成了‘积雨云’。需要注意的是，积云可以发展成积雨云，而积雨云有可能产生雷暴。</p>
<p>最重要的是，我们可以看到，模型评估并不像上一章那样简单，在上一章中，我们只是通过计算正确的垃圾短信/非垃圾短信标签的百分比来获得分类准确率。而在实际应用中，像聊天机器人这样的指令微调大语言模型（instruction-finetuned LLMs）则需要通过多种方法进行评估：</p>
<ul>
<li>简答题和多项选择题的基准测试（如 MMLU，“评估大规模多任务语言理解能力”，论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.03300%EF%BC%89%EF%BC%8C%E7%94%A8%E4%BA%8E%E6%B5%8B%E8%AF%95%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%80%9A%E7%94%A8%E7%9F%A5%E8%AF%86%E6%B0%B4%E5%B9%B3%E3%80%82">https://arxiv.org/abs/2009.03300），用于测试模型的通用知识水平。</a></li>
<li>基于人类偏好对其他大语言模型进行比较，例如 LMSYS 的 Chatbot Arena 平台（<a target="_blank" rel="noopener" href="https://arena.lmsys.org">https://arena.lmsys.org</a>）。</li>
<li>自动化对话基准测试，使用像 GPT-4 这样的 LLM 来评估回答，例如 AlpacaEval（<a target="_blank" rel="noopener" href="https://tatsulab.github.io/alpaca_eval/%EF%BC%89%E3%80%82">https://tatsulab.github.io/alpaca_eval/）。</a></li>
</ul>
<p>在实践中，以上三种评估方法（多项选择题回答、人工评估和自动化指标）都可以选择。然而，我们在本章主要关注对话性能的评估，而不仅仅是回答多选题的能力，因此第二种（人工评估）和第三种（自动化指标）可能更为相关。</p>
<p>人工评估虽然能够提供宝贵的见解，但在处理大量回复时往往耗时费力。例如，阅读并为 1,100 条回复逐一评分将需要投入相当大的精力。</p>
<p>考虑到任务规模，我们将采用类似‘方法3’的方案，通过另一个大语言模型（LLM）对生成的响应进行自动评估。这种方法能够高效地评估响应质量，无需大量的人力参与，从而节省时间和资源，同时仍能获得有意义的性能指标。</p>
<p>在接下来的部分中，我们将借鉴 AlpacaEval 的评估方法，使用另一个 LLM 来评估微调模型的响应。然而，与依赖公开的基准测试数据集不同，我们使用了自定义测试集。这样可以更有针对性地评估模型在实际应用场景中的表现，以反映微调所用指令数据集中所代表的目标任务效果。</p>
<p>为了准备评估过程中需要的响应，我们将生成的模型响应追加到 <code>test_set</code> 字典中，并将更新后的数据保存为名为 <code>instructiondata-with-response.json</code> 的文件以便记录。此外，保存这个文件后，我们可以在将来的 Python 会话中轻松加载和分析这些响应数据。</p>
<p>以下代码与之前一样使用了 <code>generate</code> 方法，但这次模型的响应不再直接打印，而是被添加到 <code>test_set</code> 字典中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 7.9 Generating test set responses</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, entry <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(test_data), total=<span class="built_in">len</span>(test_data)):</span><br><span class="line">    input_text = format_input(entry)</span><br><span class="line"></span><br><span class="line">    token_ids = generate(</span><br><span class="line">        model=model,</span><br><span class="line">        idx=text_to_token_ids(input_text, tokenizer).to(device),</span><br><span class="line">        max_new_tokens=<span class="number">256</span>,</span><br><span class="line">        context_size=BASE_CONFIG[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">        eos_id=<span class="number">50256</span></span><br><span class="line">    )</span><br><span class="line">    generated_text = token_ids_to_text(token_ids, tokenizer)</span><br><span class="line">    response_text = generated_text[<span class="built_in">len</span>(input_text):].replace(<span class="string">&quot;### Response:&quot;</span>,</span><br><span class="line"><span class="string">&quot;&quot;</span>).strip()</span><br><span class="line">    test_data[i][<span class="string">&quot;model_response&quot;</span>] = response_text</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;instruction-data-with-response.json&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    json.dump(test_data, file, indent=<span class="number">4</span>) <span class="comment"># &quot;indent&quot; for pretty-printing</span></span><br></pre></td></tr></table></figure>
<p>在 A100 GPU 上处理此数据集大约需要 1 分钟，而在 M3 MacBook Air 上则需要约 6 分钟：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">100</span>%|██████████| <span class="number">110</span>/<span class="number">110</span> [01:05&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">1.68</span>it/s]</span><br></pre></td></tr></table></figure>
<p>我们来验证一下响应是否已正确添加到测试集字典中，可以通过检查其中一个条目来实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(test_data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>从输出结果可以看出，模型响应已正确添加：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;instruction&#x27;</span>: <span class="string">&#x27;Rewrite the sentence using a simile.&#x27;</span>, <span class="string">&#x27;input&#x27;</span>: <span class="string">&#x27;The car is very</span></span><br><span class="line"><span class="string">fast.&#x27;</span>, <span class="string">&#x27;output&#x27;</span>: <span class="string">&#x27;The car is as fast as lightning.&#x27;</span>, <span class="string">&#x27;model_response&#x27;</span>: <span class="string">&#x27;The car is as</span></span><br><span class="line"><span class="string">fast as a bullet.&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>最后，我们将模型保存为文件 gpt2-medium355M-sft.pth，以便在未来的项目中复用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove white spaces and parentheses from file name</span></span><br><span class="line">file_name = <span class="string">f&quot;<span class="subst">&#123;re.sub(<span class="string">r&#x27;[ ()]&#x27;</span>, <span class="string">&#x27;&#x27;</span>, CHOOSE_MODEL) &#125;</span>-sft.pth&quot;</span></span><br><span class="line">torch.save(model.state_dict(), file_name)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model saved as <span class="subst">&#123;file_name&#125;</span>&quot;</span>)</span><br><span class="line">The saved model can then be loaded via model.load_state_dict(torch.load(<span class="string">&quot;gpt2-</span></span><br><span class="line"><span class="string">medium355M-sft.pth&quot;</span>)).</span><br></pre></td></tr></table></figure>
<h2 id="7-8-评估指令微调后的-LLM">7.8 评估指令微调后的 LLM</h2>
<p>之前章节中，我们通过查看模型在测试集中的 3 个示例上的响应来评估指令微调模型的性能。虽然这种方法可以提供模型表现的大致概况，但不适合用于大规模响应的评估。因此，我们在本节中实现了一种新方法（如图 7.19 的章节概览所示），利用另一个更大的大语言模型对微调模型的响应进行自动化评估。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.19.png" alt=""></p>
<p>为了实现图 7.19 中第 9 步（以自动化方式评估测试集响应），我们使用了 Meta AI 开发的一个经过指令微调的 Llama 3 模型，该模型拥有 80 亿参数，可以通过开源应用程序 Ollama 在本地运行（官网：<a target="_blank" rel="noopener" href="https://ollama.com">https://ollama.com</a>）。</p>
<p>Ollama 是一个高效的应用程序，适用于在笔记本电脑上运行大语言模型（LLM）。它是开源库 <code>llama.cpp</code>（<a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a>）的封装，该库用纯 C/C++ 实现了 LLM，旨在最大化效率。然而，需要注意的是，Ollama 仅用于使用 LLM 生成文本（推理），并不支持训练或微调 LLM。</p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>通过Web API使用更强大的 LLM</strong></p>
<p>拥有 80 亿参数的 Llama 3 模型是一款性能非常强大的 LLM，能够在本地运行。然而，与 OpenAI 提供的 GPT-4 等商业化大模型相比，Llama 3 的能力稍显不足。如果读者感兴趣，可以通过 OpenAI 的 API 使用 GPT-4 来评估生成的模型响应。相关代码笔记本已作为本书的补充材料提供，读者可访问以下 GitHub 链接获取更多信息：<br>
<a target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/03_model-evaluation/llm-instruction-eval-openai.ipynb">https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/03_model-evaluation/llm-instruction-eval-openai.ipynb</a></p>
</blockquote>
<p>为运行以下代码，请访问 <a target="_blank" rel="noopener" href="https://ollama.com">https://ollama.com</a> 并根据您的操作系统说明安装 Ollama：</p>
<ul>
<li>针对 macOS 和 Windows 用户：打开已下载的 Ollama 应用。如果提示安装命令行工具，请选择‘是’。</li>
<li>针对 Linux 用户：请使用 Ollama 网站提供的安装命令。</li>
</ul>
<p>在实现模型评估代码之前，我们需要先下载 Llama 3 模型，并通过命令行验证 Ollama 是否正常运行。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.20.png" alt=""></p>
<p>如图 7.20 所示，在另一终端中运行 Ollama 应用程序或 Ollama 服务后，请在命令行（不是在 Python 会话中）执行以下命令来运行具有 80 亿参数的 Llama 3 模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run llama3</span><br></pre></td></tr></table></figure>
<p>首次执行该命令时， Llama 3 模型（占用 4.7 GB 存储空间）将会自动下载。下载后的输出如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">pulling manifest</span><br><span class="line">pulling 6a0746a1ec1a... <span class="number">100</span>% ▕████████████████▏ <span class="number">4.7</span> GB</span><br><span class="line">pulling 4fa551d4f938... <span class="number">100</span>% ▕████████████████▏ <span class="number">12</span> KB</span><br><span class="line">pulling 8ab4849b038c... <span class="number">100</span>% ▕████████████████▏ <span class="number">254</span> B</span><br><span class="line">pulling 577073ffcc6c... <span class="number">100</span>% ▕████████████████▏ <span class="number">110</span> B</span><br><span class="line">pulling 3f8eb4da87fa... <span class="number">100</span>% ▕████████████████▏ <span class="number">485</span> B</span><br><span class="line">verifying sha256 digest</span><br><span class="line">writing manifest</span><br><span class="line">removing <span class="built_in">any</span> unused layers</span><br><span class="line">success</span><br></pre></td></tr></table></figure>
<blockquote>
<p>[!NOTE]</p>
<p><strong>Ollama 模型的替代方案</strong></p>
<p>需要注意的是，<code>ollama run llama3</code> 命令中的 <code>llama3</code> 指的是一个经过指令微调的 Llama 3 模型，具有 80 亿参数。运行 <code>llama3</code> 模型时，大约需要 16 GB 的内存。如果设备内存不足，建议尝试更小的模型，例如参数量为 38 亿的 <code>phi-3</code> 模型，该模型通过 <code>ollama run llama3</code> 命令加载，仅需约 8 GB 内存即可运行。</p>
<p>对于高性能计算机，你可以选择更大的 Llama 3 模型（700 亿参数版本），只需将 <code>llama3</code> 替换为 <code>llama3:70b</code>。但请注意，该模型对计算资源的需求会显著增加。</p>
</blockquote>
<p>当模型下载完成后，系统会显示一个命令行界面，用来与模型进行交互。例如，你可以试着向模型提问：“What do llamas eat?“</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>What do llamas eat?</span><br><span class="line">Llamas are ruminant animals, which means they have a four-chambered</span><br><span class="line">stomach <span class="keyword">and</span> eat plants that are high <span class="keyword">in</span> fiber. In the wild, llamas</span><br><span class="line">typically feed on:</span><br><span class="line"><span class="number">1.</span> Grasses: They love to graze on various types of grasses, including tall</span><br><span class="line">grasses, wheat, oats, <span class="keyword">and</span> barley.</span><br></pre></td></tr></table></figure>
<p>需要注意的是，Ollama 模型在当前版本中具有非确定性，因此你看到的响应可能会有所不同。</p>
<p>你可以通过输入 <code>/bye</code> 来结束当前的 ollama run llama3 会话。但请确保在本章剩余内容中，后台的 ollama serve 命令或 Ollama 应用程序继续保持运行。</p>
<p>以下代码用于验证 Ollama 会话是否正常运行，以便在评估上一节生成的测试集响应之前确保其可用性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> psutil</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_if_running</span>(<span class="params">process_name</span>):</span><br><span class="line">    running = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> proc <span class="keyword">in</span> psutil.process_iter([<span class="string">&quot;name&quot;</span>]):</span><br><span class="line">        <span class="keyword">if</span> process_name <span class="keyword">in</span> proc.info[<span class="string">&quot;name&quot;</span>]:</span><br><span class="line">            running = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> running</span><br><span class="line"></span><br><span class="line">ollama_running = check_if_running(<span class="string">&quot;ollama&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> ollama_running:</span><br><span class="line">    <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;Ollama not running. Launch ollama before proceeding.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Ollama running:&quot;</span>, check_if_running(<span class="string">&quot;ollama&quot;</span>))</span><br></pre></td></tr></table></figure>
<p>请确保执行以上代码的输出结果为 “Ollama running: True”。如果显示为 False，请检查是否已正确运行 <code>ollama serve</code> 命令或 Ollama 应用程序。</p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>在一个新的 Python 会话中运行代码</strong></p>
<p>如果你在第 7.7 节后关闭了 Python 会话，或者希望在新的会话中运行本章后续代码，可以执行以下代码。这些代码将加载我们在第 7.7 节中创建的指令和响应数据文件，同时重新定义之前使用的 <code>format_input</code> 函数（后续代码中还会使用 <code>tqdm</code> 进度条工具）。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">file_path = <span class="string">&quot;instruction-data-with-response.json&quot;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    test_data = json.load(file)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">format_input</span>(<span class="params">entry</span>):</span><br><span class="line">    instruction_text = (</span><br><span class="line">        <span class="string">f&quot;Below is an instruction that describes a task. &quot;</span></span><br><span class="line">        <span class="string">f&quot;Write a response that appropriately completes the request.&quot;</span></span><br><span class="line">        <span class="string">f&quot;\n\n### Instruction:\n<span class="subst">&#123;entry[<span class="string">&#x27;instruction&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    input_text = <span class="string">f&quot;\n\n### Input:\n<span class="subst">&#123;entry[<span class="string">&#x27;input&#x27;</span>]&#125;</span>&quot;</span> <span class="keyword">if</span> entry[<span class="string">&quot;input&quot;</span>] <span class="keyword">else</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> instruction_text + input_text</span><br></pre></td></tr></table></figure>
<p>一种替代 <code>ollama run</code> 命令与模型交互的方法是通过 Python 使用其 REST API。以下 <code>query_model</code> 函数示例演示了如何使用该 API：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 7.10 Querying a local Ollama model</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">query_model</span>(<span class="params">prompt, model=<span class="string">&quot;llama3&quot;</span>, url=<span class="string">&quot;http://localhost:11434/api/chat&quot;</span></span>):</span><br><span class="line">    data = &#123;                                                               <span class="comment">#A</span></span><br><span class="line">        <span class="string">&quot;model&quot;</span>: model,</span><br><span class="line">        <span class="string">&quot;seed&quot;</span>: <span class="number">123</span>, <span class="comment"># for deterministic responses</span></span><br><span class="line">        <span class="string">&quot;temperature&quot;</span>: <span class="number">0</span>, <span class="comment"># for deterministic responses</span></span><br><span class="line">        <span class="string">&quot;messages&quot;</span>: [</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    payload = json.dumps(data).encode(<span class="string">&quot;utf-8&quot;</span>)                             <span class="comment">#B</span></span><br><span class="line">    request = urllib.request.Request(url, data=payload, method=<span class="string">&quot;POST&quot;</span>)     <span class="comment">#C</span></span><br><span class="line">    request.add_header(<span class="string">&quot;Content-Type&quot;</span>, <span class="string">&quot;application/json&quot;</span>)                 <span class="comment">#C</span></span><br><span class="line"></span><br><span class="line">    response_data = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> urllib.request.urlopen(request) <span class="keyword">as</span> response:                      <span class="comment">#D</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            line = response.readline().decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            response_json = json.loads(line)</span><br><span class="line">            response_data += response_json[<span class="string">&quot;message&quot;</span>][<span class="string">&quot;content&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> response_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#A 将数据载荷创建为字典格式</span></span><br><span class="line"><span class="comment">#B 将字典转换为 JSON 格式字符串，并编码为字节数据</span></span><br><span class="line"><span class="comment">#C 创建请求对象，设置方法为 POST，并添加必要的请求头</span></span><br><span class="line"><span class="comment">#D 发送请求并接收响应</span></span><br></pre></td></tr></table></figure>
<p>在执行后续代码之前，请确保 Ollama 服务仍在运行。之前的代码应输出‘Ollama running: True’，以确保模型已启动并可以接收请求。</p>
<p>接下来，我们通过以下示例说明如何使用刚实现的 <code>query_llama</code> 函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = <span class="string">&quot;llama3&quot;</span></span><br><span class="line">result = query_model(<span class="string">&quot;What do Llamas eat?&quot;</span>, model)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<p>输出如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Llamas are ruminant animals, which means they have a four-chambered stomach that allows</span><br><span class="line">them to digest plant-based foods. Their diet typically consists of:</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> Grasses: Llamas love to graze on grasses, including tall grasses, short grasses, <span class="keyword">and</span></span><br><span class="line">even weeds.</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>了解了 <code>query_model</code> 函数的用法，我们现在可以通过一个<code>prompt</code>来评估微调模型的响应质量。具体来说，<code>prompt</code>要求 Llama 3 模型根据测试集中的参考响应，对微调模型的响应进行 0 到 100 的评分。</p>
<p>首先，我们将这种方法用于测试集的前三个样本，这些样本已在前文中分析过：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> entry <span class="keyword">in</span> test_data[:<span class="number">3</span>]:</span><br><span class="line">    prompt = (</span><br><span class="line">        <span class="string">f&quot;Given the input `<span class="subst">&#123;format_input(entry)&#125;</span>` &quot;</span></span><br><span class="line">        <span class="string">f&quot;and correct output `<span class="subst">&#123;entry[<span class="string">&#x27;output&#x27;</span>]&#125;</span>`, &quot;</span></span><br><span class="line">        <span class="string">f&quot;score the model response `<span class="subst">&#123;entry[<span class="string">&#x27;model_response&#x27;</span>]&#125;</span>`&quot;</span></span><br><span class="line">        <span class="string">f&quot; on a scale from 0 to 100, where 100 is the best score. &quot;</span></span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nDataset response:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&gt;&gt;&quot;</span>, entry[<span class="string">&#x27;output&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nModel response:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&gt;&gt;&quot;</span>, entry[<span class="string">&quot;model_response&quot;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nScore:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&gt;&gt;&quot;</span>, query_model(prompt))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n-------------------------&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这将打印出类似于以下的输出（请注意，截至本文写作时，Ollama 不是完全确定性的，因此生成的文本可能会有所不同）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">Dataset response:</span><br><span class="line">&gt;&gt; The car <span class="keyword">is</span> <span class="keyword">as</span> fast <span class="keyword">as</span> lightning.</span><br><span class="line"></span><br><span class="line">Model response:</span><br><span class="line">&gt;&gt; The car <span class="keyword">is</span> <span class="keyword">as</span> fast <span class="keyword">as</span> a bullet.</span><br><span class="line"></span><br><span class="line">Score:</span><br><span class="line">&gt;&gt; A scoring task!</span><br><span class="line"></span><br><span class="line">To evaluate the model response <span class="string">&quot;The car is as fast as a bullet.&quot;</span>, I<span class="string">&#x27;ll consider how well</span></span><br><span class="line"><span class="string">it follows the instruction and uses a simile that&#x27;</span>s coherent, natural-sounding, <span class="keyword">and</span></span><br><span class="line">effective <span class="keyword">in</span> conveying the idea of speed.</span><br><span class="line"></span><br><span class="line">Here are some factors to consider:</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> **Follows instruction**: Yes, the model uses a simile to rewrite the sentence.</span><br><span class="line"><span class="number">2.</span> **Coherence <span class="keyword">and</span> naturalness**: The comparison between the ca<span class="string">r&#x27;s speed and a bullet is</span></span><br><span class="line"><span class="string">common and easy to understand. It&#x27;</span>s a good choice <span class="keyword">for</span> a simile that conveys the idea of</span><br><span class="line">rapid movement.</span><br><span class="line"><span class="number">3.</span> **Effectiveness <span class="keyword">in</span> conveying idea of speed**: A bullet <span class="keyword">is</span> known <span class="keyword">for</span> its high</span><br><span class="line">velocity, which makes it an excellent choice to describe a fast-moving car.</span><br><span class="line"></span><br><span class="line">Considering these factors, I<span class="string">&#x27;d score the model response &quot;The car is as fast as a</span></span><br><span class="line"><span class="string">bullet.&quot; around 85 out of 100. The simile is well-chosen, coherent, and effectively</span></span><br><span class="line"><span class="string">conveys the idea of speed. Well done, model!</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">-------------------------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Dataset response:</span></span><br><span class="line"><span class="string">&gt;&gt; The type of cloud typically associated with thunderstorms is cumulonimbus.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Model response:</span></span><br><span class="line"><span class="string">&gt;&gt; The type of cloud associated with thunderstorms is a cumulus cloud.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Score:</span></span><br><span class="line"><span class="string">&gt;&gt; A scoring task!</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">I&#x27;</span>ll evaluate the model<span class="string">&#x27;s response based on its accuracy and relevance to the original</span></span><br><span class="line"><span class="string">instruction.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">**Accuracy:** The model&#x27;</span>s response <span class="keyword">is</span> partially correct. Cumulus clouds are indeed</span><br><span class="line">associated <span class="keyword">with</span> fair weather <span class="keyword">and</span> <span class="keyword">not</span> typically linked to thunderstorms. The correct</span><br><span class="line">answer, cumulonimbus, <span class="keyword">is</span> a <span class="built_in">type</span> of cloud that <span class="keyword">is</span> closely tied to thunderstorm formation.</span><br><span class="line"></span><br><span class="line">**Relevance:** The model<span class="string">&#x27;s response is somewhat relevant, as it mentions clouds in the</span></span><br><span class="line"><span class="string">context of thunderstorms. However, the specific type of cloud mentioned (cumulus) is not</span></span><br><span class="line"><span class="string">directly related to thunderstorms.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Considering these factors, I would score the model response a **40 out of 100**. While</span></span><br><span class="line"><span class="string">the response attempts to address the instruction, it provides an incorrect answer and</span></span><br><span class="line"><span class="string">lacks relevance to the original question.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">-------------------------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Dataset response:</span></span><br><span class="line"><span class="string">&gt;&gt; Jane Austen.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Model response:</span></span><br><span class="line"><span class="string">&gt;&gt; The author of &#x27;</span>Pride <span class="keyword">and</span> Prejudice<span class="string">&#x27; is Jane Austen.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Score:</span></span><br><span class="line"><span class="string">&gt;&gt; A simple one!</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">My model response: &quot;The author of &#x27;</span>Pride <span class="keyword">and</span> Prejudice<span class="string">&#x27; is Jane Austen.&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Score: **99**</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Reasoning:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">* The response directly answers the question, providing the correct name of the author.</span></span><br><span class="line"><span class="string">* The sentence structure is clear and easy to understand.</span></span><br><span class="line"><span class="string">* There&#x27;</span>s no room <span class="keyword">for</span> misinterpretation <span class="keyword">or</span> ambiguity.</span><br><span class="line"></span><br><span class="line">Overall, a perfect score!</span><br><span class="line"></span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure>
<p>通过生成的回答可以看出，Llama 3 模型具有合理的评估能力，即使答案不完全正确，也能够给予部分分数。例如，在对“cumulus cloud”这一回答的评估中，模型能够识别答案中的部分正确性，并对此作出相应评价。</p>
<p>以上的<code>promp返回的不仅有评分，还包括高度详细的评价内容。我们可以修改</code>prompt`，使其只生成 0 到 100 的整数评分（其中 100 表示最高分）。这样一来，我们就可以计算模型的平均分，将其作为对模型性能更简洁且量化的评估。</p>
<p>下面的 <code>generate_model_scores</code> 函数使用了一个修改后的<code>prompt</code>，要求模型‘仅回复整数’：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing 7.11 Evaluating the instruction finetuning LLM</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_model_scores</span>(<span class="params">json_data, json_key, model=<span class="string">&quot;llama3&quot;</span></span>):</span><br><span class="line">    scores = []</span><br><span class="line">    <span class="keyword">for</span> entry <span class="keyword">in</span> tqdm(json_data, desc=<span class="string">&quot;Scoring entries&quot;</span>):</span><br><span class="line">        prompt = (</span><br><span class="line">            <span class="string">f&quot;Given the input `<span class="subst">&#123;format_input(entry)&#125;</span>` &quot;</span></span><br><span class="line">            <span class="string">f&quot;and correct output `<span class="subst">&#123;entry[<span class="string">&#x27;output&#x27;</span>]&#125;</span>`, &quot;</span></span><br><span class="line">            <span class="string">f&quot;score the model response `<span class="subst">&#123;entry[json_key]&#125;</span>`&quot;</span></span><br><span class="line">            <span class="string">f&quot; on a scale from 0 to 100, where 100 is the best score. &quot;</span></span><br><span class="line">            <span class="string">f&quot;Respond with the integer number only.&quot;</span>                        <span class="comment">#A</span></span><br><span class="line">        )</span><br><span class="line">        score = query_model(prompt, model)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            scores.append(<span class="built_in">int</span>(score))</span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Could not convert score: <span class="subst">&#123;score&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line"><span class="comment">#A 修改后的指令设置为仅返回分数。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let&#x27;s now apply the generate_model_scores function to the entire test_data set, which takes about 1 minute on a M3 Macbook Air:</span></span><br><span class="line">scores = generate_model_scores(test_data, <span class="string">&quot;model_response&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of scores: <span class="subst">&#123;<span class="built_in">len</span>(scores)&#125;</span> of <span class="subst">&#123;<span class="built_in">len</span>(test_data)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Average score: <span class="subst">&#123;<span class="built_in">sum</span>(scores)/<span class="built_in">len</span>(scores):<span class="number">.2</span>f&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Scoring entries: <span class="number">100</span>%|████████████████████████| <span class="number">110</span>/<span class="number">110</span> [01:<span class="number">10</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">1.56</span>it/s]</span><br><span class="line">Number of scores: <span class="number">110</span> of <span class="number">110</span></span><br><span class="line">Average score: <span class="number">54.16</span></span><br></pre></td></tr></table></figure>
<p>评估结果显示，我们的微调模型平均得分超过 50，这为与其他模型进行对比提供了一个有用的基准，同时也可以基于该基准尝试不同的训练配置，以进一步提升模型的性能。</p>
<p>需要注意的是，撰写本文时，Ollama 的结果并非完全固定，这意味着您得到的分数可能会与上述结果略有不同。为了获得更稳定的结果，可以重复多次评估，并取平均值。</p>
<p>为了提升模型性能，我们可探索多种策略，例如：</p>
<ul>
<li>在微调阶段，可以通过调整超参数（如学习率、批次大小和训练轮数）来优化模型性能。</li>
<li>通过扩大训练数据集规模或丰富样本的多样性，以覆盖更广泛的主题和风格。</li>
<li>尝试不同的提示或指令格式，以更有效地引导模型的回答。</li>
<li>考虑使用更大的预训练模型，这类模型可能具有更强的能力，能够捕捉复杂模式并生成更准确的响应。</li>
</ul>
<blockquote>
<p>[!NOTE]</p>
<p><strong>LLaMA 3 模型性能</strong></p>
<p>作为参考，使用本节描述的方法，Llama 3 8B 基础模型（未经过任何微调）在测试集上的平均得分为 58.51。而经过在通用指令遵循数据集上微调的 Llama 3 8B 指令模型，在测试集上的平均得分高达 82.6，表现相当出色。</p>
</blockquote>
<blockquote>
<p>[!NOTE]</p>
<p><strong>练习 7.4：使用 LoRA 实现参数高效微调</strong></p>
<p>为了更高效地对 LLM 进行指令微调，请修改本章的代码，采用附录 E 中的 LoRA 方法。然后，对比修改前后训练时长和模型性能。</p>
</blockquote>
<h2 id="7-9-结语">7.9 结语</h2>
<p>本章总结了大语言模型（LLM）开发流程的关键步骤，包括实现 LLM 架构、预训练模型以及针对特定任务的微调，具体内容可参考图 7.21。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/chapter7/figure7.21.png" alt=""></p>
<p>接下来的小节将为你提供一些思路，帮助你在完成图 7.21 中展示的关键步骤后，进一步探索下去。</p>
<h3 id="7-9-1-接下来如何做？">7.9.1 接下来如何做？</h3>
<p>尽管我们已经讲解了模型训练的核心步骤（详见图 7.21），但在完成指令微调后，还可以选择进行偏好微调（Preference Finetuning）。偏好微调对于定制模型以更好地符合特定用户的需求尤为有用。如果您希望进一步了解这一过程，可以参考书籍补充资源中的 GitHub 仓库（<a target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch07/04_preference-tuning-withdpo">链接</a>），查看 <code>04_preference-tuning-with-dpo</code> 文件夹。</p>
<p>除了书中涵盖的主要内容外，GitHub 仓库还提供了丰富的额外材料，这些内容可能对您非常有价值。如需了解更多，请访问仓库 README 页面的“Bonus Material”部分：<a target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material%E3%80%82">https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material。</a></p>
<h3 id="7-9-2-如何在快速变化的前沿领域中保持领先">7.9.2 如何在快速变化的前沿领域中保持领先</h3>
<p>人工智能和大语言模型的研究领域正在迅速发展（许多人可能觉得这非常令人兴奋）。想要了解最新进展，可以浏览 arXiv 上的最新研究论文（网址：<a target="_blank" rel="noopener" href="https://arxiv.org/list/cs.LG/recent">https://arxiv.org/list/cs.LG/recent</a>）。此外，许多研究人员和从业者也会在社交媒体平台（如 X（原 Twitter）和 Reddit）上积极分享和讨论最新动态。尤其是 Reddit 的 r/LocalLLaMA 版块，是了解社区动态以及最新工具和趋势的好资源。</p>
<p>我会定期在博客上分享关于大语言模型（LLM）研究的最新动态和见解，您可以通过以下地址访问：<a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com">https://magazine.sebastianraschka.com</a> 和 <a target="_blank" rel="noopener" href="https://sebastianraschka.com/blog/%E3%80%82">https://sebastianraschka.com/blog/。</a></p>
<p>感谢你一路同行，祝愿你在未来的大语言模型和人工智能领域的探索中一切顺利！</p>
<h2 id="7-10-本章摘要">7.10 本章摘要</h2>
<ul>
<li>指令微调的过程旨在将预训练的大语言模型调整为能够遵循人类指令并生成预期回答。</li>
<li>准备数据集需要下载指令-响应数据集，对数据进行格式化，并划分为训练集、验证集和测试集。</li>
<li>自定义的<code>collate</code>函数用于构建训练批次，处理过程包括对序列数据进行填充，生成目标 token 的 ID，并对填充的 token 进行掩码处理。</li>
<li>我们加载了一个具有 3.55 亿参数的预训练 GPT-2 medium 模型，作为指令微调的起点。</li>
<li>预训练模型在指令数据集上进行了微调，训练方式类似于预训练的循环。</li>
<li>评估涉及在测试集上提取模型响应并对其进行评分，例如，使用另一个LLM进行评分。</li>
<li>Ollama 应用利用一个 80 亿参数的 Llama 模型，可以对微调模型在测试集上的响应进行自动评分，并通过平均分来量化模型的性能表现。</li>
</ul>
</div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/my-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">欣冻</div><div class="author-info-description">博客, 技术, 生活</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/bc9ae956.html" title="数据结构与算法实现"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/12/20/ClnnUM.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法实现"/></a><div class="content"><a class="title" href="/posts/bc9ae956.html" title="数据结构与算法实现">数据结构与算法实现</a><time datetime="2025-12-28T10:00:00.000Z" title="发表于 2025-12-28 18:00:00">2025-12-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/a6d4d6d1.html" title="JavaGame实现笔记"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/12/20/Cln2Wr.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="JavaGame实现笔记"/></a><div class="content"><a class="title" href="/posts/a6d4d6d1.html" title="JavaGame实现笔记">JavaGame实现笔记</a><time datetime="2025-12-25T04:24:02.000Z" title="发表于 2025-12-25 12:24:02">2025-12-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/e329799.html" title="C++后端服务器实现笔记"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/12/03/CsbqBg.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C++后端服务器实现笔记"/></a><div class="content"><a class="title" href="/posts/e329799.html" title="C++后端服务器实现笔记">C++后端服务器实现笔记</a><time datetime="2025-12-01T08:00:00.000Z" title="发表于 2025-12-01 16:00:00">2025-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/4260ab42.html" title="Transformer大语言模型架构原理学习笔记"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/11/17/CfYA2b.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer大语言模型架构原理学习笔记"/></a><div class="content"><a class="title" href="/posts/4260ab42.html" title="Transformer大语言模型架构原理学习笔记">Transformer大语言模型架构原理学习笔记</a><time datetime="2025-11-17T12:51:00.000Z" title="发表于 2025-11-17 20:51:00">2025-11-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/6f4fa4e7.html" title="快速幂、逆元与组合数学"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/12/19/CdIGkr.md.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="快速幂、逆元与组合数学"/></a><div class="content"><a class="title" href="/posts/6f4fa4e7.html" title="快速幂、逆元与组合数学">快速幂、逆元与组合数学</a><time datetime="2025-10-31T15:48:33.000Z" title="发表于 2025-10-31 23:48:33">2025-10-31</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/c%E8%AF%AD%E8%A8%80/"><span class="card-category-list-name">c语言</span><span class="card-category-list-count">1</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/python-%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91-%E9%9F%B3%E6%B8%B8/" style="font-size: 1.1em; color: #999">python,游戏开发,音游</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" style="font-size: 1.1em; color: #999">每日一题</a> <a href="/tags/hexo-github-blog-node-js-npm-git-%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2-hexo%E9%83%A8%E7%BD%B2/" style="font-size: 1.1em; color: #999">hexo, github, blog, node.js,npm,git,部署博客,hexo部署</a> <a href="/tags/%E5%A4%A7%E5%AE%B6%E5%A5%BD%EF%BC%8C%E6%88%91%E6%98%AF%E8%BF%B7%E8%B7%AF%E7%9A%84%E5%B0%8F%E6%9C%8B%E5%8F%8B/" style="font-size: 1.1em; color: #999">大家好，我是迷路的小朋友</a> <a href="/tags/%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%EF%BC%8Cbutterfly/" style="font-size: 1.1em; color: #999">一键部署，butterfly</a> <a href="/tags/c%E8%AF%AD%E8%A8%80-%E5%AD%A6%E4%B9%A0/" style="font-size: 1.1em; color: #999">c语言,学习</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 1.1em; color: #999">算法</a> <a href="/tags/Java/" style="font-size: 1.1em; color: #999">Java</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      <a class="card-more-btn" href="/archives/"
            title="查看更多">
            <i class="fas fa-angle-right"></i>
          </a>
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/12/">
            <span class="card-archive-list-date">
              十二月 2025
            </span>
            <span class="card-archive-list-count">3</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/11/">
            <span class="card-archive-list-date">
              十一月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/10/">
            <span class="card-archive-list-date">
              十月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/08/">
            <span class="card-archive-list-date">
              八月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/07/">
            <span class="card-archive-list-date">
              七月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/05/">
            <span class="card-archive-list-date">
              五月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/04/">
            <span class="card-archive-list-date">
              四月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/03/">
            <span class="card-archive-list-date">
              三月 2025
            </span>
            <span class="card-archive-list-count">7</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">26</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2026-01-07T12:37:20.075Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://i.imgs.ovh/2025/07/03/qLFy9.png);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2026 By 欣冻</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://blog-twikoo.xindon.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = (el = document, path = location.pathname) => {
    twikoo.init({
      el: el.querySelector('#twikoo-wrap'),
      envId: 'https://blog-twikoo.xindon.top/',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      },
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    

    isShuoshuo && (window.shuoshuoComment.destroyTwikoo = () => {
      if (el.children.length) {
        el.innerHTML = ''
        el.classList.add('no-comment')
      }
    })
  }

  const loadTwikoo = (el, path) => {
    if (typeof twikoo === 'object') setTimeout(() => init(el, path), 0)
    else btf.getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(() => init(el, path))
  }

  if (isShuoshuo) {
    'Twikoo' === 'Twikoo'
      ? window.shuoshuoComment = { loadComment: loadTwikoo }
      : window.loadOtherComment = loadTwikoo
    return
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><div class="aplayer no-destroy" data-id="13348674056" data-server="netease" data-type="playlist"   data-order="list" data-fixed="true" data-preload="auto" data-autoplay="false" data-mutex="true" ></div><script src="https://cdn.jsdelivr.net/npm/mermaid@10.2.4/dist/mermaid.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v 7.3.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v5.2.2" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://www.jsdelivr.com/" style="margin-inline:5px" data-title="本站使用JsDelivr为静态资源提供CDN加速" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.js"></script><script async src="/js/ali_font.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('article-sort-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__slideInRight');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/miku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>