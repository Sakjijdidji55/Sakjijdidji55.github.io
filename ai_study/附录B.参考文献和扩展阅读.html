<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>附录B.参考文献和扩展阅读 | 迷路的小朋友</title><meta name="author" content="欣冻"><meta name="copyright" content="欣冻"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="附录B. 参考文献和扩展阅读  附录B. 参考文献和扩展阅读  第一章 第二掌 第三章 第四章 第五章 第六章 第七章     第一章 正如彭博社的一个团队通过从零开始在金融数据上预训练的一个 GPT 版本所展示的那样，定制构建的 LLM 能够胜过通用 LLM。这个定制的 LLM 在金融任务上优于 ChatGPT，同时在通用 LLM 基准测试中保持了良好的性能：  BloombergGPT：金融领">
<meta property="og:type" content="website">
<meta property="og:title" content="附录B.参考文献和扩展阅读">
<meta property="og:url" content="https://sakjijdidji55.github.io/ai_study/%E9%99%84%E5%BD%95B.%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%E5%92%8C%E6%89%A9%E5%B1%95%E9%98%85%E8%AF%BB.html">
<meta property="og:site_name" content="迷路的小朋友">
<meta property="og:description" content="附录B. 参考文献和扩展阅读  附录B. 参考文献和扩展阅读  第一章 第二掌 第三章 第四章 第五章 第六章 第七章     第一章 正如彭博社的一个团队通过从零开始在金融数据上预训练的一个 GPT 版本所展示的那样，定制构建的 LLM 能够胜过通用 LLM。这个定制的 LLM 在金融任务上优于 ChatGPT，同时在通用 LLM 基准测试中保持了良好的性能：  BloombergGPT：金融领">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sakjijdidji55.github.io/img/my-icon.png">
<meta property="article:published_time" content="2025-10-26T08:00:00.000Z">
<meta property="article:modified_time" content="2025-10-26T08:00:25.616Z">
<meta property="article:author" content="欣冻">
<meta property="article:tag" content="博客, 技术, 生活, tanxin, tanxin.me, 吃好喝好, 玩好, 睡好, 迷路的小朋友,tanxin55">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sakjijdidji55.github.io/img/my-icon.png"><script type="application/ld+json"></script><link rel="shortcut icon" href="/img/logo.ico"><link rel="canonical" href="https://sakjijdidji55.github.io/ai_study/%E9%99%84%E5%BD%95B.%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%E5%92%8C%E6%89%A9%E5%B1%95%E9%98%85%E8%AF%BB.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '附录B.参考文献和扩展阅读',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'page'
}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="迷路的小朋友" type="application/atom+xml">
</head><body><div id="web_bg" style="background-image: url(https://i.imgs.ovh/2025/07/03/qLFy9.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/my-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-home"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-home-page" id="page-header" style="background-image: url(https://img.picgo.net/2025/04/05/2025-2-22fe10c0c4fb1bc202.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.png" alt="Logo"><span class="site-name">迷路的小朋友</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-home"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="page-site-info"><h1 id="site-title">附录B.参考文献和扩展阅读</h1></div></header><main class="layout" id="content-inner"><div id="page"><div class="container" id="article-container"><h1>附录B. 参考文献和扩展阅读</h1>
<ul>
<li><a href="#%E9%99%84%E5%BD%95b-%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%E5%92%8C%E6%89%A9%E5%B1%95%E9%98%85%E8%AF%BB">附录B. 参考文献和扩展阅读</a>
<ul>
<li><a href="#%E7%AC%AC%E4%B8%80%E7%AB%A0">第一章</a></li>
<li><a href="#%E7%AC%AC%E4%BA%8C%E6%8E%8C">第二掌</a></li>
<li><a href="#%E7%AC%AC%E4%B8%89%E7%AB%A0">第三章</a></li>
<li><a href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0">第四章</a></li>
<li><a href="#%E7%AC%AC%E4%BA%94%E7%AB%A0">第五章</a></li>
<li><a href="#%E7%AC%AC%E5%85%AD%E7%AB%A0">第六章</a></li>
<li><a href="#%E7%AC%AC%E4%B8%83%E7%AB%A0">第七章</a></li>
</ul>
</li>
</ul>
<hr>
<h2 id="第一章">第一章</h2>
<p><strong>正如彭博社的一个团队通过从零开始在金融数据上预训练的一个 GPT 版本所展示的那样，定制构建的 LLM 能够胜过通用 LLM。这个定制的 LLM 在金融任务上优于 ChatGPT，同时在通用 LLM 基准测试中保持了良好的性能：</strong></p>
<ul>
<li>BloombergGPT：金融领域的大型语言模型 (2023)，吴等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.17564">https://arxiv.org/abs/2303.17564</a></li>
</ul>
<br />
<p><strong>现有的 LLM 也可以通过适配和微调来胜过通用 LLM，正如 Google Research 和 Google DeepMind 的团队在医疗领域所展示的那样：</strong></p>
<ul>
<li>使用大型语言模型实现专家级医疗问答 (2023)，Singhal 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.09617">https://arxiv.org/abs/2305.09617</a></li>
</ul>
<br />
<p><strong>提出原始 Transformer 架构的论文：</strong></p>
<ul>
<li>Attention Is All You Need (2017)，Vaswani 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
</ul>
<br />
<p><strong>最初的编码器式 Transformer，称为 BERT：</strong></p>
<ul>
<li>BERT：用于语言理解的深度双向 Transformer 的预训练 (2018)，Devlin 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></li>
</ul>
<br />
<p><strong>描述解码器式 GPT-3 模型的论文，该模型启发了现代 LLM，并将作为本书中从零开始实现 LLM 的模板：</strong></p>
<ul>
<li>Language Models are Few-Shot Learners (2020)，Brown 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></li>
</ul>
<br />
<p><strong>用于图像分类的原始 Vision Transformer，它表明 Transformer 架构不仅限于文本输入：</strong></p>
<ul>
<li>An <a target="_blank" rel="noopener" href="https://myblog.xindon.top/Image">https://myblog.xindon.top/Image</a> is Worth 16x16 Words: Transformers for <a target="_blank" rel="noopener" href="https://myblog.xindon.top/Image">https://myblog.xindon.top/Image</a> Recognition at Scale (2020)，Dosovitskiy 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></li>
</ul>
<br />
<p><strong>两种实验性的（但不太流行的）LLM 架构，它们作为并非所有 LLM 都必须基于 Transformer 架构的示例：</strong></p>
<ul>
<li>RWKV：Transformer 时代 RNN 的革新 (2023)，Peng 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.13048">https://arxiv.org/abs/2305.13048</a></li>
<li>Hyena Hierarchy：迈向更大的卷积语言模型 (2023)，Poli 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.10866">https://arxiv.org/abs/2302.10866</a> Mamba：具有选择性状态空间的线性时间序列建模 (2023)，Gu 和 Dao 著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00752">https://arxiv.org/abs/2312.00752</a></li>
</ul>
<br />
<p><strong>Meta AI 的模型是类似 GPT 的流行实现，与 GPT-3 和 ChatGPT 相比，它是公开可用的：</strong></p>
<ul>
<li>Llama 2：开放基础模型和微调的聊天模型 (2023)，Touvron 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.092881">https://arxiv.org/abs/2307.092881</a></li>
</ul>
<br />
<p><strong>对于对第 1.5 节中数据集参考文献感兴趣的读者，这篇论文介绍了 Eleuther AI 策划的公开可用的 The Pile 数据集：</strong></p>
<ul>
<li>The Pile：用于语言建模的 800GB 多样化文本数据集 (2020)，Gao 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.00027">https://arxiv.org/abs/2101.00027</a></li>
</ul>
<br />
<p><strong>以下论文提供了在第 1.6 节中提及并在第 7 章中更详细讨论的用于微调 GPT-3 的 InstructGPT 的参考文献：</strong></p>
<ul>
<li>使用人类反馈训练语言模型以遵循指令 (2022)，Ouyang 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a></li>
</ul>
<br />
<h2 id="第二掌">第二掌</h2>
<ul>
<li>机器学习问答 (2023)，Sebastian Raschka 著，<a target="_blank" rel="noopener" href="https://leanpub.com/machine-learning-q-and-ai">https://leanpub.com/machine-learning-q-and-ai</a></li>
</ul>
<br />
<p><strong>以下论文更深入地讨论了字节对编码是如何作为一种分词方法使用的：</strong></p>
<ul>
<li>使用子词单元进行罕见词的神经机器翻译 (2015)，Sennrich 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.07909">https://arxiv.org/abs/1508.07909</a></li>
</ul>
<br />
<p><strong>用于训练 GPT-2 的字节对编码分词器的代码已由 OpenAI 开源：</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/openai/gpt-2/blob/master/src/encoder.py">https://github.com/openai/gpt-2/blob/master/src/encoder.py</a></li>
</ul>
<br />
<p><strong>OpenAI 提供了一个交互式 Web UI 来演示 GPT 模型中的字节对分词器是如何工作的：</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a></li>
</ul>
<br />
<p><strong>对于那些有兴趣从头开始编写和训练 BPE 分词器的读者，Andrej Karpathy 的 GitHub 仓库 minbpe 提供了一个最小且易于理解的实现：</strong></p>
<ul>
<li>一个 BPE 分词器的最小实现，<a target="_blank" rel="noopener" href="https://github.com/karpathy/minbpe">https://github.com/karpathy/minbpe</a></li>
</ul>
<br />
<p><strong>对于那些有兴趣研究其他一些流行的 LLM 使用的替代分词方案的读者，可以在 SentencePiece 和 WordPiece 的论文中找到更多信息：</strong></p>
<ul>
<li>SentencePiece：一种用于神经文本处理的简单且与语言无关的子词分词器和反分词器 (2018)，Kudo 和 Richardson 著，<a target="_blank" rel="noopener" href="https://aclanthology.org/D18-2012/">https://aclanthology.org/D18-2012/</a></li>
<li>快速 WordPiece 分词 (2020)，Song 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.15524">https://arxiv.org/abs/2012.15524</a></li>
</ul>
<br />
<h2 id="第三章">第三章</h2>
<p><strong>对于有兴趣了解更多关于 RNN 和语言翻译的 Bahdanau 注意力的读者，可以在以下论文中找到详细的见解：</strong></p>
<ul>
<li>通过联合学习对齐和翻译进行神经机器翻译 (2014)，Bahdanau、Cho 和 Bengio 著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a></li>
</ul>
<br />
<p><strong>自注意力作为缩放点积注意力的概念是在最初的 Transformer 论文中提出的：</strong></p>
<ul>
<li>Attention Is All You Need (2017)，Vaswani 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
</ul>
<br />
<p><strong>FlashAttention 是一种高效的自注意力机制实现，它通过优化内存访问模式来加速计算过程。FlashAttention 在数学上与标准的自注意力机制相同，但优化了计算过程以提高效率：</strong></p>
<ul>
<li>FlashAttention：具有 IO 感知的快速且内存高效的精确注意力 (2022)，Dao 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a></li>
<li>FlashAttention-2：具有更好并行性和工作分区的更快注意力 (2023)，Dao 著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.08691">https://arxiv.org/abs/2307.08691</a></li>
</ul>
<br />
<p><strong>PyTorch 实现了一个用于自注意力和因果注意力的函数，该函数为了提高效率而支持 FlashAttention。此功能目前为测试版，可能会发生更改：</strong></p>
<ul>
<li><code>scaled_dot_product_attention</code> 文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html</a></li>
</ul>
<br />
<p><strong>PyTorch 还实现了一个基于 <code>scaled_dot_product</code> 函数的高效 <code>MultiHeadAttention</code> 类：</strong></p>
<ul>
<li><code>MultiHeadAttention</code> 文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html">https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html</a></li>
</ul>
<br />
<p><strong>Dropout 是一种在神经网络中使用的正则化技术，通过在训练期间随机丢弃神经网络中的单元（及其连接）来防止过拟合：</strong></p>
<ul>
<li>Dropout：一种防止神经网络过拟合的简单方法 (2014)，Srivastava 等人著，<a target="_blank" rel="noopener" href="https://jmlr.org/papers/v15/srivastava14a.html">https://jmlr.org/papers/v15/srivastava14a.html</a></li>
</ul>
<br />
<p><strong>虽然在实践中，基于缩放点积注意力的多头注意力仍然是最常见的自注意力变体，但作者发现，即使没有值权重矩阵和投影层，也可能获得良好的性能：</strong></p>
<ul>
<li>简化 Transformer 模块 (2023)，He 和 Hofmann 著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.01906">https://arxiv.org/abs/2311.01906</a></li>
</ul>
<br />
<h2 id="第四章">第四章</h2>
<p><strong>这篇名为《层归一化》的论文介绍了一种技术，通过归一化隐藏层内神经元的输入总和来稳定神经网络的隐藏状态动态，与先前发表的方法相比，显著减少了训练时间：</strong></p>
<ul>
<li>层归一化 (2016)，Ba、Kiros 和 Hinton 著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a></li>
</ul>
<br />
<p><strong>原始 Transformer 模型中使用的 Post-LayerNorm 在自注意力和前馈网络之后应用层归一化。相比之下，像 GPT-2 和更新的 LLM 中采用的 Pre-LayerNorm 在这些组件之前应用层归一化，这可以带来更稳定的训练动态，并且在某些情况下已被证明可以提高性能，如下列论文所述：</strong></p>
<ul>
<li>关于 Transformer 架构中的层归一化 (2020)，Xiong 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.04745">https://arxiv.org/abs/2002.04745</a></li>
<li>ResiDual：具有双重残差连接的 Transformer (2023)，Tie 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.14802">https://arxiv.org/abs/2304.14802</a></li>
</ul>
<br />
<p><strong>由于其更高的计算效率，RMSNorm 是现代 LLM 中使用的一种流行的 LayerNorm 变体。此变体通过仅使用输入的均方根对输入进行归一化来简化归一化过程，而无需在平方之前减去均值。这意味着它在计算尺度之前不会对数据进行中心化。以下论文更详细地介绍了 RMSNorm：</strong></p>
<ul>
<li>Root Mean Square Layer Normalization (2019)，Zhang 和 Sennrich 著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.07467">https://arxiv.org/abs/1910.07467</a></li>
</ul>
<br />
<p><strong>GELU（高斯误差线性单元）激活函数结合了经典 ReLU 激活函数和正态分布累积分布函数的特性来建模层输出，从而在深度学习模型中实现随机正则化和非线性，如下列论文所述：</strong></p>
<ul>
<li>高斯误差线性单元 (GELUs) (2016)，Hendricks 和 Gimpel 著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a></li>
</ul>
<br />
<p><strong>GPT-2 的论文介绍了一系列不同规模的基于 Transformer 的 LLM——参数量分别为 1.24 亿、3.55 亿、7.74 亿和 15 亿：</strong></p>
<ul>
<li>语言模型是无监督的多任务学习者 (2019)，Radford 等人著，<a target="_blank" rel="noopener" href="https://d4mucfpksywv.cloudfront.net/better-languagemodels/language_models_are_unsupervised_multitask_learners.pdf">https://d4mucfpksywv.cloudfront.net/better-languagemodels/language_models_are_unsupervised_multitask_learners.pdf</a></li>
</ul>
<br />
<p><strong>OpenAI 的 GPT-3 从根本上使用了与 GPT-2 相同的架构，只不过其最大的版本（1750 亿参数）比最大的 GPT-2 模型大了 100 倍，并且在更多的数据上进行了训练。感兴趣的读者可以参考 OpenAI 的官方 GPT-3 论文以及 Lambda Labs 的技术概述，后者计算得出，在单个 RTX 8000 消费级 GPU 上训练 GPT-3 需要 665 年：</strong></p>
<ul>
<li>语言模型是少样本学习者 (2023)，Brown 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></li>
<li>OpenAI 的 GPT-3 语言模型：技术概述，<a target="_blank" rel="noopener" href="https://lambdalabs.com/blog/demystifying-gpt-3">https://lambdalabs.com/blog/demystifying-gpt-3</a></li>
</ul>
<br />
<p><strong>NanoGPT 是一个代码仓库，其中包含一个极简但高效的 GPT-2 模型实现，类似于本书中实现的模型。虽然本书中的代码与 nanoGPT 不同，但该仓库启发了将大型 GPT Python 父类实现重组为更小的子模块：</strong></p>
<ul>
<li>NanoGPT，一个用于训练中等规模 GPT 的仓库，<a target="_blank" rel="noopener" href="https://github.com/karpathy/nanoGPT">https://github.com/karpathy/nanoGPT</a></li>
</ul>
<br />
<p><strong>一篇信息丰富的博客文章指出，当上下文大小小于 32,000 个 token 时，LLM 中的大部分计算都花费在前馈层而不是注意力层：</strong></p>
<ul>
<li>《从长远来看（上下文）》，作者 Harm de Vries，<a target="_blank" rel="noopener" href="https://www.harmdevries.com/post/context-length/">https://www.harmdevries.com/post/context-length/</a></li>
</ul>
<br />
<h2 id="第五章">第五章</h2>
<p><strong>作者的一个视频讲座，详细介绍了损失函数并应用对数变换以使其更易于进行数学优化：</strong></p>
<ul>
<li>L8.2 逻辑回归损失函数，<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=GxJe0DZvydM">https://www.youtube.com/watch?v=GxJe0DZvydM</a></li>
</ul>
<br />
<p><strong>以下两篇论文详细介绍了用于预训练 LLM 的数据集、超参数和架构细节：</strong></p>
<ul>
<li>Pythia：用于分析跨训练和扩展的大型语言模型的套件 (2023)，Biderman 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.01373">https://arxiv.org/abs/2304.01373</a></li>
<li>OLMo：加速语言模型科学 (2024)，Groeneveld 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.00838">https://arxiv.org/abs/2402.00838</a></li>
</ul>
<br />
<p>本书提供的以下补充代码包含从古腾堡计划准备 60,000 本公共领域书籍以用于 LLM 训练的说明：</p>
<ul>
<li>在古腾堡数据集上预训练 GPT，<a target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/03_bonus_pretraining_on_gutenberg">https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/03_bonus_pretraining_on_gutenberg</a></li>
</ul>
<br />
<p><strong>第五章讨论了 LLM 的预训练，附录 D 涵盖了更高级的训练函数，例如线性预热和余弦退火。以下论文发现，类似的技术可以成功地应用于继续预训练已经预训练过的 LLM，并提供额外的技巧和见解：</strong></p>
<ul>
<li>简单且可扩展的持续预训练大型语言模型策略 (2024)，Ibrahim 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.08763">https://arxiv.org/abs/2403.08763</a></li>
</ul>
<br />
<p><strong>BloombergGPT 是一个领域特定的大型语言模型 (LLM) 的示例，它通过在通用和领域特定的文本语料库（特别是金融领域）上进行训练而创建：</strong></p>
<ul>
<li>BloombergGPT：金融领域的大型语言模型 (2023)，吴等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.17564">https://arxiv.org/abs/2303.17564</a></li>
</ul>
<br />
<p><strong>GaLore 是一个旨在提高 LLM 预训练效率的最新研究项目。所需的代码更改非常简单，只需将训练函数中 PyTorch 的 AdamW 优化器替换为 galore-torch Python 包提供的 GaLoreAdamW 优化器即可。</strong></p>
<ul>
<li>GaLore：通过梯度低秩投影实现内存高效的 LLM 训练 (2024)，Zhao 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.03507">https://arxiv.org/abs/2403.03507</a></li>
<li>GaLore 代码仓库，<a target="_blank" rel="noopener" href="https://github.com/jiaweizzhao/GaLore">https://github.com/jiaweizzhao/GaLore</a></li>
</ul>
<br />
<p><strong>以下论文和资源分享了公开可用的大规模 LLM 预训练数据集，这些数据集包含数百 GB 到数 TB 的文本数据：</strong></p>
<ul>
<li>Dolma：一个用于 LLM 预训练研究的 3 万亿 token 的开放语料库，Soldaini 等人，2024 年，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.00159">https://arxiv.org/abs/2402.00159</a></li>
<li>The Pile：一个用于语言建模的 800GB 多样化文本数据集，Gao 等人，2020 年，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.00027">https://arxiv.org/abs/2101.00027</a></li>
<li>The RefinedWeb Dataset for Falcon LLM：仅使用网络数据超越精心策划的语料库，Penedo 等人 (2023)，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.01116">https://arxiv.org/abs/2306.01116</a></li>
<li>RedPajama，Together AI，<a target="_blank" rel="noopener" href="https://github.com/togethercomputer/RedPajama-Data">https://github.com/togethercomputer/RedPajama-Data</a></li>
<li>The FineWeb dataset，包含超过 15 万亿 token 的来自 CommonCrawl 的清洗和去重后的英语网络数据，<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">https://huggingface.co/datasets/HuggingFaceFW/fineweb</a></li>
</ul>
<br />
<p><strong>最初介绍 top-k 采样的论文：</strong></p>
<ul>
<li>Hierarchical Neural Story Generation，Fan 等人 (2018)，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.04833">https://arxiv.org/abs/1805.04833</a></li>
</ul>
<br />
<p><strong>集束搜索（第五章未涵盖）是一种替代的解码算法，它通过在每个步骤仅保留得分最高的局部序列来生成输出序列，以平衡效率和质量：</strong></p>
<ul>
<li>Diverse Beam Search：从神经序列模型解码多样化解，Vijayakumar 等人 (2016)，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.02424">https://arxiv.org/abs/1610.02424</a></li>
</ul>
<br />
<h2 id="第六章">第六章</h2>
<p><strong>讨论不同类型微调的额外资源：</strong></p>
<ul>
<li>使用和微调预训练的 Transformer，<a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/using-and-finetuning-pretrained-transformers">https://magazine.sebastianraschka.com/p/using-and-finetuning-pretrained-transformers</a></li>
<li>微调大型语言模型，<a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/finetuning-large-language-models">https://magazine.sebastianraschka.com/p/finetuning-large-language-models</a></li>
</ul>
<br />
<p><strong>其他实验，包括对微调第一个输出 token 与最后一个输出 token 的比较，可以在 GitHub 上的补充代码材料中找到：</strong></p>
<ul>
<li>额外的垃圾邮件分类实验，<a target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch06/02_bonus_additional-experiments">https://github.com/rasbt/LLMs-from-scratch/tree/main/ch06/02_bonus_additional-experiments</a></li>
</ul>
<br />
<p><strong>对于二元分类任务（例如垃圾邮件分类），从技术上讲，只使用一个输出节点而不是两个输出节点是可行的，正如我在以下文章中讨论的那样：</strong></p>
<ul>
<li>损失函数学习——优化 PyTorch 中的负对数似然和交叉熵，<a target="_blank" rel="noopener" href="https://sebastianraschka.com/blog/2022/losses-learned-part1.html">https://sebastianraschka.com/blog/2022/losses-learned-part1.html</a></li>
</ul>
<br />
<p><strong>你可以在以下文章中找到关于微调 LLM 不同层的额外实验，该文章表明，除了输出层之外，微调最后一个 Transformer 模块可以显著提高预测性能：</strong></p>
<ul>
<li>微调大型语言模型，<a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/finetuning-large-language-models">https://magazine.sebastianraschka.com/p/finetuning-large-language-models</a></li>
</ul>
<br />
<p><strong>读者可以在 imbalanced-learn 的文档中找到处理不平衡分类数据集的额外资源和信息：</strong></p>
<ul>
<li>Imbalanced-learn 用户指南，<a target="_blank" rel="noopener" href="https://imbalanced-learn.org/stable/user_guide.html">https://imbalanced-learn.org/stable/user_guide.html</a></li>
</ul>
<br />
<p><strong>对于有兴趣对垃圾邮件电子邮件而不是垃圾短信进行分类的读者，以下资源提供了一个大型电子邮件垃圾邮件分类数据集，其格式与第 6 章中使用的数据集格式类似的便捷 CSV 格式：</strong></p>
<ul>
<li>电子邮件垃圾邮件分类数据集，<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/TrainingDataPro/email-spam-classification">https://huggingface.co/datasets/TrainingDataPro/email-spam-classification</a></li>
</ul>
<br />
<p><strong>GPT-2 是一种基于 Transformer 架构解码器模块的模型，其主要目的是生成新的文本。作为替代方案，诸如 BERT 和 RoBERTa 之类的基于编码器的模型对于分类任务可能更有效：</strong></p>
<ul>
<li>BERT：用于语言理解的深度双向 Transformer 的预训练 (2018)，Devlin 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></li>
<li>RoBERTa：一种鲁棒优化的 BERT 预训练方法 (2019)，Liu 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.11692">https://arxiv.org/abs/1907.11692</a></li>
<li>对 5 万条 IMDB 电影评论进行情感分类的额外实验，<a target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/03_bonus_imdb-classification">https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/03_bonus_imdb-classification</a></li>
</ul>
<br />
<p><strong>最近的论文表明，通过在分类微调过程中移除因果掩码并进行其他修改，可以进一步提高分类性能：</strong></p>
<ul>
<li>Label Supervised LLaMA Finetuning (2023)，Li 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.01208">https://arxiv.org/abs/2310.01208</a></li>
<li>LLM2Vec：大型语言模型是隐藏的强大文本编码器 (2024)，BehnamGhader 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.05961">https://arxiv.org/abs/2404.05961</a></li>
</ul>
<br />
<h2 id="第七章">第七章</h2>
<p><strong>用于指令微调的 Alpaca 数据集包含 5.2 万个指令-响应对，是首批最受欢迎的公开指令微调数据集之一：</strong></p>
<ul>
<li>Stanford Alpaca：一个遵循指令的 Llama 模型，<a target="_blank" rel="noopener" href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></li>
</ul>
<br />
<p><strong>以下列出的是适合指令微调的额外公开数据集：</strong></p>
<ul>
<li>LIMA，<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/GAIR/lima%EF%BC%9B%E5%8C%85%E5%90%AB%E4%B8%80%E5%8D%83%E4%B8%AA%E9%AB%98%E8%B4%A8%E9%87%8F%E7%9A%84%E6%8C%87%E4%BB%A4-%E5%93%8D%E5%BA%94%E5%AF%B9%EF%BC%9B%E6%9B%B4%E5%A4%9A%E4%BF%A1%E6%81%AF%E8%AF%B7%E5%8F%82%E9%98%85%E8%AE%BA%E6%96%87%E3%80%8ALIMA:">https://huggingface.co/datasets/GAIR/lima；包含一千个高质量的指令-响应对；更多信息请参阅论文《LIMA:</a> Less Is More for Alignment》(2023)，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.11206">https://arxiv.org/abs/2305.11206</a></li>
<li>UltraChat，<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/openchat/ultrachat-sharegpt%EF%BC%9B%E4%B8%80%E4%B8%AA%E5%8C%85%E5%90%AB">https://huggingface.co/datasets/openchat/ultrachat-sharegpt；一个包含</a> 80.5 万个指令-响应对的大规模数据集；更多信息请参阅论文《Enhancing Chat Language Models by Scaling High-quality Instructional Conversations》(2023)，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14233">https://arxiv.org/abs/2305.14233</a></li>
<li>Alpaca GPT4，<a target="_blank" rel="noopener" href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data.json%EF%BC%8C%E4%B8%80%E4%B8%AA%E7%B1%BB%E4%BC%BC">https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data.json，一个类似</a> Alpaca 的数据集，包含 5.2 万个使用 GPT-4 而非 GPT-3.5 生成的指令-响应对</li>
</ul>
<br />
<p><strong>Phi-3 是一个拥有 38 亿参数的模型，其指令微调变体据称可与更大的专有模型（如 GPT-3.5）相媲美：</strong></p>
<ul>
<li>Phi-3 技术报告：一款可在您的手机本地运行的高性能语言模型 (2024)，Abdin 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.14219">https://arxiv.org/abs/2404.14219</a></li>
</ul>
<br />
<p><strong>研究人员提出了一种合成指令数据生成方法，该方法从一个指令微调的 Llama-3 模型生成 30 万个高质量的指令-响应对。在一个预训练的 Llama 3 基础模型上，使用这些指令示例进行微调后，其性能与原始的指令微调 Llama-3 模型相当：</strong></p>
<ul>
<li>Magpie：通过提示对齐的 LLM 从零开始合成对齐数据 (2024)，Xu 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.08464">https://arxiv.org/abs/2406.08464</a></li>
</ul>
<br />
<p><strong>研究表明，在指令微调中不屏蔽指令和输入可以有效地提高在各种 NLP 任务和开放式生成基准上的性能，尤其是在使用包含长指令和简短输出的数据集或使用少量训练示例进行训练时：</strong></p>
<ul>
<li>Instruction Tuning With Loss Over Instructions (2024)，Shi 著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.14394">https://arxiv.org/abs/2405.14394</a></li>
</ul>
<br />
<p><strong>Prometheus 和 PHUDGE 是公开可用的大型语言模型，它们在评估具有可自定义标准的长篇回复方面与 GPT-4 相媲美。我们在第 7 章中没有使用这些模型，因为 Ollama 尚不支持它们，因此无法在笔记本电脑上高效执行。</strong></p>
<ul>
<li>Prometheus：在语言模型中引入细粒度的评估能力 (2023)，Kim 等人著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.08491">https://arxiv.org/abs/2310.08491</a></li>
<li>PHUDGE：将 Phi-3 作为可扩展的评判者 (2024)，Deshwal 和 Chawla 著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.08029">https://arxiv.org/abs/2405.08029</a></li>
<li>Prometheus 2：一个专门评估其他语言模型的开源语言模型 (2024)，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01535">https://arxiv.org/abs/2405.01535</a></li>
</ul>
<br />
<p><strong>以下报告中的结果支持这样一种观点：大型语言模型主要在预训练期间获取事实知识，而微调主要提高它们使用这些知识的效率。此外，这项研究探讨了使用新的事实信息对大型语言模型进行微调如何影响它们使用现有知识的能力，揭示了模型学习新事实的速度较慢，并且在微调期间引入新事实会增加模型生成不正确信息的倾向：</strong></p>
<ul>
<li>在新的知识上微调 LLM 是否会鼓励幻觉？(2024)，Gekhman 著，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.05904">https://arxiv.org/abs/2405.05904</a></li>
</ul>
<br />
<p><strong>偏好微调是指令微调之后的一个可选步骤，旨在使 LLM 更紧密地与人类偏好对齐。作者的以下文章提供了有关此过程的更多信息：</strong></p>
<ul>
<li>LLM 训练：RLHF 及其替代方案，<a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives">https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives</a></li>
<li>LLM 预训练和奖励模型评估技巧，<a target="_blank" rel="noopener" href="https://sebastianraschka.com/blog/2024/research-papers-in-march2024.html">https://sebastianraschka.com/blog/2024/research-papers-in-march2024.html</a></li>
</ul>
</div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/my-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">欣冻</div><div class="author-info-description">博客, 技术, 生活</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/4260ab42.html" title="Transformer大语言模型架构原理学习笔记"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/11/17/CfYA2b.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer大语言模型架构原理学习笔记"/></a><div class="content"><a class="title" href="/posts/4260ab42.html" title="Transformer大语言模型架构原理学习笔记">Transformer大语言模型架构原理学习笔记</a><time datetime="2025-11-17T12:51:00.000Z" title="发表于 2025-11-17 20:51:00">2025-11-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/6f4fa4e7.html" title="快速幂、逆元与组合数学"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/10/31/7I8gnp.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="快速幂、逆元与组合数学"/></a><div class="content"><a class="title" href="/posts/6f4fa4e7.html" title="快速幂、逆元与组合数学">快速幂、逆元与组合数学</a><time datetime="2025-10-31T15:48:33.000Z" title="发表于 2025-10-31 23:48:33">2025-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/2f58633e.html" title="常用数据结构"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://origin.picgo.net/2025/10/11/792a8743-6d0d-43fe-91b8-0a5a77b529f4a296a597708421a1.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="常用数据结构"/></a><div class="content"><a class="title" href="/posts/2f58633e.html" title="常用数据结构">常用数据结构</a><time datetime="2025-10-11T11:00:00.000Z" title="发表于 2025-10-11 19:00:00">2025-10-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/7258f8a4.html" title="THYTHM 音游"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/08/01/HotT9.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="THYTHM 音游"/></a><div class="content"><a class="title" href="/posts/7258f8a4.html" title="THYTHM 音游">THYTHM 音游</a><time datetime="2025-08-01T04:00:00.000Z" title="发表于 2025-08-01 12:00:00">2025-08-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/73e7a68a.html" title="力扣每日一题讲解"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/07/24/QEaxN.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="力扣每日一题讲解"/></a><div class="content"><a class="title" href="/posts/73e7a68a.html" title="力扣每日一题讲解">力扣每日一题讲解</a><time datetime="2025-07-24T08:50:00.000Z" title="发表于 2025-07-24 16:50:00">2025-07-24</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/c%E8%AF%AD%E8%A8%80/"><span class="card-category-list-name">c语言</span><span class="card-category-list-count">1</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 1.1em; color: #999">算法</a> <a href="/tags/hexo-github-blog-node-js-npm-git-%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2-hexo%E9%83%A8%E7%BD%B2/" style="font-size: 1.1em; color: #999">hexo, github, blog, node.js,npm,git,部署博客,hexo部署</a> <a href="/tags/%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%EF%BC%8Cbutterfly/" style="font-size: 1.1em; color: #999">一键部署，butterfly</a> <a href="/tags/c%E8%AF%AD%E8%A8%80-%E5%AD%A6%E4%B9%A0/" style="font-size: 1.1em; color: #999">c语言,学习</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" style="font-size: 1.1em; color: #999">每日一题</a> <a href="/tags/%E5%A4%A7%E5%AE%B6%E5%A5%BD%EF%BC%8C%E6%88%91%E6%98%AF%E8%BF%B7%E8%B7%AF%E7%9A%84%E5%B0%8F%E6%9C%8B%E5%8F%8B/" style="font-size: 1.1em; color: #999">大家好，我是迷路的小朋友</a> <a href="/tags/python-%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91-%E9%9F%B3%E6%B8%B8/" style="font-size: 1.1em; color: #999">python,游戏开发,音游</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/11/">
            <span class="card-archive-list-date">
              十一月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/10/">
            <span class="card-archive-list-date">
              十月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/08/">
            <span class="card-archive-list-date">
              八月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/07/">
            <span class="card-archive-list-date">
              七月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/05/">
            <span class="card-archive-list-date">
              五月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/04/">
            <span class="card-archive-list-date">
              四月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/03/">
            <span class="card-archive-list-date">
              三月 2025
            </span>
            <span class="card-archive-list-count">7</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/02/">
            <span class="card-archive-list-date">
              二月 2025
            </span>
            <span class="card-archive-list-count">7</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">23</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-11-18T15:13:37.124Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://i.imgs.ovh/2025/07/03/qLFy9.png);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2019 - 2025 By 欣冻</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://blog-twikoo.xindon.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = (el = document, path = location.pathname) => {
    twikoo.init({
      el: el.querySelector('#twikoo-wrap'),
      envId: 'https://blog-twikoo.xindon.top/',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      },
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    

    isShuoshuo && (window.shuoshuoComment.destroyTwikoo = () => {
      if (el.children.length) {
        el.innerHTML = ''
        el.classList.add('no-comment')
      }
    })
  }

  const loadTwikoo = (el, path) => {
    if (typeof twikoo === 'object') setTimeout(() => init(el, path), 0)
    else btf.getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(() => init(el, path))
  }

  if (isShuoshuo) {
    'Twikoo' === 'Twikoo'
      ? window.shuoshuoComment = { loadComment: loadTwikoo }
      : window.loadOtherComment = loadTwikoo
    return
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><div class="aplayer no-destroy" data-id="13348674056" data-server="netease" data-type="playlist"   data-order="list" data-fixed="true" data-preload="auto" data-autoplay="false" data-mutex="true" ></div><script src="https://cdn.jsdelivr.net/npm/mermaid@10.2.4/dist/mermaid.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v 7.3.0" title=""><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v5.2.2" title=""><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://www.jsdelivr.com/" style="margin-inline:5px" data-title="本站使用JsDelivr为静态资源提供CDN加速" title=""><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('article-sort-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__slideInRight');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script><script async src="/js/ali_font.js"></script><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var qweather_key = '0cca502ccc7341c2be6ba09309916622';
  var gaud_map_key = '5653914d2fc43aad14b253ab6cf762b9';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '112.982279,28.19409';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/miku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>