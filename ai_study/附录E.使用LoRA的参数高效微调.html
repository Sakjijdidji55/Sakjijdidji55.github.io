<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>附录E.使用LoRA的参数高效微调 | 迷路的小朋友</title><meta name="author" content="欣冻"><meta name="copyright" content="欣冻"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="附录E. 使用LoRA的参数高效微调本附录介绍低秩适应 (LoRA)，这是最广泛使用的参数高效微调技术之一。在解释 LoRA 背后的主要思想之后，本附录将基于第 6 章中的垃圾邮件分类微调示例并对 LLM 进行微调。然而，需要注意的是，LoRA 微调也适用于第 7 章中讨论的有监督的指令微调。   附录E. 使用LoRA的参数高效微调 E.1 LoRA 简介 E.2 准备数据集 E.3 初始化模型">
<meta property="og:type" content="website">
<meta property="og:title" content="附录E.使用LoRA的参数高效微调">
<meta property="og:url" content="https://sakjijdidji55.github.io/ai_study/%E9%99%84%E5%BD%95E.%E4%BD%BF%E7%94%A8LoRA%E7%9A%84%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83.html">
<meta property="og:site_name" content="迷路的小朋友">
<meta property="og:description" content="附录E. 使用LoRA的参数高效微调本附录介绍低秩适应 (LoRA)，这是最广泛使用的参数高效微调技术之一。在解释 LoRA 背后的主要思想之后，本附录将基于第 6 章中的垃圾邮件分类微调示例并对 LLM 进行微调。然而，需要注意的是，LoRA 微调也适用于第 7 章中讨论的有监督的指令微调。   附录E. 使用LoRA的参数高效微调 E.1 LoRA 简介 E.2 准备数据集 E.3 初始化模型">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sakjijdidji55.github.io/img/my-icon.png">
<meta property="article:published_time" content="2025-10-26T08:00:00.000Z">
<meta property="article:modified_time" content="2025-10-26T08:24:41.378Z">
<meta property="article:author" content="欣冻">
<meta property="article:tag" content="博客, 技术, 生活, tanxin, tanxin.me, 吃好喝好, 玩好, 睡好, 迷路的小朋友,tanxin55">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sakjijdidji55.github.io/img/my-icon.png"><link rel="shortcut icon" href="/img/logo.ico"><link rel="canonical" href="https://sakjijdidji55.github.io/ai_study/%E9%99%84%E5%BD%95E.%E4%BD%BF%E7%94%A8LoRA%E7%9A%84%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '附录E.使用LoRA的参数高效微调',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'page'
}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"<!-- hexo injector head_end start --><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="迷路的小朋友" type="application/atom+xml">
</head><body><div id="web_bg" style="background-image: url(https://i.imgs.ovh/2025/07/03/qLFy9.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/my-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-home"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-home-page" id="page-header" style="background-image: url(https://img.picgo.net/2025/04/05/2025-2-22fe10c0c4fb1bc202.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.png" alt="Logo"><span class="site-name">迷路的小朋友</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-home"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="page-site-info"><h1 id="site-title">附录E.使用LoRA的参数高效微调</h1></div></header><main class="layout" id="content-inner"><div id="page"><div class="container" id="article-container"><h1 id="附录E-使用LoRA的参数高效微调"><a href="#附录E-使用LoRA的参数高效微调" class="headerlink" title="附录E. 使用LoRA的参数高效微调"></a>附录E. 使用LoRA的参数高效微调</h1><p>本附录介绍低秩适应 (LoRA)，这是最广泛使用的参数高效微调技术之一。在解释 LoRA 背后的主要思想之后，本附录将基于第 6 章中的垃圾邮件分类微调示例并对 LLM 进行微调。然而，需要注意的是，LoRA 微调也适用于第 7 章中讨论的有监督的指令微调。</p>
<hr>
<ul>
<li><a href="#附录e-使用lora的参数高效微调">附录E. 使用LoRA的参数高效微调</a><ul>
<li><a href="#e1-lora-简介">E.1 LoRA 简介</a></li>
<li><a href="#e2-准备数据集">E.2 准备数据集</a></li>
<li><a href="#e3-初始化模型">E.3 初始化模型</a></li>
<li><a href="#e4-使用-lora-高效微调">E.4 使用 LoRA 高效微调</a></li>
</ul>
</li>
</ul>
<hr>
<h2 id="E-1-LoRA-简介"><a href="#E-1-LoRA-简介" class="headerlink" title="E.1 LoRA 简介"></a>E.1 LoRA 简介</h2><p>LoRA，即低秩适应，是一种仅调整模型权重参数的一小部分，就可以让预训练模型更好地适应特定（通常较小）数据集的技术。“低秩”指的是将模型调整限制在总权重参数空间的一个较小维度子空间的数学概念，这有效地捕获了训练期间权重参数变化的最具影响力的方向。</p>
<blockquote>
<p>[!TIP]</p>
<p><strong>个人思考：</strong> LoRA技术现在用的比较多，我们该怎么理解LoRA，想象你有一个非常厉害的工具箱（预训练模型），里面有很多工具，可以做各种各样的事情。现在你只想用这个工具箱来修自行车（特定任务）。</p>
<p>LoRA 就像是给你的工具箱增加了一些专门用来修自行车的“小工具”（模型的一小部分权重参数）。你不需要把整个工具箱里的工具都换掉或者重新学习怎么使用它们，只需要学会用这些新增的“小工具”就行了。</p>
<p>“低秩”的意思是，这些新增的“小工具”并不是全新的、非常复杂的工具。它们是在已有的工具基础上进行了一些简单的调整或者组合，就能很好地完成修自行车的任务。这就好比你不需要重新发明轮子，只需要给现有的扳手加个特殊的套筒就能拧紧自行车上的螺丝。</p>
<p>所以，LoRA 的好处就是，它能让预训练模型快速适应新的任务，而且只需要学习和调整很少的“小工具”，这样就更高效、更省资源。</p>
</blockquote>
<p>LoRA 方法之所以有用且受欢迎，是因为它能够高效地在特定任务的数据上对大型模型进行微调，从而显著降低了传统微调方法所需的计算成本和资源。</p>
<p>为了解释 LoRA 的工作原理，假设存在一个与特定层相关联的大型权重矩阵 W。LoRA 可以应用于 LLM 中的所有线性层（稍后将会看到），为了说明，我们先关注单个层。</p>
<p>在训练深度神经网络时，在反向传播过程中，我们会学习到一个 ΔW 矩阵，它包含了关于我们如何更新原始权重参数的信息，以便在训练过程中最小化损失函数。在本附录的其余部分，我们将使用术语“权重”作为模型权重参数的简称。</p>
<p>在传统训练和微调中，权重更新矩阵定义如下：</p>
<p>W<sub>updated</sub> = W + ΔW</p>
<p>Hu等人提出的 LoRA 方法 (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a>) 提供了一种更有效的计算权重更新ΔW的替代方法，它学习的是ΔW的近似值：<br>ΔW ≈ AB</p>
<p>其中 A 和 B 是两个远小于 W 的矩阵， AB 表示 A 和 B 之间的矩阵乘法积。使用 LoRA，我们现在可以按如下方式重新定义权重更新矩阵：<br>W<sub>updated</sub> = W + AB</p>
<p>图 E.1 并排展示了完整微调和 LoRA 的权重更新公式。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/AppendixE/E.1.png" alt=""></p>
<p>如果你仔细观察，你可能会注意到图 E.1 中完整微调和 LoRA 的视觉表示与之前呈现的公式略有不同。这种差异归因于矩阵乘法的分配律，该定律允许我们分离原始权重和更新后的权重，而不是将它们组合在一起。例如，在进行常规微调的情况下，以 x 作为输入数据，我们可以将计算按如下表示:</p>
<p>x ( W + ΔW) = xW + xΔW</p>
<p>同样，我们也可以将 LoRA 按如下表示：</p>
<p>x ( W + AB) = xW + xAB</p>
<p>除了能减少训练期间需要更新的权重数量之外，将 LoRA 权重矩阵与原始模型权重分离的能力使得 LoRA 的实用性更强。这意味着预训练模型的权重可以保持不变，而 LoRA 权重矩阵在训练后使用模型时则可以被动态地应用。</p>
<blockquote>
<p>[!TIP]</p>
<p><strong>个人思考：</strong> 这段描述的关键在于强调<strong>LoRA 的权重是独立于原始模型权重的</strong>，这带来了很多实际的好处。你可以这样理解：</p>
<p>想象你已经拥有一个非常庞大的、功能强大的模型，它就像一个已经掌握了很多知识和技能的“超级大脑”。现在你想让这个“超级大脑”专注于解决某个特定的问题，比如识别图片中的猫。</p>
<p><strong>传统的微调</strong>就像是直接调整这个“超级大脑”内部的很多连接和参数，让它更擅长识别猫。这个过程可能会比较复杂，需要大量的计算资源，而且可能会影响它之前学到的其他知识。</p>
<p><strong>LoRA 的做法则更聪明：</strong></p>
<p>它不是直接修改“超级大脑”原有的结构，而是在它的基础上<strong>增加了一些非常小的、专门用于识别猫的“插件”或者“补丁”</strong>。这些“插件”就是 LoRA 的权重矩阵（A 和 B）。</p>
<p>关键在于，这些“插件”是<strong>独立</strong>于“超级大脑”本身的核心知识（原始模型权重）的。这意味着：</p>
<ol>
<li><strong>原始的“超级大脑”保持不变：</strong> 它仍然拥有之前学到的所有通用知识。你不需要担心为了让它识别猫而忘记了其他技能。</li>
<li><strong>“插件”很小，训练起来更快更省资源：</strong> 因为 LoRA 只训练这些新增的“插件”，它们的参数量比原始模型小得多，所以训练起来更快，需要的计算资源也更少。</li>
<li><strong>可以灵活地切换任务：</strong> 想象一下，你不仅想让这个“超级大脑”识别猫，还想让它识别狗。使用 LoRA，你可以在同一个原始模型的基础上，训练出另一个专门识别狗的“插件”。当你需要识别猫时，就加载猫的“插件”；需要识别狗时，就加载狗的“插件”。原始的“超级大脑”本身不需要改变。</li>
<li><strong>部署和存储更方便：</strong> 因为原始模型很大，而 LoRA 的“插件”很小，所以你只需要存储原始模型一次，然后为不同的任务存储不同的“插件”就可以了，这样可以节省大量的存储空间。</li>
</ol>
</blockquote>
<p>在实践中，将 LoRA 权重分开非常有用，因为它可以在不需要存储 LLM 的多个完整版本的情况下实现模型定制。这显著降低了存储需求并提高了可伸缩性，因为当为每个特定的客户或应用程序定制 LLM 时，只需要调整和保存较小的 LoRA 矩阵。</p>
<p>目前我们已经讨论了 LoRA 的全部内容，在接下来的章节中，让我们看看如何使用它来微调 LLM 以进行垃圾邮件分类，类似于第 6 章中的微调示例。</p>
<h2 id="E-2-准备数据集"><a href="#E-2-准备数据集" class="headerlink" title="E.2 准备数据集"></a>E.2 准备数据集</h2><p>在将 LoRA 应用于第 6 章中的垃圾邮件分类示例之前，我们必须加载将要使用的数据集和预训练模型。</p>
<p>本节中的代码复用了第 6 章中的数据准备工作。（请注意，除了在本节中复用代码之外，我们还可以打开并运行第 6 章的 notebook，然后将 E.4 节中的 LoRA 代码插入到那里。）</p>
<p>首先，我们下载数据集并将其保存为 CSV 文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing E.1 Downloading and preparing the dataset</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> ch06 <span class="keyword">import</span> (</span><br><span class="line">    download_and_unzip_spam_data,</span><br><span class="line">    create_balanced_dataset,</span><br><span class="line">    random_split</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip&quot;</span></span><br><span class="line">zip_path = <span class="string">&quot;sms_spam_collection.zip&quot;</span></span><br><span class="line">extracted_path = <span class="string">&quot;sms_spam_collection&quot;</span></span><br><span class="line">data_file_path = Path(extracted_path) / <span class="string">&quot;SMSSpamCollection.tsv&quot;</span></span><br><span class="line"></span><br><span class="line">download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(data_file_path, sep=<span class="string">&quot;\t&quot;</span>, header=<span class="literal">None</span>, names=[<span class="string">&quot;Label&quot;</span>, <span class="string">&quot;Text&quot;</span>])</span><br><span class="line">balanced_df = create_balanced_dataset(df)</span><br><span class="line">balanced_df[<span class="string">&quot;Label&quot;</span>] = balanced_df[<span class="string">&quot;Label&quot;</span>].<span class="built_in">map</span>(&#123;<span class="string">&quot;ham&quot;</span>: <span class="number">0</span>, <span class="string">&quot;spam&quot;</span>: <span class="number">1</span>&#125;)</span><br><span class="line"></span><br><span class="line">train_df, validation_df, test_df = random_split(balanced_df, <span class="number">0.7</span>, <span class="number">0.1</span>)</span><br><span class="line">train_df.to_csv(<span class="string">&quot;train.csv&quot;</span>, index=<span class="literal">None</span>)</span><br><span class="line">validation_df.to_csv(<span class="string">&quot;validation.csv&quot;</span>, index=<span class="literal">None</span>)</span><br><span class="line">test_df.to_csv(<span class="string">&quot;test.csv&quot;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>接着，我们来创建 <code>SpamDataset</code> 实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing E.2 Instantiating PyTorch datasets</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"><span class="keyword">from</span> previous_chapters <span class="keyword">import</span> SpamDataset</span><br><span class="line"></span><br><span class="line">tokenizer = tiktoken.get_encoding(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">train_dataset = SpamDataset(<span class="string">&quot;train.csv&quot;</span>, max_length=<span class="literal">None</span>, tokenizer=tokenizer)</span><br><span class="line">val_dataset = SpamDataset(<span class="string">&quot;validation.csv&quot;</span>, max_length=train_dataset.max_length,</span><br><span class="line">tokenizer=tokenizer)</span><br><span class="line">test_dataset = SpamDataset(<span class="string">&quot;test.csv&quot;</span>, max_length=train_dataset.max_length,</span><br><span class="line">tokenizer=tokenizer)</span><br></pre></td></tr></table></figure>
<p>在创建 PyTorch 数据集对象之后，我们开始实例化数据加载器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing E.3 Creating PyTorch data loaders</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">num_workers = <span class="number">0</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(</span><br><span class="line">    dataset=train_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    num_workers=num_workers,</span><br><span class="line">    drop_last=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">val_loader = DataLoader(</span><br><span class="line">    dataset=val_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    num_workers=num_workers,</span><br><span class="line">    drop_last=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_loader = DataLoader(</span><br><span class="line">    dataset=test_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    num_workers=num_workers,</span><br><span class="line">    drop_last=<span class="literal">False</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>作为验证步骤，我们遍历数据加载器并检查每个批次是否包含 8 个训练示例，其中每个训练示例包含 120 个 token：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Train loader:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> input_batch, target_batch <span class="keyword">in</span> train_loader:</span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Input batch dimensions:&quot;</span>, input_batch.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Label batch dimensions&quot;</span>, target_batch.shape)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Train loader:</span><br><span class="line">Input batch dimensions: torch.Size([<span class="number">8</span>, <span class="number">120</span>])</span><br><span class="line">Label batch dimensions torch.Size([<span class="number">8</span>])</span><br></pre></td></tr></table></figure>
<p>最后，我们打印每个数据集中的总批次数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;<span class="built_in">len</span>(train_loader)&#125;</span> training batches&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;<span class="built_in">len</span>(val_loader)&#125;</span> validation batches&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;<span class="built_in">len</span>(test_loader)&#125;</span> test batches&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在这种情况下，我们每个数据集拥有的批次数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">130</span> training batches</span><br><span class="line"><span class="number">19</span> validation batches</span><br><span class="line"><span class="number">38</span> test batches</span><br></pre></td></tr></table></figure>
<h2 id="E-3-初始化模型"><a href="#E-3-初始化模型" class="headerlink" title="E.3 初始化模型"></a>E.3 初始化模型</h2><p>本节将复用第 6 章中的代码来加载和准备预训练的 GPT 模型。我们首先下载模型权重，然后将它们加载到 <code>GPTModel</code> 类中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing E.4 Loading a pretrained GPT model</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gpt_download <span class="keyword">import</span> download_and_load_gpt2</span><br><span class="line"><span class="keyword">from</span> previous_chapters <span class="keyword">import</span> GPTModel, load_weights_into_gpt</span><br><span class="line"></span><br><span class="line">CHOOSE_MODEL = <span class="string">&quot;gpt2-small (124M)&quot;</span></span><br><span class="line">INPUT_PROMPT = <span class="string">&quot;Every effort moves&quot;</span></span><br><span class="line"></span><br><span class="line">BASE_CONFIG = &#123;</span><br><span class="line">    <span class="string">&quot;vocab_size&quot;</span>: <span class="number">50257</span>, <span class="comment"># Vocabulary size</span></span><br><span class="line">    <span class="string">&quot;context_length&quot;</span>: <span class="number">1024</span>, <span class="comment"># Context length</span></span><br><span class="line">    <span class="string">&quot;drop_rate&quot;</span>: <span class="number">0.0</span>, <span class="comment"># Dropout rate</span></span><br><span class="line">    <span class="string">&quot;qkv_bias&quot;</span>: <span class="literal">True</span> <span class="comment"># Query-key-value bias</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">model_configs = &#123;</span><br><span class="line">    <span class="string">&quot;gpt2-small (124M)&quot;</span>: &#123;<span class="string">&quot;emb_dim&quot;</span>: <span class="number">768</span>, <span class="string">&quot;n_layers&quot;</span>: <span class="number">12</span>, <span class="string">&quot;n_heads&quot;</span>: <span class="number">12</span>&#125;,</span><br><span class="line">    <span class="string">&quot;gpt2-medium (355M)&quot;</span>: &#123;<span class="string">&quot;emb_dim&quot;</span>: <span class="number">1024</span>, <span class="string">&quot;n_layers&quot;</span>: <span class="number">24</span>, <span class="string">&quot;n_heads&quot;</span>: <span class="number">16</span>&#125;,</span><br><span class="line">    <span class="string">&quot;gpt2-large (774M)&quot;</span>: &#123;<span class="string">&quot;emb_dim&quot;</span>: <span class="number">1280</span>, <span class="string">&quot;n_layers&quot;</span>: <span class="number">36</span>, <span class="string">&quot;n_heads&quot;</span>: <span class="number">20</span>&#125;,</span><br><span class="line">    <span class="string">&quot;gpt2-xl (1558M)&quot;</span>: &#123;<span class="string">&quot;emb_dim&quot;</span>: <span class="number">1600</span>, <span class="string">&quot;n_layers&quot;</span>: <span class="number">48</span>, <span class="string">&quot;n_heads&quot;</span>: <span class="number">25</span>&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">BASE_CONFIG.update(model_configs[CHOOSE_MODEL])</span><br><span class="line"></span><br><span class="line">model_size = CHOOSE_MODEL.split(<span class="string">&quot; &quot;</span>)[-<span class="number">1</span>].lstrip(<span class="string">&quot;(&quot;</span>).rstrip(<span class="string">&quot;)&quot;</span>)</span><br><span class="line">settings, params = download_and_load_gpt2(model_size=model_size, models_dir=<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line"></span><br><span class="line">model = GPTModel(BASE_CONFIG)</span><br><span class="line">load_weights_into_gpt(model, params)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<p>为了确保模型已正确加载，让我们再次检查它是否能生成连贯的文本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> previous_chapters <span class="keyword">import</span> (</span><br><span class="line">    generate_text_simple,</span><br><span class="line">    text_to_token_ids,</span><br><span class="line">    token_ids_to_text</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">text_1 = <span class="string">&quot;Every effort moves you&quot;</span></span><br><span class="line"></span><br><span class="line">token_ids = generate_text_simple(</span><br><span class="line">    model=model,</span><br><span class="line">    idx=text_to_token_ids(text_1, tokenizer),</span><br><span class="line">    max_new_tokens=<span class="number">15</span>,</span><br><span class="line">    context_size=BASE_CONFIG[<span class="string">&quot;context_length&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(token_ids_to_text(token_ids, tokenizer))</span><br></pre></td></tr></table></figure>
<p>如下所示，该模型生成了连贯的文本，这表明模型权重已正确加载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Every effort moves you forward.</span><br><span class="line">The first step <span class="keyword">is</span> to understand the importance of your work</span><br></pre></td></tr></table></figure>
<p>接着，我们准备模型以进行分类微调，类似于第 6 章那样替换掉输出层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">num_classes = <span class="number">2</span></span><br><span class="line">model.out_head = torch.nn.Linear(in_features=<span class="number">768</span>, out_features=num_classes)</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<p>最后，让我们计算未微调模型的初始分类准确率（我们预计大概为 50%，这意味着该模型尚无法可靠地区分垃圾邮件和非垃圾邮件）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ch06 <span class="keyword">import</span> calc_accuracy_loader</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=<span class="number">10</span>)</span><br><span class="line">val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=<span class="number">10</span>)</span><br><span class="line">test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training accuracy: <span class="subst">&#123;train_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Validation accuracy: <span class="subst">&#123;val_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test accuracy: <span class="subst">&#123;test_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>初始预测准确率如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Training accuracy: <span class="number">46.25</span>%</span><br><span class="line">Validation accuracy: <span class="number">45.00</span>%</span><br><span class="line">Test accuracy: <span class="number">48.75</span>%</span><br></pre></td></tr></table></figure>
<h2 id="E-4-使用-LoRA-高效微调"><a href="#E-4-使用-LoRA-高效微调" class="headerlink" title="E.4 使用 LoRA 高效微调"></a>E.4 使用 LoRA 高效微调</h2><p>在本节中，我们将使用 LoRA 修改和微调 LLM。我们首先初始化一个 <code>LoRALayer</code>，该层会创建矩阵 A 和 B，以及 alpha 缩放因子和秩 (r) 设置。</p>
<p>该层可以接受一个输入并计算相应的输出，如图 E.2 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/AppendixE/E.2.png" alt=""></p>
<p>我们可以通过以下代码来实现图 E.2 中描述的 LoRA 层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing E.5 Implementing a LoRA layer</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LoRALayer</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, out_dim, rank, alpha</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.A = torch.nn.Parameter(torch.empty(in_dim, rank))</span><br><span class="line">        torch.nn.init.kaiming_uniform_(<span class="variable language_">self</span>.A, a=math.sqrt(<span class="number">5</span>))      <span class="comment">#A</span></span><br><span class="line">        <span class="variable language_">self</span>.B = torch.nn.Parameter(torch.zeros(rank, out_dim))</span><br><span class="line">        <span class="variable language_">self</span>.alpha = alpha</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.alpha * (x @ <span class="variable language_">self</span>.A @ <span class="variable language_">self</span>.B)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line"><span class="comment">#A 使用与 PyTorch 中线性层相同的初始化方式</span></span><br></pre></td></tr></table></figure>
<p>在以上代码中，秩决定了矩阵 A 和 B 的内部维度。本质上，这一设置确定了 LoRA 引入的额外参数的数量，用于在模型的适应性和其效率之间通过使用的参数数量进行平衡。</p>
<p>另一个重要的设置 alpha，用作低秩适应输出的缩放因子。它主要决定了来自适应层的输出对原始层输出的影响程度。这可以看作是一种调节低秩适应对层输出影响的方式。</p>
<blockquote>
<p>[!TIP]</p>
<p><strong>个人思考：</strong> 关于LoRA的重要设置参数，这里讲的不是很清楚，其实这段话解释了 LoRA 方法中两个非常重要的设置：<strong>秩 (rank)</strong> 和 <strong>Alpha</strong>。可以这样理解它们：</p>
<p><strong>秩 (Rank):</strong></p>
<ul>
<li><strong>决定了 LoRA “小工具” 的大小:</strong> 还记得之前我们把 LoRA 比作给预训练模型添加一些专门的“小工具”吗？这里的“秩”就决定了这些“小工具”（更具体地说是矩阵 A 和 B）的内部大小。你可以想象成，秩越大，“小工具”就越复杂，包含的信息就越多。</li>
<li><strong>影响额外学习的参数数量:</strong> 秩越大，LoRA 引入的需要学习的额外参数就越多。反之，秩越小，需要学习的参数就越少。</li>
<li><strong>平衡模型的学习能力和效率:</strong><ul>
<li><strong>秩高一点:</strong> 模型可以学习到更复杂、更细致的针对特定任务的调整，性能可能会更好。但是，需要学习的参数也更多，训练起来可能更慢，更耗费资源。</li>
<li><strong>秩低一点:</strong> 模型学习的参数更少，训练速度更快，更节省资源。但是，如果秩太低，模型可能没有足够的“能力”来学习到足够好的调整，导致性能不够理想。</li>
<li><strong>就像给自行车加辅助轮:</strong> 秩就像辅助轮的大小。大的辅助轮（高秩）更容易保持平衡，但可能不够灵活。小的辅助轮（低秩）更灵活，但可能需要更高的骑行技巧。你需要找到一个合适的平衡点。</li>
</ul>
</li>
</ul>
<p><strong>Alpha:</strong></p>
<ul>
<li><strong>LoRA “小工具” 输出的音量调节器:</strong> Alpha 可以看作是一个调节 LoRA 带来的改变有多大的“音量旋钮”。它是一个数字，用来乘以 LoRA “小工具” 的输出结果。</li>
<li><strong>控制适应层对原始层的影响:</strong> Alpha 的大小决定了 LoRA 学习到的调整对原始模型输出的影响程度。<ul>
<li><strong>Alpha 大一点:</strong> LoRA 带来的改变会更明显，模型会更倾向于学习新的任务。</li>
<li><strong>Alpha 小一点:</strong> LoRA 带来的改变会更微妙，模型更多地还是依赖于它原本学到的知识，只是做一些微小的调整。</li>
<li><strong>就像调味品:</strong> Alpha 就像你做菜时放的盐。盐放多了（Alpha 大了），菜的味道变化就大；盐放少了（Alpha 小了），菜的味道变化就小。你需要根据你的口味来调整。</li>
</ul>
</li>
</ul>
<p><strong>总结一下：</strong></p>
<ul>
<li><strong>秩 (Rank)</strong> 决定了 LoRA 可以学习多少新的信息，以及需要多少额外的参数。</li>
<li><strong>Alpha</strong> 决定了 LoRA 学习到的信息对最终结果的影响有多大。</li>
</ul>
<p>这两个参数都需要根据具体的任务和模型进行调整，以达到最佳的性能和效率。</p>
</blockquote>
<p>我们目前实现的 <code>LoRALayer</code> 类使我们能够转换层的输入。</p>
<p>在 LoRA 中，典型的目标是替换现有的线性层，从而允许将权重更新直接应用于预先存在的预训练权重，如图 E.3 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/AppendixE/E.3.png" alt=""></p>
<p>为了集成图 E.3 所示的原始线性层权重，我们现在创建一个 <code>LinearWithLoRA</code> 层。该层利用了之前实现的 <code>LoRALayer</code>，旨在替换神经网络中现有的线性层，例如 <code>GPTModel</code> 中的自注意力模块或前馈模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing E.6 A LinearWithLora layer to replace Linear layers</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearWithLoRA</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, linear, rank, alpha</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = linear</span><br><span class="line">        <span class="variable language_">self</span>.lora = LoRALayer(</span><br><span class="line">            linear.in_features, linear.out_features, rank, alpha</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">   		 <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x) + <span class="variable language_">self</span>.lora(x)</span><br></pre></td></tr></table></figure>
<p>前面的代码将一个标准的线性层与 <code>LoRALayer</code> 结合在一起。<code>forward</code> 方法通过将原始线性层和 LoRA 层的输出相加来计算最终输出。</p>
<p>由于权重矩阵 B（在 <code>LoRALayer</code> 中是 <code>self.B</code>）被初始化为零值，矩阵 A 和 B 的乘积将得到一个零矩阵。这确保了该乘法不会改变原始权重，因为加零不会改变它们。</p>
<p>为了将 LoRA 应用于之前定义的 <code>GPTModel</code>，我们还引入了一个 <code>replace_linear_with_lora</code> 函数。该函数会将模型中所有现有的线性层替换为新创建的 <code>LinearWithLoRA</code> 层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">replace_linear_with_lora</span>(<span class="params">model, rank, alpha</span>):</span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_children():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, torch.nn.Linear):               <span class="comment">#A</span></span><br><span class="line">            <span class="built_in">setattr</span>(model, name, LinearWithLoRA(module, rank, alpha))</span><br><span class="line">        <span class="keyword">else</span>:                                                 <span class="comment">#B</span></span><br><span class="line">            replace_linear_with_lora(module, rank, alpha)</span><br><span class="line">            </span><br><span class="line">    </span><br><span class="line"><span class="comment">#A 将线性层替换为 LinearWithLoRA</span></span><br><span class="line"><span class="comment">#B 将相同的函数递归地应用于子模块</span></span><br></pre></td></tr></table></figure>
<p>我们现在已经实现了所有必要的代码，以将 <code>GPTModel</code> 中的线性层替换为新开发的 <code>LinearWithLoRA</code> 层，从而实现参数高效微调。在接下来的章节中，我们将把 <code>LinearWithLoRA</code> 升级应用于 <code>GPTModel</code> 的多头注意力模块、前馈模块和输出层中的所有线性层，如图 E.4 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/AppendixE/E.4.png" alt=""></p>
<p>在我们应用如图 E.4 所示的 <code>LinearWithLoRA</code> 层升级之前，我们首先需要冻结原始模型的参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total trainable parameters before: <span class="subst">&#123;total_params:,&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">		param.requires_grad = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">total_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total trainable parameters after: <span class="subst">&#123;total_params:,&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>运行代码，可以看到，该模型的所有 1.24 亿个参数现在都不可训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Total trainable parameters before: <span class="number">124</span>,<span class="number">441</span>,<span class="number">346</span></span><br><span class="line">Total trainable parameters after: <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>接着，我们使用 <code>replace_linear_with_lora</code> 函数来替换线性层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">replace_linear_with_lora(model, rank=<span class="number">16</span>, alpha=<span class="number">16</span>)</span><br><span class="line">total_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total trainable LoRA parameters: <span class="subst">&#123;total_params:,&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>添加 LoRA 层后，可训练参数的数量如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Total trainable LoRA parameters: <span class="number">2</span>,<span class="number">666</span>,<span class="number">528</span></span><br></pre></td></tr></table></figure>
<p>如我们所见，使用 LoRA 后，可训练参数的数量减少了近 50 倍。秩和 alpha 一般都默认设置为 16 ，但通常也会增加秩的大小，这从而增加可训练参数的数量。Alpha 通常选择为秩的一半、两倍或相等。</p>
<p>现在可以通过打印模型架构来验证这些层是否已按预期修改：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">GPTModel(</span><br><span class="line">  (tok_emb): Embedding(<span class="number">50257</span>, <span class="number">768</span>)</span><br><span class="line">  (pos_emb): Embedding(<span class="number">1024</span>, <span class="number">768</span>)</span><br><span class="line">  (drop_emb): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  (trf_blocks): Sequential(</span><br><span class="line">		...</span><br><span class="line">    (<span class="number">11</span>): TransformerBlock(</span><br><span class="line">    	(att): MultiHeadAttention(</span><br><span class="line">   		 (W_query): LinearWithLoRA(</span><br><span class="line">   			 (linear): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    		 (lora): LoRALayer()</span><br><span class="line">    	 )</span><br><span class="line">      (W_key): LinearWithLoRA(</span><br><span class="line">        (linear): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (lora): LoRALayer()</span><br><span class="line">      )</span><br><span class="line">      (W_value): LinearWithLoRA(</span><br><span class="line">        (linear): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (lora): LoRALayer()</span><br><span class="line">      )</span><br><span class="line">      (out_proj): LinearWithLoRA(</span><br><span class="line">        (linear): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (lora): LoRALayer()</span><br><span class="line">      )</span><br><span class="line">			(dropout): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">		)</span><br><span class="line">    (ff): FeedForward(</span><br><span class="line">      (layers): Sequential(</span><br><span class="line">        (<span class="number">0</span>): LinearWithLoRA(</span><br><span class="line">          (linear): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (lora): LoRALayer()</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">1</span>): GELU()</span><br><span class="line">        (<span class="number">2</span>): LinearWithLoRA(</span><br><span class="line">          (linear): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (lora): LoRALayer()</span><br><span class="line">        )</span><br><span class="line">    	)</span><br><span class="line">    )</span><br><span class="line">    (norm1): LayerNorm()</span><br><span class="line">    (norm2): LayerNorm()</span><br><span class="line">    (drop_resid): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">	 )</span><br><span class="line"> )</span><br><span class="line"> (final_norm): LayerNorm()</span><br><span class="line"> (out_head): LinearWithLoRA(</span><br><span class="line">   (linear): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">   (lora): LoRALayer()</span><br><span class="line"> )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>通过输出可以看到，模型现在包含了新的 <code>LinearWithLoRA</code> 层，这些层本身包含原始的线性层（我们已将其设置为不可训练）以及我们将要微调的新 LoRA 层。</p>
<p>然而，在开始微调模型之前，我们先计算一下初始分类准确率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=<span class="number">10</span>)</span><br><span class="line">val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=<span class="number">10</span>)</span><br><span class="line">test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training accuracy: <span class="subst">&#123;train_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Validation accuracy: <span class="subst">&#123;val_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test accuracy: <span class="subst">&#123;test_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>得到的准确率值如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Training accuracy: <span class="number">46.25</span>%</span><br><span class="line">Validation accuracy: <span class="number">45.00</span>%</span><br><span class="line">Test accuracy: <span class="number">48.75</span>%</span><br></pre></td></tr></table></figure>
<p>如果将这些准确率与第 6 章中的初始值进行比较，我们会发现它们是相同的。这是因为我们将 LoRA 矩阵 B 初始化为零。因此，矩阵 AB 的乘积得到一个零矩阵。这确保了在开始微调之前，该乘法不会改变原始权重，因为加零不会改变它们。</p>
<p>现在，让我们进入激动人心的部分，使用第 6 章中的训练函数来微调模型。在 M3 MacBook Air 笔记本电脑上，训练大约需要 15 分钟；而在 V100 或 A100 GPU 上，则不到半分钟：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Listing E.7 Finetuning a model with LoRA layers</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> ch06 <span class="keyword">import</span> train_classifier_simple</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">5e-5</span>, weight_decay=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(</span><br><span class="line">    model, train_loader, val_loader, optimizer, device,</span><br><span class="line">    num_epochs=num_epochs, eval_freq=<span class="number">50</span>, eval_iter=<span class="number">5</span>,</span><br><span class="line">    tokenizer=tokenizer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">end_time = time.time()</span><br><span class="line">execution_time_minutes = (end_time - start_time) / <span class="number">60</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training completed in <span class="subst">&#123;execution_time_minutes:<span class="number">.2</span>f&#125;</span> minutes.&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在训练过程中可以看到如下输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Ep <span class="number">1</span> (Step <span class="number">000000</span>): Train loss <span class="number">3.820</span>, Val loss <span class="number">3.462</span></span><br><span class="line">Ep <span class="number">1</span> (Step 000050): Train loss <span class="number">0.396</span>, Val loss <span class="number">0.364</span></span><br><span class="line">Ep <span class="number">1</span> (Step <span class="number">000</span>100): Train loss <span class="number">0.111</span>, Val loss <span class="number">0.229</span></span><br><span class="line">Training accuracy: <span class="number">97.50</span>% | Validation accuracy: <span class="number">95.00</span>%</span><br><span class="line">Ep <span class="number">2</span> (Step 000150): Train loss <span class="number">0.135</span>, Val loss <span class="number">0.073</span></span><br><span class="line">Ep <span class="number">2</span> (Step 000200): Train loss <span class="number">0.008</span>, Val loss <span class="number">0.052</span></span><br><span class="line">Ep <span class="number">2</span> (Step 000250): Train loss <span class="number">0.021</span>, Val loss <span class="number">0.179</span></span><br><span class="line">Training accuracy: <span class="number">97.50</span>% | Validation accuracy: <span class="number">97.50</span>%</span><br><span class="line">Ep <span class="number">3</span> (Step 000300): Train loss <span class="number">0.096</span>, Val loss <span class="number">0.080</span></span><br><span class="line">Ep <span class="number">3</span> (Step 000350): Train loss <span class="number">0.010</span>, Val loss <span class="number">0.116</span></span><br><span class="line">Training accuracy: <span class="number">97.50</span>% | Validation accuracy: <span class="number">95.00</span>%</span><br><span class="line">Ep <span class="number">4</span> (Step 000400): Train loss <span class="number">0.003</span>, Val loss <span class="number">0.151</span></span><br><span class="line">Ep <span class="number">4</span> (Step 000450): Train loss <span class="number">0.008</span>, Val loss <span class="number">0.077</span></span><br><span class="line">Ep <span class="number">4</span> (Step 000500): Train loss <span class="number">0.001</span>, Val loss <span class="number">0.147</span></span><br><span class="line">Training accuracy: <span class="number">100.00</span>% | Validation accuracy: <span class="number">97.50</span>%</span><br><span class="line">Ep <span class="number">5</span> (Step 000550): Train loss <span class="number">0.007</span>, Val loss <span class="number">0.094</span></span><br><span class="line">Ep <span class="number">5</span> (Step 000600): Train loss <span class="number">0.000</span>, Val loss <span class="number">0.056</span></span><br><span class="line">Training accuracy: <span class="number">100.00</span>% | Validation accuracy: <span class="number">97.50</span>%</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">Training completed <span class="keyword">in</span> <span class="number">12.10</span> minutes.  </span><br></pre></td></tr></table></figure>
<p>请注意，使用 LoRA 训练模型比第 6 章中不使用 LoRA 训练模型花费更长的时间，因为 LoRA 层在正向传播过程中引入了额外的计算。然而，对于更大的模型，当反向传播的成本变得更高时，模型使用LoRA训练的速度通常比不使用LoRA更快。</p>
<p>可以看到，该模型获得了完美的训练准确率和非常高的验证准确率。我们还可以将损失曲线可视化，以更好地观察训练是否已经收敛。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ch06 <span class="keyword">import</span> plot_values</span><br><span class="line"></span><br><span class="line">epochs_tensor = torch.linspace(<span class="number">0</span>, num_epochs, <span class="built_in">len</span>(train_losses))</span><br><span class="line">examples_seen_tensor = torch.linspace(<span class="number">0</span>, examples_seen, <span class="built_in">len</span>(train_losses))</span><br><span class="line"></span><br><span class="line">plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=<span class="string">&quot;loss&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果如图 E.5 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://myblog.xindon.top/Image/AppendixE/E.5.png" alt=""></p>
<p>除了基于图 E.5 中显示的损失曲线评估模型外，我们还要计算在完整训练集、验证集和测试集上的准确率（在训练过程中，我们通过 <code>eval_iter=5</code> 设置从 5 个批次中近似计算了训练集和验证集的准确率）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> previous_chapters <span class="keyword">import</span> calc_accuracy_loader</span><br><span class="line"></span><br><span class="line">train_accuracy = calc_accuracy_loader(train_loader, model, device)</span><br><span class="line">val_accuracy = calc_accuracy_loader(val_loader, model, device)</span><br><span class="line">test_accuracy = calc_accuracy_loader(test_loader, model, device)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training accuracy: <span class="subst">&#123;train_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Validation accuracy: <span class="subst">&#123;val_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test accuracy: <span class="subst">&#123;test_accuracy*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>最终得到的准确率值如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Training accuracy: <span class="number">100.00</span>%</span><br><span class="line">Validation accuracy: <span class="number">96.64</span>%</span><br><span class="line">Test accuracy: <span class="number">98.00</span>%</span><br></pre></td></tr></table></figure>
<p>最终得到的准确率表明，该模型在训练集、验证集和测试集上都表现良好。训练准确率达到 100%，表明该模型已完美地学习了训练数据。然而，略低的验证集和测试集准确率（分别为 96.64% 和 97.33%）表明存在轻微的过拟合，因为与训练集相比，该模型在新数据上的泛化能力稍差。总的来说，考虑到我们只微调了相对较少数量的模型权重（270 万个 LoRA 权重，而不是原来的 1.24 亿个模型权重），这个结果已经非常不错。</p>
</div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/my-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">欣冻</div><div class="author-info-description">博客, 技术, 生活</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/6f4fa4e7.html" title="快速幂、逆元与组合数学"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/10/31/7I8gnp.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="快速幂、逆元与组合数学"/></a><div class="content"><a class="title" href="/posts/6f4fa4e7.html" title="快速幂、逆元与组合数学">快速幂、逆元与组合数学</a><time datetime="2025-10-31T15:48:33.000Z" title="发表于 2025-10-31 23:48:33">2025-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/2f58633e.html" title="常用数据结构"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://origin.picgo.net/2025/10/11/792a8743-6d0d-43fe-91b8-0a5a77b529f4a296a597708421a1.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="常用数据结构"/></a><div class="content"><a class="title" href="/posts/2f58633e.html" title="常用数据结构">常用数据结构</a><time datetime="2025-10-11T11:00:00.000Z" title="发表于 2025-10-11 19:00:00">2025-10-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/7258f8a4.html" title="THYTHM 音游"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/08/01/HotT9.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="THYTHM 音游"/></a><div class="content"><a class="title" href="/posts/7258f8a4.html" title="THYTHM 音游">THYTHM 音游</a><time datetime="2025-08-01T04:00:00.000Z" title="发表于 2025-08-01 12:00:00">2025-08-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/73e7a68a.html" title="力扣每日一题讲解"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/07/24/QEaxN.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="力扣每日一题讲解"/></a><div class="content"><a class="title" href="/posts/73e7a68a.html" title="力扣每日一题讲解">力扣每日一题讲解</a><time datetime="2025-07-24T08:50:00.000Z" title="发表于 2025-07-24 16:50:00">2025-07-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/4729e793.html" title="数据结构入门"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/07/05/qp0G9.md.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构入门"/></a><div class="content"><a class="title" href="/posts/4729e793.html" title="数据结构入门">数据结构入门</a><time datetime="2025-07-04T16:11:10.000Z" title="发表于 2025-07-05 00:11:10">2025-07-05</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/c%E8%AF%AD%E8%A8%80/"><span class="card-category-list-name">c语言</span><span class="card-category-list-count">1</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%EF%BC%8Cbutterfly/" style="font-size: 1.1em; color: #999">一键部署，butterfly</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" style="font-size: 1.1em; color: #999">每日一题</a> <a href="/tags/python-%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91-%E9%9F%B3%E6%B8%B8/" style="font-size: 1.1em; color: #999">python,游戏开发,音游</a> <a href="/tags/hexo-github-blog-node-js-npm-git-%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2-hexo%E9%83%A8%E7%BD%B2/" style="font-size: 1.1em; color: #999">hexo, github, blog, node.js,npm,git,部署博客,hexo部署</a> <a href="/tags/c%E8%AF%AD%E8%A8%80-%E5%AD%A6%E4%B9%A0/" style="font-size: 1.1em; color: #999">c语言,学习</a> <a href="/tags/%E5%A4%A7%E5%AE%B6%E5%A5%BD%EF%BC%8C%E6%88%91%E6%98%AF%E8%BF%B7%E8%B7%AF%E7%9A%84%E5%B0%8F%E6%9C%8B%E5%8F%8B/" style="font-size: 1.1em; color: #999">大家好，我是迷路的小朋友</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 1.1em; color: #999">算法</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/10/">
            <span class="card-archive-list-date">
              十月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/08/">
            <span class="card-archive-list-date">
              八月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/07/">
            <span class="card-archive-list-date">
              七月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/05/">
            <span class="card-archive-list-date">
              五月 2025
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/04/">
            <span class="card-archive-list-date">
              四月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/03/">
            <span class="card-archive-list-date">
              三月 2025
            </span>
            <span class="card-archive-list-count">7</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/02/">
            <span class="card-archive-list-date">
              二月 2025
            </span>
            <span class="card-archive-list-count">7</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">22</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-10-31T13:42:25.148Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://i.imgs.ovh/2025/07/03/qLFy9.png);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By 欣冻</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://blog-twikoo.xindon.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = (el = document, path = location.pathname) => {
    twikoo.init({
      el: el.querySelector('#twikoo-wrap'),
      envId: 'https://blog-twikoo.xindon.top/',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      },
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    

    isShuoshuo && (window.shuoshuoComment.destroyTwikoo = () => {
      if (el.children.length) {
        el.innerHTML = ''
        el.classList.add('no-comment')
      }
    })
  }

  const loadTwikoo = (el, path) => {
    if (typeof twikoo === 'object') setTimeout(() => init(el, path), 0)
    else btf.getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(() => init(el, path))
  }

  if (isShuoshuo) {
    'Twikoo' === 'Twikoo'
      ? window.shuoshuoComment = { loadComment: loadTwikoo }
      : window.loadOtherComment = loadTwikoo
    return
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><div class="aplayer no-destroy" data-id="13348674056" data-server="netease" data-type="playlist"   data-order="list" data-fixed="true" data-preload="auto" data-autoplay="false" data-mutex="true" ></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v 7.3.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v5.2.2" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://www.jsdelivr.com/" style="margin-inline:5px" data-title="本站使用JsDelivr为静态资源提供CDN加速" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.js"></script><script async src="/js/ali_font.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('article-sort-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__slideInRight');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var qweather_key = '0cca502ccc7341c2be6ba09309916622';
  var gaud_map_key = '5653914d2fc43aad14b253ab6cf762b9';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '112.982279,28.19409';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/miku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>